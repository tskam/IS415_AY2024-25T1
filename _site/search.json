[
  {
    "objectID": "th_ex1.html",
    "href": "th_ex1.html",
    "title": "Take-home Exercise 1: Geospatial Analytics for Social Good: Application of Spatial and Spatio-temporal Point Patterns Analysis to discover the geographical distribution of Armed Conflict in Myanmar",
    "section": "",
    "text": "Millions of people have their lives shattered by armed conflict – wars – every year.\nArmed conflict has been on the rise since about 2012, after a decline in the 1990s and early 2000s. First came conflicts in Libya, Syria and Yemen, triggered by the 2011 Arab uprisings. Libya’s instability spilled south, helping set off a protracted crisis in the Sahel region. A fresh wave of major combat followed: the 2020 Azerbaijani-Armenian war over the Nagorno-Karabakh enclave, horrific fighting in Ethiopia’s northern Tigray region that began weeks later, the conflict prompted by the Myanmar army’s 2021 power grab and Russia’s 2022 assault on Ukraine. Add to those 2023’s devastation in Sudan and Gaza. Around the globe, more people are dying in fighting, being forced from their homes or in need of life-saving aid than in decades.\nSource: 10 Conflicts to Watch in 2024"
  },
  {
    "objectID": "th_ex1.html#setting-the-scene",
    "href": "th_ex1.html#setting-the-scene",
    "title": "Take-home Exercise 1: Geospatial Analytics for Social Good: Application of Spatial and Spatio-temporal Point Patterns Analysis to discover the geographical distribution of Armed Conflict in Myanmar",
    "section": "",
    "text": "Millions of people have their lives shattered by armed conflict – wars – every year.\nArmed conflict has been on the rise since about 2012, after a decline in the 1990s and early 2000s. First came conflicts in Libya, Syria and Yemen, triggered by the 2011 Arab uprisings. Libya’s instability spilled south, helping set off a protracted crisis in the Sahel region. A fresh wave of major combat followed: the 2020 Azerbaijani-Armenian war over the Nagorno-Karabakh enclave, horrific fighting in Ethiopia’s northern Tigray region that began weeks later, the conflict prompted by the Myanmar army’s 2021 power grab and Russia’s 2022 assault on Ukraine. Add to those 2023’s devastation in Sudan and Gaza. Around the globe, more people are dying in fighting, being forced from their homes or in need of life-saving aid than in decades.\nSource: 10 Conflicts to Watch in 2024"
  },
  {
    "objectID": "th_ex1.html#objectives",
    "href": "th_ex1.html#objectives",
    "title": "Take-home Exercise 1: Geospatial Analytics for Social Good: Application of Spatial and Spatio-temporal Point Patterns Analysis to discover the geographical distribution of Armed Conflict in Myanmar",
    "section": "Objectives",
    "text": "Objectives\nGeospatial analytics hold tremendous potential to address complex problems facing society. In this study, you are tasked to apply spatial point patterns analysis methods to discover the spatial and spatio-temporal distribution of armed conflict in Myanmar."
  },
  {
    "objectID": "th_ex1.html#the-task",
    "href": "th_ex1.html#the-task",
    "title": "Take-home Exercise 1: Geospatial Analytics for Social Good: Application of Spatial and Spatio-temporal Point Patterns Analysis to discover the geographical distribution of Armed Conflict in Myanmar",
    "section": "The Task",
    "text": "The Task\nThe specific tasks of this take-home exercise are as follows:\n\nUsing appropriate function of sf and tidyverse packages, import and transform the downloaded armed conflict data and administrative boundary data into sf tibble data.frames.\nUsing the geospatial data sets prepared, derive quarterly KDE layers.\nUsing the geospatial data sets prepared, perform 2nd-Order Spatial Point Patterns Analysis.\nUsing the geospatial data sets prepared, derive quarterly spatio-temporal KDE layers.\nUsing the geospatial data sets prepared, perform 2nd-Order Spatio-temporal Point Patterns Analysis.\nUsing appropriate tmap functions, display the KDE and Spatio-temporal KDE layers on openstreetmap of Myanmar.\nDescribe the spatial patterns revealed by the KDE and Spatio-temporal KDE maps."
  },
  {
    "objectID": "th_ex1.html#the-data",
    "href": "th_ex1.html#the-data",
    "title": "Take-home Exercise 1: Geospatial Analytics for Social Good: Application of Spatial and Spatio-temporal Point Patterns Analysis to discover the geographical distribution of Armed Conflict in Myanmar",
    "section": "The Data",
    "text": "The Data\n\nArmed conflict data\nFor the purpose of this assignment, armed conflict data of Myanmar between 2021-2024 from Armed Conflict Location & Event Data (ACLED), an independent, impartial, international non-profit organization collecting data on violent conflict and protest in all countries and territories in the world, should be used.\nIn terms of event types, students should focus on at least four main event types, namely: Battles, Explosion/Remote violence, Strategic developments, and Violence against civilians.\nIn terms of study period, students should focus on quarterly armed conflict events from January 2021 until June 2024.\n\n\nGIS Data\n\nGeospatial data on Myanmar Information Management Unit, MIMU"
  },
  {
    "objectID": "th_ex1.html#grading-criteria",
    "href": "th_ex1.html#grading-criteria",
    "title": "Take-home Exercise 1: Geospatial Analytics for Social Good: Application of Spatial and Spatio-temporal Point Patterns Analysis to discover the geographical distribution of Armed Conflict in Myanmar",
    "section": "Grading Criteria",
    "text": "Grading Criteria\nThis exercise will be graded by using the following criteria:\n\nGeospatial Data Wrangling (20 marks): This is an important aspect of geospatial analytics. You will be assessed on your ability to employ appropriate R functions from various R packages specifically designed for modern data science such as readxl, tidyverse (tidyr, dplyr, ggplot2), sf just to mention a few of them, to perform the entire geospatial data wrangling processes, including. This is not limited to data import, data extraction, data cleaning and data transformation. Besides assessing your ability to use the R functions, this criterion also includes your ability to clean and derive appropriate variables to meet the analysis need.\n\n\n\n\n\n\n\nWarning\n\n\n\nAll data are like vast grassland full of land mines. Your job is to clear those mines and not to step on them).\n\n\n\nGeospatial Analysis (30 marks): In this exercise, you are expected to use the appropriate spatial point patterns analysis methods and R packages introduced in class to analysis the geospatial data prepared. You will be assessed on your ability to derive analytical products by using appropriate kernel estimation techniques.\nGeovisualisation and geocommunication (20 marks): In this section, you will be assessed on your ability to communicate Exploratory Spatial Data Analysis and Confirmatory Spatial Data Analysis results in layman friendly visual representations. This course is geospatial centric, hence, it is important for you to demonstrate your competency in using appropriate geovisualisation techniques to reveal and communicate the findings of your analysis.\nReproducibility (15 marks): This is an important learning outcome of this exercise. You will be assessed on your ability to provide a comprehensive documentation of the analysis procedures in the form of code chunks of Quarto. It is important to note that it is not enough by merely providing the code chunk without any explanation on the purpose and R function(s) used.\nBonus (15 marks): Demonstrate your ability to employ methods beyond what you had learned in class to gain insights from the data."
  },
  {
    "objectID": "th_ex1.html#submission-instructions",
    "href": "th_ex1.html#submission-instructions",
    "title": "Take-home Exercise 1: Geospatial Analytics for Social Good: Application of Spatial and Spatio-temporal Point Patterns Analysis to discover the geographical distribution of Armed Conflict in Myanmar",
    "section": "Submission Instructions",
    "text": "Submission Instructions\n\nThe write-up of the take-home exercise must be in Quarto html document format. You are required to publish the write-up on Netlify.\nZip the take-home exercise folder and upload it onto eLearn. If the size of the zip file is beyond the capacity of eLearn, you can upload it on SMU OneDrive and provide the download link on eLearn.."
  },
  {
    "objectID": "th_ex1.html#due-date",
    "href": "th_ex1.html#due-date",
    "title": "Take-home Exercise 1: Geospatial Analytics for Social Good: Application of Spatial and Spatio-temporal Point Patterns Analysis to discover the geographical distribution of Armed Conflict in Myanmar",
    "section": "Due Date",
    "text": "Due Date\n22nd September 2024 (Sunday, mid-night 11:59pm)."
  },
  {
    "objectID": "th_ex1.html#reference",
    "href": "th_ex1.html#reference",
    "title": "Take-home Exercise 1: Geospatial Analytics for Social Good: Application of Spatial and Spatio-temporal Point Patterns Analysis to discover the geographical distribution of Armed Conflict in Myanmar",
    "section": "Reference",
    "text": "Reference\n\nMyanmar’s Troubled History: Coups, Military Rule, and Ethnic Conflict\nMyanmar conflict\nMyanmar civil war (2021–present)"
  },
  {
    "objectID": "th_ex1.html#learning-from-senior",
    "href": "th_ex1.html#learning-from-senior",
    "title": "Take-home Exercise 1: Geospatial Analytics for Social Good: Application of Spatial and Spatio-temporal Point Patterns Analysis to discover the geographical distribution of Armed Conflict in Myanmar",
    "section": "Learning from senior",
    "text": "Learning from senior\nYou are advised to review these sample submissions prepared by your seniors.\n\nANN MEI YI VICTORIA GRACE.  The discussion on Network-Constrained Kernel Density Estimation (NKDE) & Analysis section is very interesting.\nKHANT MIN NAING . A very well researched submission. Did exceptionally well in performing geospatial analysis and geocommucating and did well in geospatial data wrangling and geovisualisation.\nMATTHEW HO YIWEN Did exceptionally well in geospatial data wrangling and reproducibility, excellence in geospatial analysis and moderate well in geovisualisation and geocommunication."
  },
  {
    "objectID": "ShinyWorkshop/Shiny3/Shiny3.html#overview",
    "href": "ShinyWorkshop/Shiny3/Shiny3.html#overview",
    "title": "Building Web-enabled Geospatial Analytics Application with Shiny: Beyond the basic",
    "section": "Overview",
    "text": "Overview\nIn this lesson, selected advanced methods of Shiny will be discussed. You will also gain hands-on experiences on using these advanced methods to build Shiny applications.\nBy the end of this lesson, you will be able to:\n\nunderstanding the basic development cycle of creating apps, making changes, and experimenting with the results,\ndebug errors in the codes,\nbuild complex Shiny application using module, and\nimprove the productivity of Shiny applications development"
  },
  {
    "objectID": "ShinyWorkshop/Shiny3/Shiny3.html#working-with-shiny-layout",
    "href": "ShinyWorkshop/Shiny3/Shiny3.html#working-with-shiny-layout",
    "title": "Building Web-enabled Geospatial Analytics Application with Shiny: Beyond the basic",
    "section": "Working with Shiny Layout",
    "text": "Working with Shiny Layout\nShiny includes a number of facilities for laying out the components of an application. This guide describes the following application layout features:\n\nA sidebarLayout(): for placing a sidebarPanel() of inputs alongside a mainPanel() output content.\nCustom layouts using Shiny’s grid layout system (i.e., fluidRow() & column()).\nSegmenting layouts using the tabsetPanel() and navlistPanel() functions.\nCreating applications with multiple top-level components using the navbarPage() function."
  },
  {
    "objectID": "ShinyWorkshop/Shiny3/Shiny3.html#multi-page-layouts",
    "href": "ShinyWorkshop/Shiny3/Shiny3.html#multi-page-layouts",
    "title": "Building Web-enabled Geospatial Analytics Application with Shiny: Beyond the basic",
    "section": "Multi-page layouts",
    "text": "Multi-page layouts\n\n\nShiny provides several functions for building multi-page layout, they are:\n\ntabsetPanel() + tabPanel()\nnavlistPanel() + tabPanel()\nnavbarMenu() + tabPanel()"
  },
  {
    "objectID": "ShinyWorkshop/Shiny3/Shiny3.html#shiny-themes",
    "href": "ShinyWorkshop/Shiny3/Shiny3.html#shiny-themes",
    "title": "Building Web-enabled Geospatial Analytics Application with Shiny: Beyond the basic",
    "section": "Shiny Themes",
    "text": "Shiny Themes\n\n\n\n\nShiny v1.6 and higher integrates with the bslib package providing easy access to modern versions of Bootstrap, Bootswatch themes, as well as custom themes that can even be modified in real time!\nTo use bslib in your own Shiny app, pass a bs_theme() object to the theme argument of the relevant page layout function, such as navbarPage() or fluidPage().\nInside bs_theme(), you can specify a version of Bootstrap and (optionally) a Bootswatch theme (e.g. cyborg)\n\n\n\n\nlibrary(shiny)\nlibrary(tidyverse)\nlibrary(bslib)     \nexam &lt;- read_csv(\"data/Exam_data.csv\")\nui &lt;- fluidPage(\n  theme = bs_theme(bootswatch = \"cyborg\"), \n  titlePanel(\"Pupils Examination Results Dashboard\"),"
  },
  {
    "objectID": "ShinyWorkshop/Shiny3/Shiny3.html#introducing-shiny-module",
    "href": "ShinyWorkshop/Shiny3/Shiny3.html#introducing-shiny-module",
    "title": "Building Web-enabled Geospatial Analytics Application with Shiny: Beyond the basic",
    "section": "Introducing Shiny Module",
    "text": "Introducing Shiny Module\n\nAs Shiny applications grow larger and more complicated, modules are used to manage the growing complexity of Shiny application code.\nFunctions are the fundamental unit of abstraction in R, and we designed Shiny to work with them.\nWe can write UI-generating functions and call them from our app, and we can write functions to be used in the server function that define outputs and create reactive expressions."
  },
  {
    "objectID": "ShinyWorkshop/Shiny3/Shiny3.html#debugging",
    "href": "ShinyWorkshop/Shiny3/Shiny3.html#debugging",
    "title": "Building Web-enabled Geospatial Analytics Application with Shiny: Beyond the basic",
    "section": "Debugging",
    "text": "Debugging\n\n\n\nProgramming == Frustration?\n\nSource: The original article.\n\nThree main cases of problems which we’ll discuss below:\n\nYou get an unexpected error. This is the easiest case, because you’ll get a traceback which allows you to figure out exactly where the error occurred.\nYou don’t get any errors, but some value is incorrect. Here, you’ll need to use the interactive debugger, along with your investigative skills to track down the root cause.\nAll the values are correct, but they’re not updated when you expect. This is the most challenging problem because it’s unique to Shiny, so you can’t take advantage of your existing R debugging skills."
  },
  {
    "objectID": "ShinyWorkshop/Shiny3/Shiny3.html#standard-r-debugging-tools",
    "href": "ShinyWorkshop/Shiny3/Shiny3.html#standard-r-debugging-tools",
    "title": "Building Web-enabled Geospatial Analytics Application with Shiny: Beyond the basic",
    "section": "Standard R debugging tools",
    "text": "Standard R debugging tools\n\nTracing\n\ntracebacks\nprint()/cat()/str()\nrenderPrint eats messages, must use cat(file = stderr(), ““…)\nAlso consider shinyjs package’s logjs, which puts messages in the browser’s JavaScript console\n\n\nDebugger\n\nSet breakpoints in RStudio\nbrowser()\nConditionals: if (!is.null(input$x)) browser()"
  },
  {
    "objectID": "ShinyWorkshop/Shiny3/Shiny3.html#in-class-exercise-working-with-rstudios-interactive-debugger",
    "href": "ShinyWorkshop/Shiny3/Shiny3.html#in-class-exercise-working-with-rstudios-interactive-debugger",
    "title": "Building Web-enabled Geospatial Analytics Application with Shiny: Beyond the basic",
    "section": "In-class Exercise: Working with RStudio’s Interactive Debugger",
    "text": "In-class Exercise: Working with RStudio’s Interactive Debugger\n\n\nIn this hands-on Exercise, you will learn how to work with the interactive debugger in RStudio.\n\nDouble click prototype7a.\nDouble click on prototype7a.rproj file to open the project file in RStudio.\nClick on app.R file to open the Shiny app file on RStudio\nAdd a call to browser() in your source code (for example line 5).\nClick in Run App button to run the Shiny app.\n\n\n\n\nNext (press n): executes the next step in the function. Note that if you have a variable named n, you’ll need to use print(n) to display its value.\nContinue (press c): leaves interactive debugging and continues regular execution of the function. This is useful if you’ve fixed the bad state and want to check that the function proceeds correctly.\nStop (press Q): stops debugging, terminates the function, and returns to the global workspace. Use this once you’ve figured out where the problem is, and you’re ready to fix it and reload the code."
  },
  {
    "objectID": "ShinyWorkshop/Shiny3/Shiny3.html#shinytest",
    "href": "ShinyWorkshop/Shiny3/Shiny3.html#shinytest",
    "title": "Building Web-enabled Geospatial Analytics Application with Shiny: Beyond the basic",
    "section": "Shinytest",
    "text": "Shinytest\n\nShinytest uses snapshot-based testing strategy.\nThe first time it runs a set of tests for an application, it performs some scripted interactions with the app and takes one or more snapshots of the application’s state.\nThese snapshots are saved to disk so that future runs of the tests can compare their results to them."
  },
  {
    "objectID": "ShinyWorkshop/Shiny3/Shiny3.html#references",
    "href": "ShinyWorkshop/Shiny3/Shiny3.html#references",
    "title": "Building Web-enabled Geospatial Analytics Application with Shiny: Beyond the basic",
    "section": "References",
    "text": "References\nShiny Layout\n\nApplication layout guide\n\nShiny Module\n\nChapter 19 Shiny modules of Mastering Shiny.\nModularizing Shiny app code, online article\nCommunication between modules. This is a relatively old article, some functions have changed.\nShiny Modules\nShiny Modules (part 1) : Why using modules?\nShiny Modules (part 2): Share reactive among multiple modules\nShiny Modules (part 3): Dynamic module call"
  },
  {
    "objectID": "ShinyWorkshop/Shiny1/Shiny1.html#content",
    "href": "ShinyWorkshop/Shiny1/Shiny1.html#content",
    "title": "Building Web-enabled Geospatial Analytics Applications with Shiny: Shiny basic",
    "section": "",
    "text": "What is a Web-enabled Geospatial Analytics Application?\nWhy building Web-enabled Geospatial Analytical Application?\nEvolution of web-based Technology\nGetting to Know Shiny"
  },
  {
    "objectID": "ShinyWorkshop/Shiny1/Shiny1.html#what-is-a-web-enabled-geospatial-analytics-application",
    "href": "ShinyWorkshop/Shiny1/Shiny1.html#what-is-a-web-enabled-geospatial-analytics-application",
    "title": "Building Web-enabled Geospatial Analytics Applications with Shiny: Shiny basic",
    "section": "What is a Web-enabled Geospatial Analytics Application?",
    "text": "What is a Web-enabled Geospatial Analytics Application?\n\n\n\nFocuses and emphasises on interactivity and effective integration of techniques from data analytics, visualization and human-computer interaction (HCI).\n\n\n\n\n\n\n\nWhy building a Web-enabled Geospatial Analytics Application?\n\n\n\nTo explore how the best of these different but related domains can be combined such that the sum is greater than the parts.\nTo democratise data and analytics through web-based analytical applications for data exploration, visualisation analysis and modelling.\n\n\n\n\nSource: Democratize data analytics customer data platform\n\n\n\n\n\n\nTechnology Challenges\n\n\n\nMainframe computing tend to have low usability and low accessibility.\nDesktop computing tend to have high usability but low accessibility.\nWeb-based computing (including mobile computing) are highly accesible but with relatively low capability.\n\n\n\n\n\n\n\n\nWeb-based data visualisation\n\n\n\nThe break-through is Rich Internet Applications (RIA)\n\n\n\n\n\n\nReference: Rich Internet Applications\n\n\n\n\nDevelopment of RIA\n\n\n\nFirst generation RIA data visualisation (2000~)\n\nAdobe Flex Builder\n\nFlare\n\nMicrosoft Silverlight\nJavaFX\n\n\n\n\nSecond generation RIA data visualisation (2010~)\n\nHTML 5 + JavaScript + SVG + CSS\n\nClient-side rendering\nNo plug-in is required\nMobile computing enabled\n\nD3.js - Data Driven Document\n\n\n\n\n\n\n\nMethodological Challenges\n\n\n\nLack of analysis functions.\nNot reproducible.\nNot extendable.\nRequire to learn multiple technologies and methods."
  },
  {
    "objectID": "ShinyWorkshop/Shiny1/Shiny1.html#getting-to-know-shiny",
    "href": "ShinyWorkshop/Shiny1/Shiny1.html#getting-to-know-shiny",
    "title": "Building Web-enabled Geospatial Analytics Applications with Shiny: Shiny basic",
    "section": "Getting to Know Shiny",
    "text": "Getting to Know Shiny\n\nShiny: Overview\n\n\n\nShiny is an open source package from Posit (formally called RStudio).\nIt provides a web application framework to create interactive web applications (visualization) called “Shiny apps”.\nTo learn more about Shiny, visit its homepage"
  },
  {
    "objectID": "ShinyWorkshop/Shiny1/Shiny1.html#getting-to-know-shiny-1",
    "href": "ShinyWorkshop/Shiny1/Shiny1.html#getting-to-know-shiny-1",
    "title": "Building Web-enabled Geospatial Analytics Applications with Shiny: Shiny basic",
    "section": "Getting to Know Shiny",
    "text": "Getting to Know Shiny\n\nWhat is so special about Shiny?\n\n\nIt allows R users:\n\nto build and share highly interactive web-enabled applications without having to invest significant among of time and efforts to master core web design technologies such as html5, Javascript and CSS.\nto integrate the analytical and visualisation packages of R without having to change from one programming language to another."
  },
  {
    "objectID": "ShinyWorkshop/Shiny1/Shiny1.html#getting-to-know-shiny-2",
    "href": "ShinyWorkshop/Shiny1/Shiny1.html#getting-to-know-shiny-2",
    "title": "Building Web-enabled Geospatial Analytics Applications with Shiny: Shiny basic",
    "section": "Getting to Know Shiny",
    "text": "Getting to Know Shiny\n\nUnderstanding the architecture\n\nEvery Shiny app is maintained by a computer running R."
  },
  {
    "objectID": "ShinyWorkshop/Shiny1/Shiny1.html#getting-to-know-shiny-3",
    "href": "ShinyWorkshop/Shiny1/Shiny1.html#getting-to-know-shiny-3",
    "title": "Building Web-enabled Geospatial Analytics Applications with Shiny: Shiny basic",
    "section": "Getting to Know Shiny",
    "text": "Getting to Know Shiny\n\nThe Structure of a Shiny app\n\n\n\nA Shiny app comprises of two components, namely:\n\na user-interface script, and\na server script."
  },
  {
    "objectID": "ShinyWorkshop/Shiny1/Shiny1.html#getting-to-know-shiny-4",
    "href": "ShinyWorkshop/Shiny1/Shiny1.html#getting-to-know-shiny-4",
    "title": "Building Web-enabled Geospatial Analytics Applications with Shiny: Shiny basic",
    "section": "Getting to Know Shiny",
    "text": "Getting to Know Shiny\n\nShiny’s user-interface, ui.R\n\nThe ui.R script controls the layout and appearance of a shiny app.\n\nIt is defined in a source script name ui.R.\nActually, ui is a web document that the user gets to see, it is based on the famous Twitter bootstrap framework, which makes the look and layout highly customizable and fully responsive.\nIn fact, you only need to know R and how to use the shiny package to build a pretty web application. Also, a little knowledge of HTML, CSS, and JavaScript may help."
  },
  {
    "objectID": "ShinyWorkshop/Shiny1/Shiny1.html#getting-to-know-shiny-5",
    "href": "ShinyWorkshop/Shiny1/Shiny1.html#getting-to-know-shiny-5",
    "title": "Building Web-enabled Geospatial Analytics Applications with Shiny: Shiny basic",
    "section": "Getting to Know Shiny",
    "text": "Getting to Know Shiny\n\nShiny’s server server.R\n\nThe server.R script contains the instructions that your computer needs to build your Shiny app.\nYou are expected to:\n\nknow how to programme with R.\nfamiliar with Tidyverse, specifically dplyr, tidyr and ggplot2"
  },
  {
    "objectID": "ShinyWorkshop/Shiny1/Shiny1.html#getting-to-know-shiny-6",
    "href": "ShinyWorkshop/Shiny1/Shiny1.html#getting-to-know-shiny-6",
    "title": "Building Web-enabled Geospatial Analytics Applications with Shiny: Shiny basic",
    "section": "Getting to Know Shiny",
    "text": "Getting to Know Shiny\n\nShiny Examples\nThe Shiny package has eleven built-in examples that each demonstrates how Shiny works."
  },
  {
    "objectID": "ShinyWorkshop/Shiny1/Shiny1.html#getting-to-know-shiny-7",
    "href": "ShinyWorkshop/Shiny1/Shiny1.html#getting-to-know-shiny-7",
    "title": "Building Web-enabled Geospatial Analytics Applications with Shiny: Shiny basic",
    "section": "Getting to Know Shiny",
    "text": "Getting to Know Shiny\n\nRunning Shiny example\n\n\n\nlibrary(shiny)\nrunExample(\"01_hello\")"
  },
  {
    "objectID": "ShinyWorkshop/Shiny1/Shiny1.html#building-a-shiny-app",
    "href": "ShinyWorkshop/Shiny1/Shiny1.html#building-a-shiny-app",
    "title": "Building Web-enabled Geospatial Analytics Applications with Shiny: Shiny basic",
    "section": "Building a Shiny app",
    "text": "Building a Shiny app\n\nA Shiny app can be in a form of a single file called app.R.\nAlternatively, a Shiny app can be also created using separate ui.R and server.R files.\nThe seperate files way is preferred when the app is complex and involves more codes.\n\n\n\n\nA basic Shiny app script\n\n\n\n\nImportant tips of Shiny app file\n\n\n\n\n\n\n\n\nTip\n\n\n\n\nIt is very important that the name of the file is app.R, otherwise it would not be recognized as a Shiny app.\nYou should not have any R code after the shinyApp(ui = ui, server = server) line. That line needs to be the last line in your file.\nIt is good practice to place this app in its own folder, and not in a folder that already has other R scripts or files, unless those other files are used by your app.\n\n\n\n\n\n\n\n\nLoading the dataset\n\n\n\n\npacman::p_load(shiny, sf, tmap,\n               bslib, tidyverse)\n\nhunan &lt;- st_read(dsn = \"data/geospatial\",\n                 layer = \"Hunan\")\ndata &lt;- read_csv(\"data/aspatial/Hunan_2012.csv\")\nhunan_data &lt;- left_join(hunan, data,\n                        by = c(\"County\" = \"COUNTY\"))\n\nui &lt;- fluidPage()\nserver &lt;- function(input, output){}\n\nshinyApp(ui = ui, server = server)\n\n\n\n\nMake sure that the data file path and file name are correct.\nTo check if the dataset has been added correctly, you can add a print() argument after reading the data."
  },
  {
    "objectID": "ShinyWorkshop/Shiny1/Shiny1.html#shiny-layout",
    "href": "ShinyWorkshop/Shiny1/Shiny1.html#shiny-layout",
    "title": "Building Web-enabled Geospatial Analytics Applications with Shiny: Shiny basic",
    "section": "Shiny Layout",
    "text": "Shiny Layout\n\nShiny ui.R scripts use the function fluidPage to create a display that automatically adjusts to the dimensions of your user’s browser window.\nYou lay out your app by placing elements in the fluidPage function.\ntitlePanel and sidebarLayout are the two most popular elements to add to fluidPage. They create a basic Shiny app with a sidebar."
  },
  {
    "objectID": "ShinyWorkshop/Shiny1/Shiny1.html#shiny-layout-panels",
    "href": "ShinyWorkshop/Shiny1/Shiny1.html#shiny-layout-panels",
    "title": "Building Web-enabled Geospatial Analytics Applications with Shiny: Shiny basic",
    "section": "Shiny Layout Panels",
    "text": "Shiny Layout Panels\n\nPanels are used to group multiple elements into a single element that has its own properties.\nEspecially important and useful for complex apps with a large number of inputs and outputs such that it might not be clear to the user where to get started.\n\n\n\n\nWorking with titlePanel\n\n\n\ntitlePanel() is used to add the application title.\n\n\n\npacman::p_load(shiny, sf, tmap,\n               bslib, tidyverse)\n\nhunan &lt;- st_read(dsn = \"data/geospatial\",\n                 layer = \"Hunan\")\ndata &lt;- read_csv(\"data/aspatial/Hunan_2012.csv\")\nhunan_data &lt;- left_join(hunan, data,\n                        by = c(\"County\" = \"COUNTY\"))\n\nui &lt;- fluidPage(\n  titlePanel(\"Choropleth Mapping\")\n)\nserver &lt;- function(input, output){}\n\nshinyApp(ui = ui, server = server)\n\n\n\n\n\n\n\n\n\nShiny Layout Panel : sidebarLayout()\n\n\n\nsidebarLayout() always takes two arguments:\n\nsidebarPanel() function output\nmainPanel() function output\n\nThese functions place content in either the sidebar or the main panels.\nThe sidebarPanel() will appear on the left side of your app by default. You can move it to the right side by giving sidebarLayout() the optional argument position = “right”.\n\n\n\n\n\n\n\n\n\nHands-on Exercise: Working with sidebarLayout()\n\n\nLets add the highlighted codes into the original code chunk.\n\n\nRefresh Shiny App and your screen should look similar to the figure below.\n\n\n\n\n\n\n\nNote\n\n\n\nNote that in a fluid design your sidebar and other elements may “collapse” if your browser view is not wide enough."
  },
  {
    "objectID": "ShinyWorkshop/Shiny1/Shiny1.html#shiny-inputs",
    "href": "ShinyWorkshop/Shiny1/Shiny1.html#shiny-inputs",
    "title": "Building Web-enabled Geospatial Analytics Applications with Shiny: Shiny basic",
    "section": "Shiny Inputs",
    "text": "Shiny Inputs\n\nAn overview of Shiny Inputs\n\n\n\nInputs are what gives users a way to interact with a Shiny app.\nShiny provides many input functions to support many kinds of interactions that the user could have with an app.\n\n\n\n\n\nReference: Refer to 2 Basic UI of Master Shiny to learn more about Shiny UI usage and arguments."
  },
  {
    "objectID": "ShinyWorkshop/Shiny1/Shiny1.html#shiny-inputs-1",
    "href": "ShinyWorkshop/Shiny1/Shiny1.html#shiny-inputs-1",
    "title": "Building Web-enabled Geospatial Analytics Applications with Shiny: Shiny basic",
    "section": "Shiny Inputs",
    "text": "Shiny Inputs\n\nInputs syntax\n\n\n\nAll input functions have the same first two arguments: inputId and label.\nThe inputId will be the name that Shiny will use to refer to this input when you want to retrieve its current value.\nIt is important to note that every input must have a unique inputId.\nThe label argument specifies the text in the display label that goes along with the input widget.\nEvery input can also have multiple other arguments specific to that input type.\n\n\n\n\n\n\n\n\nHands-on Exercise: Adding inputs\nAdding inputs to the UI by using selectInput() and sliderInput()."
  },
  {
    "objectID": "ShinyWorkshop/Shiny1/Shiny1.html#shiny-output",
    "href": "ShinyWorkshop/Shiny1/Shiny1.html#shiny-output",
    "title": "Building Web-enabled Geospatial Analytics Applications with Shiny: Shiny basic",
    "section": "Shiny Output()",
    "text": "Shiny Output()\n\nAn overview of Shiny Output()\n\nAfter creating all the inputs, we should add elements to the UI to display the outputs.\nTo display output, add it to fluidPage() with an Output() function.\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nSimilarly to the input functions, all the output functions have a outputId argument that is used to identify each output, and this argument must be unique for each output.\nEach output needs to be constructed in the server code later."
  },
  {
    "objectID": "ShinyWorkshop/Shiny1/Shiny1.html#shiny-output-1",
    "href": "ShinyWorkshop/Shiny1/Shiny1.html#shiny-output-1",
    "title": "Building Web-enabled Geospatial Analytics Applications with Shiny: Shiny basic",
    "section": "Shiny Output()",
    "text": "Shiny Output()\n\nShiny Output() options\n\n\n\nOutputs can be any object that R creates and that we want to display in our app - such as a plot, a table, or text.\n\n\n\n\n\n\n\n\nHands-on Exercise:\n\n\nAdding plotOutput()"
  },
  {
    "objectID": "ShinyWorkshop/Shiny1/Shiny1.html#shiny-server.r",
    "href": "ShinyWorkshop/Shiny1/Shiny1.html#shiny-server.r",
    "title": "Building Web-enabled Geospatial Analytics Applications with Shiny: Shiny basic",
    "section": "Shiny server.R",
    "text": "Shiny server.R\n\nBuilding an output\nThere are three rules to build an output in Shiny, they are:\n\nSave the output object into the output list (remember the app template - every server function has an output argument).\nBuild the object with a render() function, where is the type of output.\nAccess input values using the input list (every server function has an input argument)\n\n\n\n\n\n\n\nNote\n\n\n\nThe third rule is only required if you want your output to depend on some input."
  },
  {
    "objectID": "ShinyWorkshop/Shiny1/Shiny1.html#shiny-server.r-1",
    "href": "ShinyWorkshop/Shiny1/Shiny1.html#shiny-server.r-1",
    "title": "Building Web-enabled Geospatial Analytics Applications with Shiny: Shiny basic",
    "section": "Shiny server.R",
    "text": "Shiny server.R\n\nA generic Shiny render() syntax"
  },
  {
    "objectID": "ShinyWorkshop/Shiny1/Shiny1.html#shiny-server.r-2",
    "href": "ShinyWorkshop/Shiny1/Shiny1.html#shiny-server.r-2",
    "title": "Building Web-enabled Geospatial Analytics Applications with Shiny: Shiny basic",
    "section": "Shiny server.R",
    "text": "Shiny server.R\n\nShiny render()\n\n\n\n\nHands-on Exercise: Building a basic output\n\n\nLet’s first see how to build a very basic output using renderPlot(). We’ll create a plot and send it to the mapPlot output.\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nThis simple code shows the first two rules: we’re creating a plot inside the renderPlot() function, and assigning it to distPlot in the output list.\nRemember that every output created in the UI must have a unique ID, now we see why. In order to attach an R object to an output with ID x, we assign the R object to output$distPlot.\nSince distPlot was defined as a plotOutput, we must use the renderPlot() function, and we must create a plot inside the renderPlot() function."
  },
  {
    "objectID": "ShinyWorkshop/Shiny1/Shiny1.html#the-shinyapp",
    "href": "ShinyWorkshop/Shiny1/Shiny1.html#the-shinyapp",
    "title": "Building Web-enabled Geospatial Analytics Applications with Shiny: Shiny basic",
    "section": "The shinyApp()",
    "text": "The shinyApp()\n\n\n\n\n\n\n\n\nImportant\n\n\n\n\nIt is important to add shinyApp() at the end of your Shiny application.\n\n\n\n\nshinyApp(ui = ui, server = server)"
  },
  {
    "objectID": "ShinyWorkshop/Shiny1/Shiny1.html#programming-shiny-survival-tip",
    "href": "ShinyWorkshop/Shiny1/Shiny1.html#programming-shiny-survival-tip",
    "title": "Building Web-enabled Geospatial Analytics Applications with Shiny: Shiny basic",
    "section": "Programming Shiny: Survival Tip!",
    "text": "Programming Shiny: Survival Tip!\n\n\n\n\n\n\nTip\n\n\n\n\nAlways run the entire script, not just up to the point where you’re developing code.\nSometimes the best way to see what’s wrong is to run the app and review the error.\nWatch out for commas!"
  },
  {
    "objectID": "ShinyWorkshop/Shiny1/Shiny1.html#building-shiny-application-survival-tip",
    "href": "ShinyWorkshop/Shiny1/Shiny1.html#building-shiny-application-survival-tip",
    "title": "Building Web-enabled Geospatial Analytics Applications with Shiny: Shiny basic",
    "section": "Building Shiny Application: Survival Tip!",
    "text": "Building Shiny Application: Survival Tip!\n\n\n\nWhat can we learn from Lego?\n\n\n\n\nSketch the storyboard\nBuilding the app incrementally\n\nUsing prototyping approach\nStart as simple as possible\nAdding features one at a time\n\nSave -&gt; Run App"
  },
  {
    "objectID": "ShinyWorkshop/Shiny1/Shiny1.html#references",
    "href": "ShinyWorkshop/Shiny1/Shiny1.html#references",
    "title": "Building Web-enabled Geospatial Analytics Applications with Shiny: Shiny basic",
    "section": "References",
    "text": "References\n\nHadley Wickham (2021) Mastering Shiny, O’Reilly Media. This is a highly recommended book.\nBuilding Web Applications with Shiny, especially Module 1 and 2.\nShiny Three Parts Tutorial.\nOnline Function reference\nThe basic parts of a Shiny app\nHow to build a Shiny app\nThe Shiny Cheat sheet\n\nBeyond Uncle Google! Last but not least, when you need help\n\nHow to get help"
  },
  {
    "objectID": "R.html",
    "href": "R.html",
    "title": "R Resources",
    "section": "",
    "text": "This page provides links to a selected resources to learn modern R packages, especially those related to tidyverse framework."
  },
  {
    "objectID": "R.html#getting-started-with-r",
    "href": "R.html#getting-started-with-r",
    "title": "R Resources",
    "section": "Getting Started with R",
    "text": "Getting Started with R\n\nR for Data Science by Garrett Grolemund and Hadley Wickham.\nModern R with the tidyverse by Bruno Rodrigues. Chapter 2 provides a detail discussion on R data objects.\nBrendan R. E. Ansell Introduction to R - tidyverse\nThe Comprehensive Guide to Installing R Packages from CRAN, Bioconductor, GitHub and Co.. This article provides useful tips on how to install R packages from different sources.\nR Workflow"
  },
  {
    "objectID": "R.html#blog-post",
    "href": "R.html#blog-post",
    "title": "R Resources",
    "section": "Blog post",
    "text": "Blog post\n\nR Workflow from Statistical Thinking."
  },
  {
    "objectID": "outline/Lesson12_outline.html",
    "href": "outline/Lesson12_outline.html",
    "title": "Lesson 12: Geographically Weighted Predictive Modelling",
    "section": "",
    "text": "What is Predictive Modelling?\nWhat is Geospatial Predictive Modelling\nIntroducing Recursive Partitioning\nAdvanced Recursive Partitioning: Random Forest\nIntroducing Geographically Weighted Random Forest"
  },
  {
    "objectID": "outline/Lesson12_outline.html#content",
    "href": "outline/Lesson12_outline.html#content",
    "title": "Lesson 12: Geographically Weighted Predictive Modelling",
    "section": "",
    "text": "What is Predictive Modelling?\nWhat is Geospatial Predictive Modelling\nIntroducing Recursive Partitioning\nAdvanced Recursive Partitioning: Random Forest\nIntroducing Geographically Weighted Random Forest"
  },
  {
    "objectID": "outline/Lesson12_outline.html#lesson-slides-and-hands-on-notes",
    "href": "outline/Lesson12_outline.html#lesson-slides-and-hands-on-notes",
    "title": "Lesson 12: Geographically Weighted Predictive Modelling",
    "section": "Lesson Slides and Hands-on Notes",
    "text": "Lesson Slides and Hands-on Notes\n\nLesson 12 slides.\nHandout of Hands-on Exercise 12, excluding Section 14.8.\n\n\nSelf-reading Before Meet-up\nTo read before class:\n\nStefanos Georganos et. al. (2019) “Geographical Random Forests: A Spatial Extension of the Random Forest Algorithm to Address Spatial Heterogeneity in Remote Sensing and Population Modelling”, Geocarto International, DOI: 10.1080/10106049.2019.1595177.\nGeorganos, S. and Kalogirou, S. (2022) “A Forest of Forests: A Spatially Weighted and Computationally Efficient Formulation of Geographical Random Forests”. ISPRS, International Journal of Geo-Information, 2022, 11, 471. https://www.mdpi.com/2220-9964/11/9/471\n\n\n\nReferences\n\nGeorge Grekousis et. al. (2022) “Ranking the importance of demographic, socioeconomic, and underlying health factors on US COVID-19 deaths: A geographical random forest approach”, Health and Place, vol. 74, pp. 1-12.\nYaowen Luo, Jianguo Yan & Stephen McClure. (2020) “Distribution of the environmental and socioeconomic risk factors on COVID-19 death rate across continental USA: a spatial nonlinear analysis”, Environmental Science and Pollution Research, 28:6587–6599.\nEun-Hee Koh, Eunhee Lee, & Kang-Kun Lee (2020) “Application of geographically weighted regression models to predict spatial characteristics of nitrate contamination: Implications for an effective groundwater management strategy”, Journal of Environmental Management. Vol. 268\n\n\n\nAll About R\n\nSpatialML"
  },
  {
    "objectID": "outline/Lesson12_outline.html#self-reading-before-meet-up",
    "href": "outline/Lesson12_outline.html#self-reading-before-meet-up",
    "title": "Lesson 11: Spatial Interaction Models",
    "section": "Self-reading Before Meet-up",
    "text": "Self-reading Before Meet-up\nRead before lesson:\n\nKingsley E. Haynes and A. Stewart Fotheringham (2020) Gravity and Spatial Interaction Models, Web Book of Regional Science. Chapter 1 and Sub-section 2.2.\nFarmer, C. and Oshan, T. (2017). “Spatial interaction”. The Geographic Information Science & Technology Body of Knowledge (4th Quarter 2017 Edition), John P. Wilson (ed.). DOI: 10.22224/gistbok/2017.4.5 (link is external)\nFlowerdew R, Aitkin M (1982) “A method of fitting the gravity model based on the Poisson distribution”. Journal of Regional Science, 22: 191–202.\n” Chapter 6. Spatial econometrics - common models”, Handbook of Spatial Analysis.\nJames P. LeSage and R. Kelley Pace (2008) “Spatial Econometric Modeling of Origin-Destination Flows”. Journal of Regional Science, Vol. 48, No. 5, pp. 941-967."
  },
  {
    "objectID": "outline/Lesson12_outline.html#reference",
    "href": "outline/Lesson12_outline.html#reference",
    "title": "Lesson 11: Spatial Interaction Models",
    "section": "Reference",
    "text": "Reference\n\nPavel KLAPKA, et. al. (2013) “The Footfall of Shopping Centre in Olomouc (CZECH REPUBLIC): An Application of The Gravity Model”, Moravian Geographical Reports, Vol 21, pp. 12-26.\n\nYang Yue et. al. (2012) “Exploratory calibration of a spatial interaction model using taxi GPS trajectories”, Computers, Environment and Urban Systems, 36 (2012) 140–153.\nMorito Tsutsumi and Kazuki Tamesue (2012) “Intraregional flow problem in spatial econometric model for origin-destination flows”, Environment and Planning B: Planning and Design, Vol. 39, pp. 1006-1015. Access via SMU e-journal."
  },
  {
    "objectID": "outline/Lesson12_outline.html#all-about-r",
    "href": "outline/Lesson12_outline.html#all-about-r",
    "title": "Lesson 11: Spatial Interaction Models",
    "section": "All About R:",
    "text": "All About R:\n\nPaul Roback and Julie Legler (2020) Beyond Multiple Linear Regression: Applied Generalized Linear Models and Multilevel Models in R, CRC Press. On line version (January 26, 2021). Chapter 4\nNegative Binomial Regression and Poisson Regression from UCLA Institute for Digital Research & Education.\nspflow\nHome-to-work commuting flows within the municipalities around Paris"
  },
  {
    "objectID": "outline/Lesson10_outline.html",
    "href": "outline/Lesson10_outline.html",
    "title": "Lesson 10: Regression Modelling of Geographically Reference Data",
    "section": "",
    "text": "In this lesson, you will learn the basic concepts and methods of simple and multiple linear regression and their applications on geographically referenced data."
  },
  {
    "objectID": "outline/Lesson10_outline.html#content",
    "href": "outline/Lesson10_outline.html#content",
    "title": "Lesson 10: Regression Modelling of Geographically Reference Data",
    "section": "Content",
    "text": "Content\n\nBasic concepts and principles of linear regression\n\nSimple linear regression\nMultiple linear regression\n\nRegression Diagnostics\n\nLinearity: The relationship between X and the mean of Y is linear.\nHomoscedasticity: The variance of residual is the same for any value of X.\nIndependence: Observations are independent of each other.\nNormality: For any fixed value of X, Y is normally distributed.\n\nThe spatial stationarity assumption of multivariate regression."
  },
  {
    "objectID": "outline/Lesson10_outline.html#lesson-slides-and-hands-on-notes",
    "href": "outline/Lesson10_outline.html#lesson-slides-and-hands-on-notes",
    "title": "Lesson 10: Regression Modelling of Geographically Reference Data",
    "section": "Lesson Slides and Hands-on Notes",
    "text": "Lesson Slides and Hands-on Notes\n\nLesson 10 slides.\nHands-on Exercise 8. Section 13.1-13.8 only."
  },
  {
    "objectID": "outline/Lesson08_outline.html#content",
    "href": "outline/Lesson08_outline.html#content",
    "title": "Lesson 8: Hierarchical Clustering for Multivariate Geographically Referenced Data",
    "section": "Content",
    "text": "Content\n\nBasic concepts of clustering analysis\nHierarchical clustering methods\n\nProximity matrices\nAgglomerative hierarchical clustering algorithms\n\nHierarchical clustering processes and best practices"
  },
  {
    "objectID": "outline/Lesson08_outline.html#lesson-slides",
    "href": "outline/Lesson08_outline.html#lesson-slides",
    "title": "Lesson 8: Hierarchical Clustering for Multivariate Geographically Referenced Data",
    "section": "Lesson slides",
    "text": "Lesson slides\n\nLesson 8: Hierarchical Clustering for Multivariate Geographically Referenced Data slides."
  },
  {
    "objectID": "outline/Lesson08_outline.html#hands-on-exercise",
    "href": "outline/Lesson08_outline.html#hands-on-exercise",
    "title": "Lesson 8: Hierarchical Clustering for Multivariate Geographically Referenced Data",
    "section": "Hands-on Exercise",
    "text": "Hands-on Exercise\n\n12 Geographical Segmentation with Spatially Constrained Clustering Techniques, Section 12.1-12.7 only."
  },
  {
    "objectID": "outline/Lesson06_outline.html",
    "href": "outline/Lesson06_outline.html",
    "title": "Lesson 6: Global and Local Measures of Spatial Autocorrelation",
    "section": "",
    "text": "In this lesson, you will learn a collection of geospatial statistical methods specially designed for measuring global and local spatial association.\nThese spatial statistics are well suited for:"
  },
  {
    "objectID": "outline/Lesson06_outline.html#content",
    "href": "outline/Lesson06_outline.html#content",
    "title": "Lesson 6: Global and Local Measures of Spatial Autocorrelation",
    "section": "Content",
    "text": "Content\n\nWhat is Spatial Autocorrelation\n\nMeasures of Global Spatial Autocorrelation\nMeasures of Global High/Low Clustering\n\nIntroducing Localised Geospatial Analysis\n\nLocal Indicators of Spatial Association (LISA)\n\nCluster and Outlier Analysis\n\nLocal Moran and Local Geary\nMoran scatterplot\nLISA Cluster Map\n\nHot Spot and Cold Spot Areas Analysis\n\nGetis and Ord’s G-statistics"
  },
  {
    "objectID": "outline/Lesson06_outline.html#lesson-slides-and-hands-on-notes",
    "href": "outline/Lesson06_outline.html#lesson-slides-and-hands-on-notes",
    "title": "Lesson 6: Global and Local Measures of Spatial Autocorrelation",
    "section": "Lesson Slides and Hands-on Notes",
    "text": "Lesson Slides and Hands-on Notes\n\nLesson 6 slides.\nHands-on Exercise 6: Global Measures of Spatial Autocorrelation.\nHands-on Exercise 6: Local Measures of Spatial Autocorrelation"
  },
  {
    "objectID": "outline/Lesson06_outline.html#self-reading-before-meet-up",
    "href": "outline/Lesson06_outline.html#self-reading-before-meet-up",
    "title": "Lesson 6: Global and Local Measures of Spatial Autocorrelation",
    "section": "Self-reading Before Meet-up",
    "text": "Self-reading Before Meet-up\nTo read before class:\n\nMoran, P. A. P. (1950). “Notes on Continuous Stochastic Phenomena”. Biometrika. 37 (1): 17–23.\nGeary, R.C. (1954) “The Contiguity Ratio and Statistical Mapping”. The Incorporated Statistician, Vol. 5, No. 3, pp. 115-127.\nGetis, A., & Ord, K. (1992). “The Analysis of Spatial Association by Use of Distance Statistics”. Geographical Analysis, 24, 189–206.\nAnselin, L. (1995). “Local indicators of spatial association – LISA”. Geographical Analysis, 27(4): 93-115.\nGetis, A. and Ord, J.K. (1992) “The analysis of spatial association by use of distance statistics”. Geographical Analysis, 24(3): 189-206.\nOrd, J.K. and Getis, A. (2010) “Local spatial autocorrelation statistics: Distributional issues and an application”. Geographical Analysis, 27(4): 286-306.\n\nThese six papers are classics of Global and Local Spatial Autocorrelation. Be warned: All classic papers assume that the readers are academic researchers."
  },
  {
    "objectID": "outline/Lesson06_outline.html#references",
    "href": "outline/Lesson06_outline.html#references",
    "title": "Lesson 6: Global and Local Measures of Spatial Autocorrelation",
    "section": "References",
    "text": "References\n\nD. A. Griffith (2009) “Spatial autocorrelation”.\nGetis, A., 2010 “B.3 Spatial Autocorrelation” in Fischer, M.M., and Getis, A. 2010 Handbook of Applied Spatial Analysis: Software Tools, Methods and Applications, Springer.\nAnselin, L. (1996) “The Moran scatterplot as an ESDA tool to assess local instability in spatial association”\nGriffith, Daniel (2009) “Modeling spatial autocorrelation in spatial interaction data: empirical evidence from 2002 Germany journey-to-work flows”. Journal of Geographical Systems, Vol.11(2), pp.117-140.\nCelebioglu, F., and Dall’erba, S. (2010) “Spatial disparities across the regions of Turkey: An exploratory spatial data analysis”. The Annals of Regional Science, 45:379–400.\nMack, Z.W.V. and Kam T.S. (2018) “Is There Space for Violence?: A Data-driven Approach to the Exploration of Spatial-Temporal Dimensions of Conflict” Proceedings of 2nd ACM SIGSPATIAL Workshop on Geospatial Humanities (ACM SIGSPATIAL’18). Seattle, Washington, USA, 10 pages.\nTAN, Yong Ying and KAM, Tin Seong (2019). “Exploring and Visualizing Household Electricity Consumption Patterns in Singapore: A Geospatial Analytics Approach”, Information in Contemporary Society: 14th International Conference, iConference 2019, Washington, DC, USA, March 31–April 3, 2019, Proceedings. Pp 785-796.\nSingh A., Pathak P.K., Chauhan R.K., and Pan, W. (2011) “Infant and Child Mortality in India in the Last Two Decades: A Geospatial Analysis”. PLoS ONE 6(11), 1:19."
  },
  {
    "objectID": "outline/Lesson05a_outline.html",
    "href": "outline/Lesson05a_outline.html",
    "title": "Lesson 6: Emerging Hot Spot Analysis",
    "section": "",
    "text": "Emerging Hot Spot Analysis (EHSA) is a spatio-temporal analysis method for revealing and describing how hot spot and cold spot areas evolve over time. The analysis consist of four main steps:\nIn this lesson, you will learn the basic concepts of time-space cube and EHSA. You will also gain hands-on experience of how to build a time-spce cube and to perform EHSA by using sfdep package."
  },
  {
    "objectID": "outline/Lesson05a_outline.html#content",
    "href": "outline/Lesson05a_outline.html#content",
    "title": "Lesson 6: Emerging Hot Spot Analysis",
    "section": "Content",
    "text": "Content\n\nWhat is Spatial Autocorrelation\n\nMeasures of Global Spatial Autocorrelation\nMeasures of Global High/Low Clustering\n\nIntroducing Localised Geospatial Analysis\n\nLocal Indicators of Spatial Association (LISA)\n\nCluster and Outlier Analysis\n\nLocal Moran and Local Geary\nMoran scatterplot\nLISA Cluster Map\n\nHot Spot and Cold Spot Areas Analysis\n\nGetis and Ord’s G-statistics\n\nCase Studies"
  },
  {
    "objectID": "outline/Lesson05a_outline.html#lesson-slides-and-hands-on-notes",
    "href": "outline/Lesson05a_outline.html#lesson-slides-and-hands-on-notes",
    "title": "Lesson 6: Emerging Hot Spot Analysis",
    "section": "Lesson Slides and Hands-on Notes",
    "text": "Lesson Slides and Hands-on Notes\n\nLesson 5 slides.\nHands-on Exercise 5: Global Measures of Spatial Autocorrelation.\nHands-on Exercise 5: Local Measures of Spatial Autocorrelation"
  },
  {
    "objectID": "outline/Lesson05a_outline.html#self-reading-before-meet-up",
    "href": "outline/Lesson05a_outline.html#self-reading-before-meet-up",
    "title": "Lesson 6: Emerging Hot Spot Analysis",
    "section": "Self-reading Before Meet-up",
    "text": "Self-reading Before Meet-up\nTo read before class:\n\nMoran, P. A. P. (1950). “Notes on Continuous Stochastic Phenomena”. Biometrika. 37 (1): 17–23.\nGeary, R.C. (1954) “The Contiguity Ratio and Statistical Mapping”. The Incorporated Statistician, Vol. 5, No. 3, pp. 115-127.\nGetis, A., & Ord, K. (1992). “The Analysis of Spatial Association by Use of Distance Statistics”. Geographical Analysis, 24, 189–206.\nAnselin, L. (1995). “Local indicators of spatial association – LISA”. Geographical Analysis, 27(4): 93-115.\nGetis, A. and Ord, J.K. (1992) “The analysis of spatial association by use of distance statistics”. Geographical Analysis, 24(3): 189-206.\nOrd, J.K. and Getis, A. (2010) “Local spatial autocorrelation statistics: Distributional issues and an application”. Geographical Analysis, 27(4): 286-306.\n\nThese six papers are classics of Global and Local Spatial Autocorrelation. Be warned: All classic papers assume that the readers are academic researchers."
  },
  {
    "objectID": "outline/Lesson05a_outline.html#references",
    "href": "outline/Lesson05a_outline.html#references",
    "title": "Lesson 6: Emerging Hot Spot Analysis",
    "section": "References",
    "text": "References\n\nD. A. Griffith (2009) “Spatial autocorrelation”.\nGetis, A., 2010 “B.3 Spatial Autocorrelation” in Fischer, M.M., and Getis, A. 2010 Handbook of Applied Spatial Analysis: Software Tools, Methods and Applications, Springer.\nAnselin, L. (1996) “The Moran scatterplot as an ESDA tool to assess local instability in spatial association”\nGriffith, Daniel (2009) “Modeling spatial autocorrelation in spatial interaction data: empirical evidence from 2002 Germany journey-to-work flows”. Journal of Geographical Systems, Vol.11(2), pp.117-140.\nCelebioglu, F., and Dall’erba, S. (2010) “Spatial disparities across the regions of Turkey: An exploratory spatial data analysis”. The Annals of Regional Science, 45:379–400.\nMack, Z.W.V. and Kam T.S. (2018) “Is There Space for Violence?: A Data-driven Approach to the Exploration of Spatial-Temporal Dimensions of Conflict” Proceedings of 2nd ACM SIGSPATIAL Workshop on Geospatial Humanities (ACM SIGSPATIAL’18). Seattle, Washington, USA, 10 pages.\nTAN, Yong Ying and KAM, Tin Seong (2019). “Exploring and Visualizing Household Electricity Consumption Patterns in Singapore: A Geospatial Analytics Approach”, Information in Contemporary Society: 14th International Conference, iConference 2019, Washington, DC, USA, March 31–April 3, 2019, Proceedings. Pp 785-796.\nSingh A., Pathak P.K., Chauhan R.K., and Pan, W. (2011) “Infant and Child Mortality in India in the Last Two Decades: A Geospatial Analysis”. PLoS ONE 6(11), 1:19."
  },
  {
    "objectID": "outline/Lesson03_outline.html",
    "href": "outline/Lesson03_outline.html",
    "title": "Lesson 3: Spatial Point Patterns Analysis",
    "section": "",
    "text": "Spatial Point Pattern Analysis is the evaluation of the pattern, or distribution, of a set of points on a surface. It can refer to the actual spatial or temporal location of these points or also include data from point sources. It is one of the most fundamental concepts in geography and spatial analysis. This lesson aims to share with you the basic concepts and methods of Spatial Point Pattern Analysis. You will also gain hands experience on using spatstat, an R package specially designed for Spatial Point Pattern Analysis."
  },
  {
    "objectID": "outline/Lesson03_outline.html#content",
    "href": "outline/Lesson03_outline.html#content",
    "title": "Lesson 3: Spatial Point Patterns Analysis",
    "section": "Content",
    "text": "Content\n\nIntroducing Spatial Point Patterns\n\nThe basic concepts of spatial point patterns\nSpatial Point Patterns in real world\n\n1st Order Spatial Point Patterns Analysis\n\nKernel Density Estimation (KDE)\n\n2nd Order Spatial Point Patterns Analysis (SPPA)\n\nBasic concepts of 2nd order spatial point patterns\n2nd order SPPA methods: J, F, K and L functions"
  },
  {
    "objectID": "outline/Lesson03_outline.html#lesson-slides",
    "href": "outline/Lesson03_outline.html#lesson-slides",
    "title": "Lesson 3: Spatial Point Patterns Analysis",
    "section": "Lesson Slides",
    "text": "Lesson Slides\n\nLesson 3 slides."
  },
  {
    "objectID": "outline/Lesson03_outline.html#hands-on-exercise",
    "href": "outline/Lesson03_outline.html#hands-on-exercise",
    "title": "Lesson 3: Spatial Point Patterns Analysis",
    "section": "Hands-on Exercise",
    "text": "Hands-on Exercise\n\nChapter 4: 1st Order Spatial Point Patterns Analysis Methods\nChapter 5: 2nd Order Spatial Point Patterns Analysis Methods\n\n\nSelf-reading Before Meet-up\n\nChapter 7 Spatial Point Pattern Analysis of Roger S. Bivand, Edzer Pebesma and Virgilio Gómez-Rubio (2013) Applied Spatial Data Analysis with R (2nd Edition), Springer.\nYuan, Y., Qiang, Y., Bin Asad, K., and Chow, T. E. (2020). Point Pattern Analysis. The Geographic Information Science & Technology Body of Knowledge (1st Quarter 2020 Edition), John P. Wilson (ed.). DOI: 10.22224/gistbok/2020.1.13.\nYin, P. (2020). Kernels and Density Estimation. The Geographic Information Science & Technology Body of Knowledge (1st Quarter 2020 Edition), John P. Wilson (ed.). DOI: 10.22224/gistbok/2020.1.12\n\n\n\nEnrichment Resources\nProf. Luc Anselin on point pattern analysis (YouTube):\n\nPoint Pattern Analysis Concepts\nPoint Pattern Analysis: Clustered, Regular and Dispersed Patterns\nPoint Pattern Analysis: Nearest Neighbor Statistics\nPoint Pattern Analysis: Quadrat Counts\nPoint Pattern Analysis: F and J Functions\nPoint Pattern Analysis: K, L and Kd Functions"
  },
  {
    "objectID": "outline/Lesson03_outline.html#references",
    "href": "outline/Lesson03_outline.html#references",
    "title": "Lesson 3: Spatial Point Patterns Analysis",
    "section": "References",
    "text": "References\n\nO’Sullivan, D., and Unwin, D. (2010) Geographic Information Analysis, Second Edition. John Wiley & Sons Inc., New Jersey, Canada. Chapter 5-6.\nBaddeley A., Rubak E. and Turner R. (2015) Spatial Point Patterns: Methodology and Applications with R, Chapman and Hall/CRC.\nChapter 11 Point Pattern Analysis of Intro to GIS and Spatial Analysis. Section 11.2, 11.3, 11.3.1 and 11.4 • Ripley’s K-function.\nATSUYUKI OKABE, TOSHIAKI SATOH & KOKICHI SUGIHARA (2009) “A kernel density estimation method for networks, its computational method and a GIS-based tool”, International Journal of Geographical Information Science, Vol. 23, No. 1, January 2009, pp. 7–32.\nIkuho Yamada & Jean-Claude Thill (2007) “Local Indicators of Network-Constrained Clusters in Spatial Point Patterns”, Geographical Analysis, Vol. 39, pp 268–292.\nJérémy Gelb & Philippe Apparicio (2023) “Temporal Network Kernel Density Estimation”, Geographical Analysis. (Online open access version)\n\n\nApplications\n\nNaveen Donthu and Roland T. Rust (1989) “Estimating Geographic Customer Densities Using Kernel Density Estimation”, Marketing Science, Vol. 8, No. 2, pp. 191-203.\nJoseph Wartman and Nicholas E. Malasavage (2010). “Spatial Analysis for Identifying Concentrations of Urban Damage” in Methods and Techniques in Urban Engineering, Armando Carlos de Pina Filho and Aloisio Carlos dePina (Ed.), ISBN: 978-953-307-096-4, InTech.\nGiuseppe Borruso and Andrea Porceddu (2009) “A Tale of Two Cities: Density Analysis of CBD on Two Midsize Urban Areas in Northeastern Italy” in Murgante, Beniamino; Borruso, Giuseppe & Lapucci, Alessandra (2009) Studies in Computational Intelligence, Geocomputation and Urban Planning, pp.37-56.\nKang, Youngok ; Cho, Nahye ; Son, Serin; Chen, Peng (2018) “Spatiotemporal characteristics of elderly population’s traffic accidents in Seoul using space-time cube and space-time kernel density estimation”, PloS one, 2018, Vol.13 (5)."
  },
  {
    "objectID": "outline/Lesson03_outline.html#all-about-r",
    "href": "outline/Lesson03_outline.html#all-about-r",
    "title": "Lesson 3: Spatial Point Patterns Analysis",
    "section": "All About R",
    "text": "All About R\n\nspatstat at R Cran\n\nspatstat resource."
  },
  {
    "objectID": "outline/Lesson01_outline.html",
    "href": "outline/Lesson01_outline.html",
    "title": "Lesson 1: Introduction to Geospatial Analytics",
    "section": "",
    "text": "This lesson consists of two parts. First, it provides an overview of geospatial analytics. Second, R objects used to import, integrate, wrangle, process geospatial data will be discussed. The discussion will focus on sfpackage. Other R packages for storing (i.e. sp), transforming (i.e. rgdal), and processing (i.e. rgeos) geospatial data will be discussed briefly too."
  },
  {
    "objectID": "outline/Lesson01_outline.html#overview",
    "href": "outline/Lesson01_outline.html#overview",
    "title": "Lesson 1: Introduction to Geospatial Analytics",
    "section": "",
    "text": "This lesson consists of two parts. First, it provides an overview of geospatial analytics. Second, R objects used to import, integrate, wrangle, process geospatial data will be discussed. The discussion will focus on sfpackage. Other R packages for storing (i.e. sp), transforming (i.e. rgdal), and processing (i.e. rgeos) geospatial data will be discussed briefly too."
  },
  {
    "objectID": "outline/Lesson01_outline.html#content",
    "href": "outline/Lesson01_outline.html#content",
    "title": "Lesson 1: Introduction to Geospatial Analytics",
    "section": "Content",
    "text": "Content\n\nIntroduction to Geospatial Analytics\n\nDemystifying Geospatial Analytics\nMotivation of Geospatial Analytics\nA Tour Through the Geospatial Analytics Zoo\nGeospatial Analytics and Social Consciousness\n\nGeospatial Data Science\n\nAn overview of Geospatial Data Models\nMap Projection and Georeferencing\nGeocoding\nClasses of Spatial Data in R: Simple features class"
  },
  {
    "objectID": "outline/Lesson01_outline.html#lesson-slides",
    "href": "outline/Lesson01_outline.html#lesson-slides",
    "title": "Lesson 1: Introduction to Geospatial Analytics",
    "section": "Lesson Slides",
    "text": "Lesson Slides\n\nLesson 1: Introduction to Geospatial Analytics slides.\nLesson 1: Geospatial Data Science with R slides."
  },
  {
    "objectID": "outline/Lesson01_outline.html#hands-on-exercise",
    "href": "outline/Lesson01_outline.html#hands-on-exercise",
    "title": "Lesson 1: Introduction to Geospatial Analytics",
    "section": "Hands-on Exercise",
    "text": "Hands-on Exercise\n\nHands-on Exercise 1: Geospatial Data Wrangling with R"
  },
  {
    "objectID": "outline/Lesson01_outline.html#references",
    "href": "outline/Lesson01_outline.html#references",
    "title": "Lesson 1: Introduction to Geospatial Analytics",
    "section": "References",
    "text": "References\n\n“Spatial Data, Spatial Analysis, Spatial Data Science” by Prof. Luc Anselin. (This is a long lecture 1hr 15minutes but don’t turn away just because it is lengthy.)\nXie, Yiqun et. al. (2017) “Transdisciplinary Foundations of Geospatial Data Science” ISPRS International Journal of Geo-information, 2017, Vol.6 (12), p.395.\nPaez, A., and Scott, D.M. (2004) “Spatial statistics for urban analysis: A review of techniques with examples”, GeoJournal, 61: 53-67. Available in SMU eLibrary.\n“Geospatial Analytics Will Eat The World, And You Won’t Even Know It”."
  },
  {
    "objectID": "outline/Lesson01_outline.html#r-packages-for-data-science",
    "href": "outline/Lesson01_outline.html#r-packages-for-data-science",
    "title": "Lesson 1: Introduction to Geospatial Analytics",
    "section": "R packages for Data Science",
    "text": "R packages for Data Science\n\nsf package.\n\nSimple Features for R\nReading, Writing and Converting Simple Features\nManipulating Simple Feature Geometries\nManipulating Simple Features\n\ntidyverse: a family of modern R packages specially designed to meet the tasks of Data Science in R.\n\nreadr: a fast and effective library to parse csv, txt, and tsv files as tibble data.frame in R. To get started, refer to Chapter 11 Data import of R for Data Science book.\n\ntidyr: an R package for tidying data. To get started, refer to Chapter 5 Data tidying of R for Data Science book.\n\ndplyr: a grammar of data manipulation. To get started, read articles under Getting Started and Articles tabs.\nggplot2: a grammar of graphics. To get started, read Chapter 1: Data Visualization, Chapter 10 Exploratory Data Analysis and Chapter 11 Communication of R for Data Science (2ed) book.\npipes: a powerful tool for clearly expressing a sequence of multiple operations. To get started, read Chapter 5 Workflow: pipes of R for Data Science (2ed) book."
  },
  {
    "objectID": "lesson/Lesson03/Lesson03-SPPA.html#content",
    "href": "lesson/Lesson03/Lesson03-SPPA.html#content",
    "title": "Lesson 3: Spatial Point Patterns Analysis",
    "section": "",
    "text": "Introducing Spatial Point Patterns\n\nThe basic concepts of spatial point patterns\n1st Order versus 2nd Order\nSpatial Point Patterns in real world\n\n1st Order Spatial Point Patterns Analysis\n\nQuadrat analysis\nKernel density estimation\n\n\n\n\n2nd Order Spatial Point Patterns Analysis\n\nNearest Neighbour Index\nG-function\nF-function\nK-function\nL-function\n\n\n\n\n\nWelcome to Lesson 4: Spatial Point Pattern Analysis, a family of spatial statistics specially developed to model and to test distribution of spatial point events.\nThe lesson proceeding is divided into three main section. First, I will share with you what are real world spatial point events. This is followed by explaining the concepts and methods of 1st-order and 2nd-order spatial point patterns analysis."
  },
  {
    "objectID": "lesson/Lesson03/Lesson03-SPPA.html#what-is-spatial-point-patterns",
    "href": "lesson/Lesson03/Lesson03-SPPA.html#what-is-spatial-point-patterns",
    "title": "Lesson 3: Spatial Point Patterns Analysis",
    "section": "What is Spatial Point Patterns",
    "text": "What is Spatial Point Patterns\n\nPoints as Events\nMapped pattern\n\nNot a sample\nSelection bias\n\nEvents are mapped, but non-events are not\n\n\n\nSpatial Point Patterns in Real World\n\nDistribution of dieses such as dengue fever.\n\n\n\n\n\nSpatial Point Patterns in Real World\n\nDistribution of car collisions.\n\n\n\n\n\n\n\n\n\nSpatial Point Patterns in Real World\n\nDistribution of crime incidents.\n\n\n\n\n\n\n\n\n\nSpatial Point Patterns in Real World\n\nDistribution of public services such as education institutions.\n\n\n\n\n\nSpatial Point Patterns in Real World\n\nLocations of the different channel stores.\n\n\n\n\n\n\n\n\n\nSpatial Point Patterns in Real World\n\nDistribution of social media data such as tweets."
  },
  {
    "objectID": "lesson/Lesson03/Lesson03-SPPA.html#real-world-question",
    "href": "lesson/Lesson03/Lesson03-SPPA.html#real-world-question",
    "title": "Lesson 3: Spatial Point Patterns Analysis",
    "section": "Real World Question",
    "text": "Real World Question\n\nLocation only\n\nare points randomly located or patterned\n\nLocation and value\n\nmarked point pattern\nis combination of location and value random or patterned\n\nWhat is the underlying process?\n\n\nIt is important note that SPPA is exploratory and confirmatory in nature. They are specially developed for describing the spatial point pattern and for confirming the observed patterns statistically. However, they are explanatory nor for prediction."
  },
  {
    "objectID": "lesson/Lesson03/Lesson03-SPPA.html#points-on-a-plane",
    "href": "lesson/Lesson03/Lesson03-SPPA.html#points-on-a-plane",
    "title": "Lesson 3: Spatial Point Patterns Analysis",
    "section": "Points on a Plane",
    "text": "Points on a Plane\n\nClassic point pattern analysis\n\npoints on an isotropic plane\nno effect of translation and rotation\nclassic examples: tree seedlings, rocks, etc\n\nDistance\n\nstraight line only\n\n\n\n\nReal world spatial point patterns\n\nIs this a random distribution?\n\n\n\n\n\nReal world spatial point patterns\n\nIs this a random distribution?"
  },
  {
    "objectID": "lesson/Lesson03/Lesson03-SPPA.html#spatial-point-patterns-analysis",
    "href": "lesson/Lesson03/Lesson03-SPPA.html#spatial-point-patterns-analysis",
    "title": "Lesson 3: Spatial Point Patterns Analysis",
    "section": "Spatial Point Patterns Analysis",
    "text": "Spatial Point Patterns Analysis\n\nPoint pattern analysis (PPA) is the study of the spatial arrangements of points in (usually 2-dimensional) space.\nThe simplest formulation is a set X = {x ∈ D} where D, which can be called the study region, is a subset of Rn, a n-dimensional Euclidean space.\nA fundamental problem of PPA is inferring whether a given arrangement is merely random or the result of some process."
  },
  {
    "objectID": "lesson/Lesson03/Lesson03-SPPA.html#spatial-point-patterns-analysis-techniques",
    "href": "lesson/Lesson03/Lesson03-SPPA.html#spatial-point-patterns-analysis-techniques",
    "title": "Lesson 3: Spatial Point Patterns Analysis",
    "section": "Spatial Point Patterns Analysis Techniques",
    "text": "Spatial Point Patterns Analysis Techniques\n\nFirst-order vs Second-order Analysis of spatial point patterns.\n\n\nReference: 11.4 First and second order effects of Intro to GIS and Spatial Analysis\n\nThe first-order properties describe the way in which the expected value (mean or average) of the spatial point pattern varies across space (i.e., the intensity of the spatial point pattern). Such properties are usually measured with the so-called quadrat analysis, nearest neighbour index and kernel estimation. Second-order properties describe the covariance (or correlation) between values of the spatial point pattern at different regions in space and are usually measured with the G function, K function and L function. Applied to point event data, both properties could be used to explore the spatial variation in the risk of being victimized by a crime, spatial and space-time clustering of criminal activities, and the raised incidence of criminal activities around point sources, such as robberies around ATM machines, subway entrances and exits, etc.\n\n\n\nFirst-order Spatial Point Patterns Analysis Techniques\n\nDensity-based\n\nKernel density estimation\nQuadrat analysis,\n\nDistance-based\n\nNearest Neighbour Index\n\n\n\n\n\nBasic concept of density-based measures"
  },
  {
    "objectID": "lesson/Lesson03/Lesson03-SPPA.html#kernel-density-estimation-silverman-1986",
    "href": "lesson/Lesson03/Lesson03-SPPA.html#kernel-density-estimation-silverman-1986",
    "title": "Lesson 3: Spatial Point Patterns Analysis",
    "section": "Kernel density estimation (Silverman 1986)",
    "text": "Kernel density estimation (Silverman 1986)\n\nA method to compute the intensity of a point distribution.\n\n\n\nThe general formula:\n\n\nGraphically\n\n\n\n\n\nKDE Step 1: Computing point intensity\n\n\n\n\nKDE Step 2: Spatial interpolation using kernel function\n\n\n\n\nKDE Map of Childcare Services, Singapore\n\n\n\n\nAdaptive Bandwidth\n\n\nAdaptive schemes adjust itself according to the density of data: - Shorter bandwidths where data are dense and longer where sparse.\n\nFinding nearest neighbors are one of the often used approaches.\n\n\n\n\n\n\n\n\nFixed bandwidth\n\n\n\nMight produce large estimate variances where data are sparse, while mask subtle local variations where data are dense.\nIn extreme condition, fixed schemes might not be able to calibrate in local areas where data are too sparse to satisfy the calibration requirements (observations must be more than parameters).\n\n\n\n\n\n\n\n\nQuadrat Analysis – Step 1\n\n\n\nDivide the study area into subregion of equal size,\n\noften squares, but don’t have to be.\n\n\n\n\n\n\n\n\nQuadrat Analysis – Step 2\n\n\n\nCount the frequency of events in each region.\n\n\n\n\n\n\n\nQuadrat Analysis – Step 3\n\n\n\nCalculate the intensity of events in each region.\n\n\n\n\n\n\n\nQuadrat Analysis – Step 4\n\nCalculate the quadrat statistics and perform CSR test.\n\n\n\n\n\nQuadrat Analysis – Variance-Mean Ratio (VMR)\n\nFor an uniform distribution, the variance is zero, - therefore, we expect a variance-mean ratio close to 0.\nFor a random distribution, the variance and mean are the same,\n\ntherefore, we expect a variance-mean ratio close to 1.\n\nFor a cluster distribution, the variance is relatively large,\n\ntherefore, we expect a variance-mean ratio greater than 1.]\n\n\n\n\n\nComplete Spatial Randomness (CSR)\n\n\n\nCSR/IRP satisfy two conditions: - Any event has equal probability of being in any location, a 1st order effect.\n\nThe location of one event is independent of the location of another event, a 2nd order effect.\n\n\nReference: Chapter 12 Hypothesis testing of Intro to GIS and Spatial Analysis\n\n\n\n\n\n\n\nQuadrat Analysis: The interpretation\n\n\n\n\n\n\n\n\nWeaknesses of quadrat analysis\n\n\n\nIt is sensitive to the quadrat size.\n\nIf the quadrat size is too small, they may contain only a couple of points, and\nIf the quadrat size is too large, they may contain too many points.\n\nIt is a measure of dispersion rather than a measure of pattern.\nIt results in a single measure for the entire distribution, so variation within the region are not recognised."
  },
  {
    "objectID": "lesson/Lesson03/Lesson03-SPPA.html#distance-based-nearest-neighbour-index",
    "href": "lesson/Lesson03/Lesson03-SPPA.html#distance-based-nearest-neighbour-index",
    "title": "Lesson 3: Spatial Point Patterns Analysis",
    "section": "Distance-based: Nearest Neighbour Index",
    "text": "Distance-based: Nearest Neighbour Index\n\nWhat is Nearest Neighbour?\nDirect distance from a point to its nearest neighbour."
  },
  {
    "objectID": "lesson/Lesson03/Lesson03-SPPA.html#nearest-neighbour-index",
    "href": "lesson/Lesson03/Lesson03-SPPA.html#nearest-neighbour-index",
    "title": "Lesson 3: Spatial Point Patterns Analysis",
    "section": "Nearest Neighbour Index",
    "text": "Nearest Neighbour Index\nThe Nearest Neighbour Index is expressed as the ratio of the Observed Mean Distance to the Expected Mean Distance.\n\n\n\nCalculating Nearest Neighbour Index\n\n\n\n\nInterpreting Nearest Neighbour Index\nThe expected distance is the average distance between neighbours in a hypothetical random distribution.\n\nIf the index is less than 1, the pattern exhibits clustering,\nIf the index is equal to 1, the patterns exhibits random, and\nIf the index is greater than 1, the trend is toward dispersion or competition.\n\n\n\n\n\n\n\n\n\nThe test statistics\n\n\n\nNull Hypothesis: Points are randomly distributed\nTest statistics:\n\n\n\nReject the null hypothesis if the z-score is large and p-value is smaller than the alpha value.\n\n\n\n\n\n\nInterpreting Nearest Neighbour Index\n\n\n\nThe p-value is smaller than 0.05 =&gt; Reject the null hypothesis that the point patterns are randomly distributed."
  },
  {
    "objectID": "lesson/Lesson03/Lesson03-SPPA.html#g-function",
    "href": "lesson/Lesson03/Lesson03-SPPA.html#g-function",
    "title": "Lesson 3: Spatial Point Patterns Analysis",
    "section": "G function",
    "text": "G function\n\n\nThe formula\n\n\n\n\n\n\n\nInterpretation of G-function\n\n\nThe shape of G-function tells us the way the events are spaced in a point pattern.\n\nClustered: G increases rapidly at short distance.\nEvenness: G increases slowly up to distance where most events spaced, then increases rapidly.\n\n\n\n\n\n\n\n\nHow do we tell if G is significant?\n\n\n\nThe significant of any departure from CSR (either cluster or regularity) can be evaluated using simulated “confidence envelopes”\n\n\n\n\n\n\n\n\nMonte Carlo simulation test of CSR\n\nPerform m independent simulation of n events (i.e. 999) in the study region.\nFor each simulated point pattern, estimate G(r) and use the maximum (95th) and minimum (5th) of these functions for the simulated patterns to define an upper and lower simulation envelope.\nIf the estimated G(r) lies above the upper envelope or below the lower envelope, the estimated G(r) is statistically significant.\n\n\n\n\nThe significant test of G-function"
  },
  {
    "objectID": "lesson/Lesson03/Lesson03-SPPA.html#f-function",
    "href": "lesson/Lesson03/Lesson03-SPPA.html#f-function",
    "title": "Lesson 3: Spatial Point Patterns Analysis",
    "section": "F function",
    "text": "F function\n\nSelect a sample of point locations anywhere in the study region at random\n\nDetermine minimum distance from each point to any event in the study area.\n\nThree steps:\n\nRandomly select m points (p1, p2, ….., pn),\nCalculate dmin(pi,s) as the minimum distance from location pi to any event in the point patterns, and\nCalculate F(d).\n\n\n\n\nThe F function formula\n\n\n\n\n\n\n\n\nInterpretation of F-function\n\nClustered = F(r) rises slowly at first, but more rapidly at longer distances.\nEvenness = F(r) rises rapidly at first, then slowly at longer distances.\n\n\n\n\nThe significant test of F-function\n\n\n\n\nComparison between G and F"
  },
  {
    "objectID": "lesson/Lesson03/Lesson03-SPPA.html#ripleys-k-function-ripley-1981",
    "href": "lesson/Lesson03/Lesson03-SPPA.html#ripleys-k-function-ripley-1981",
    "title": "Lesson 3: Spatial Point Patterns Analysis",
    "section": "Ripley’s K function (Ripley, 1981)",
    "text": "Ripley’s K function (Ripley, 1981)\n\nLimitation of nearest neighbor distance method is that it uses only nearest distance\nConsiders only the shortest scales of variation.\nK function uses more points.\n\nProvides an estimate of spatial dependence over a wider range of scales.\nBased on all the distances between events in the study area.\nAssumes isotropy over the region.\n\n\n\n\nCalculating the K function\n\n\n\nConstruct a circle of radius h around each point event(i).\nCount the number of other events (j) that fall inside this circle.\nRepeat these two steps for all points (i) and sum results.\nIncrement h by a small amount and repeat the calculation.\n\n\n\n\n\n\n\n\nK function\n\n\nThe formula:\n\n\n\n\n\n\nThe K function complete spatial randomness test\n\nK(h) can be plotted against different values of h.\nBut what should K look like for no spatial dependence?\nConsider what K(h) should look like for a random point process (CSR)\n\nThe probability of an event at any point in R is independent of what other events have occurred and equally likely anywhere in R\n\n\n\n\n\nInterpreting the K function complete spatial randomness test\n\n\nUnder the assumption of CSR, the expected number of events within distance h of an event is:\n\nCompare K(h) to 𝜋ℎ^2\n\nK(h) &lt; 𝜋ℎ^2 if point pattern is regular\nK(h) &gt; 𝜋ℎ^2 if point pattern is clustered\n\n\n\n\nAbove the envelop: significant cluster pattern - Below the envelop: significant regular\nInside the envelop: CSR"
  },
  {
    "objectID": "lesson/Lesson03/Lesson03-SPPA.html#the-l-function-besag-1977",
    "href": "lesson/Lesson03/Lesson03-SPPA.html#the-l-function-besag-1977",
    "title": "Lesson 3: Spatial Point Patterns Analysis",
    "section": "The L function (Besag 1977)",
    "text": "The L function (Besag 1977)\n\n\nIn practice, K function will be normalised to obtained a benchmark of zero.\nThe formula:\n\n\n\n\n\nInterpreting the L function complete spatial randomness test\n\n\n\n\nWhen an observed L value is greater than its corresponding L(theo)(i.e. red break line) value for a particular distance and above the upper confidence envelop, spatial clustering for that distance is statistically significant (e.g. distance beyond C).\nWhen an observed L value is greater than its corresponding L(theo) value for a particular distance and lower than the upper confidence envelop, spatial clustering for that distance is statistically NOT significant (e.g. distance between B and C).\nWhen an observed L value is smaller than its corresponding L(theo) value for a particular distance and beyond the lower confidence envelop, spatial dispersion for that distance is statistically significant. - When an observed L value is smaller than its corresponding L(theo) value for a particular distance and within the lower confidence envelop, spatial dispersion for that distance is statistically NOT significant (e.g. distance between A and B).\n\n\n\n\n\nThe grey zone indicates the confident envelop (i.e. 95%).\n\n\n\n\n\n\n\n\nThe L function (Besag 1977)\n\n\nThe modified L function\n\n\nL(r)&gt;0 indicates that the observed distribution is geographically concentrated.\nL(r)&lt;0 implies dispersion.\nL(r)=0 indicates complete spatial randomness (CRS)."
  },
  {
    "objectID": "lesson/Lesson03/Lesson03-SPPA.html#references",
    "href": "lesson/Lesson03/Lesson03-SPPA.html#references",
    "title": "Lesson 3: Spatial Point Patterns Analysis",
    "section": "References",
    "text": "References\n\nChapter 11 Point Pattern Analysis of Intro to GIS and Spatial Analysis. Section 11.2, 11.3, 11.3.1 and 11.4\nGIS&T Body of Knowledge AM-07-Point Pattern Analysis\nGIS&T Body of Knowledge AM-08-Kernels and Density Estimation\nAnalyzing Patterns in Business Point Data, Directions Magazine March 17, 2005.\nO’Sullivan, D., and Unwin, D. (2010) Geographic Information Analysis, Second Edition. John Wiley & Sons Inc., New Jersey, Canada. Chapter 5-6.\nBaddeley A., Rubak E. and Turner R. (2015) Spatial Point Patterns: Methodology and Applications with R, Chapman and Hall/CRC."
  },
  {
    "objectID": "lesson/Lesson02/Lesson02-GeoVis.html#content",
    "href": "lesson/Lesson02/Lesson02-GeoVis.html#content",
    "title": "Lesson 2: Fundamental of Geospatial Data Visualisation and tmap Methods",
    "section": "Content",
    "text": "Content\n\nIntroducing maps\nTypology of maps\n\nReference maps\nThematic maps\n\nProportional Symbol Map\nChoropleth Mapping\nIntroduction to tmap Methods\n\n\nThis lesson consists of two parts. First, I will share with you the concepts and design principles of choropleth maps. Next, I will introduce you to tmap, an R package specially designed for thematic mapping based on Layered Grammar of Graphics"
  },
  {
    "objectID": "lesson/Lesson02/Lesson02-GeoVis.html#choropleth-map",
    "href": "lesson/Lesson02/Lesson02-GeoVis.html#choropleth-map",
    "title": "Lesson 2: Fundamental of Geospatial Data Visualisation and tmap Methods",
    "section": "Choropleth Map",
    "text": "Choropleth Map\nA choropleth map is a type of thematic map in which areas are shaded or patterned in proportion to a statistical variable that represents an aggregate summary of a geographic characteristic within each area, such as population or per-capita income."
  },
  {
    "objectID": "lesson/Lesson02/Lesson02-GeoVis.html#data-classification",
    "href": "lesson/Lesson02/Lesson02-GeoVis.html#data-classification",
    "title": "Lesson 2: Fundamental of Geospatial Data Visualisation and tmap Methods",
    "section": "Data classification",
    "text": "Data classification\n\n\nNot sure how many classes to use? Have a look at the distribution of your data in a histogram (see examples below): Are there obvious clusters within your data? Are there large gaps in your data range that suggest nice compact data classes? If so, pick that number of classes and place those class breaks around those clusters."
  },
  {
    "objectID": "lesson/Lesson02/Lesson02-GeoVis.html#colour-scheme",
    "href": "lesson/Lesson02/Lesson02-GeoVis.html#colour-scheme",
    "title": "Lesson 2: Fundamental of Geospatial Data Visualisation and tmap Methods",
    "section": "Colour scheme",
    "text": "Colour scheme\nColorBrewer"
  },
  {
    "objectID": "lesson/Lesson02/Lesson02-GeoVis.html#mapping-packages-in-r",
    "href": "lesson/Lesson02/Lesson02-GeoVis.html#mapping-packages-in-r",
    "title": "Lesson 2: Fundamental of Geospatial Data Visualisation and tmap Methods",
    "section": "Mapping packages in R",
    "text": "Mapping packages in R\n\n\nSelected popular mapping packages\nCRAN Task View: Analysis of Spatial Data\n\ntmap\nmapsf\nleaflet\nggplot2. Read Chapter 6: Maps of ‘ggplot2: Elegant Graphics for Data Analysis’ for more detail.\nggmap\nquickmapr\nmapview\n\n\nOther packages\n\nRColorBrewer\nclassInt"
  },
  {
    "objectID": "lesson/Lesson02/Lesson02-GeoVis.html#introducing-tmap",
    "href": "lesson/Lesson02/Lesson02-GeoVis.html#introducing-tmap",
    "title": "Lesson 2: Fundamental of Geospatial Data Visualisation and tmap Methods",
    "section": "Introducing tmap",
    "text": "Introducing tmap\n\n\n\ntmap is a R package specially designed for creating thematic maps using the pricinples of the Grammar of Graphics.\nIt offers a flexible, layer-based, and easy to use approach to create thematic maps, such as choropleths and proportional symbol maps.\nIt supports two modes: plot(static maps) and view (interactive maps).\nIt provides shiny integration(with tmapOutput and renderTmap)."
  },
  {
    "objectID": "lesson/Lesson02/Lesson02-GeoVis.html#tmap-elements",
    "href": "lesson/Lesson02/Lesson02-GeoVis.html#tmap-elements",
    "title": "Lesson 2: Fundamental of Geospatial Data Visualisation and tmap Methods",
    "section": "tmap elements",
    "text": "tmap elements\ntm_shape()\n\nThe first element to start with is tm_shape(), which specifies the shape object."
  },
  {
    "objectID": "lesson/Lesson02/Lesson02-GeoVis.html#tmap-elements-1",
    "href": "lesson/Lesson02/Lesson02-GeoVis.html#tmap-elements-1",
    "title": "Lesson 2: Fundamental of Geospatial Data Visualisation and tmap Methods",
    "section": "tmap elements",
    "text": "tmap elements\nBase layers\n\nNext, one, or a combination of the following drawing layers should be specified:\n\n\n\nLinks to tm_polygons(), tm_symbols(), tm_lines(), tm_raster() and tm_text()"
  },
  {
    "objectID": "lesson/Lesson02/Lesson02-GeoVis.html#tmap-elements-2",
    "href": "lesson/Lesson02/Lesson02-GeoVis.html#tmap-elements-2",
    "title": "Lesson 2: Fundamental of Geospatial Data Visualisation and tmap Methods",
    "section": "tmap elements",
    "text": "tmap elements\nBase layers\n\nEach of these functions specifies the geometry, mapping, and scaling component of the LGTM.\nAn aesthetic can take a constant value, a data variable name, or a vector consisting of values or variable names.\nIf a data variable is provided, the scale is automatically configured according to the values of this variable, but can be adjusted with several arguments. For instance, the main scaling arguments for a color aesthetic are color palette, the preferred number of classes, and a style to create classes.\nAlso, for each aesthetic, except for the text labels, a legend is automatically created.\nIf a vector of variable names is provided, small multiples are created, which will be explained further below."
  },
  {
    "objectID": "lesson/Lesson02/Lesson02-GeoVis.html#tmap-elements-3",
    "href": "lesson/Lesson02/Lesson02-GeoVis.html#tmap-elements-3",
    "title": "Lesson 2: Fundamental of Geospatial Data Visualisation and tmap Methods",
    "section": "tmap elements",
    "text": "tmap elements\nDerived layers\n\n\nEach aesthetic can take a constant value or a data variable name. For instance, tm_fill(col=\"blue\") colors all polygons blue, while tm_fill(col=\"var1\"), where “var1” is the name of a data variable in the shape object, creates a choropleth.\n\nThe supported derived layers are as follows:"
  },
  {
    "objectID": "lesson/Lesson02/Lesson02-GeoVis.html#data-classification-methods-of-tmap",
    "href": "lesson/Lesson02/Lesson02-GeoVis.html#data-classification-methods-of-tmap",
    "title": "Lesson 2: Fundamental of Geospatial Data Visualisation and tmap Methods",
    "section": "Data classification methods of tmap",
    "text": "Data classification methods of tmap\n\n\nMost choropleth maps employ some method of data classification. The point of classification is to take a large number of observations and group them into data ranges or classes.\n\n\n\n\n\n\nNote\n\n\n\ntmap provides a total ten data classification methods, namely: fixed, sd, equal, pretty (default), quantile, kmeans, hclust, bclust, fisher, and jenks.\nTo define a data classification method, the style argument of tm_fill() or tm_polygons() will be used.\nThe choropleth map on the right shows a quantile data classification with 8 classes are used.\n\n\n\n\n\n\n\ntm_shape(mpszpop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 8,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "lesson/Lesson02/Lesson02-GeoVis.html#colour-scheme-1",
    "href": "lesson/Lesson02/Lesson02-GeoVis.html#colour-scheme-1",
    "title": "Lesson 2: Fundamental of Geospatial Data Visualisation and tmap Methods",
    "section": "Colour Scheme",
    "text": "Colour Scheme\n\n\ntmap supports colour ramps either defined by the user or a set of predefined colour ramps from the RColorBrewer package.\n\n\n\n\n\n\nNote\n\n\n\nTo change the colour, we assign the preferred colour to palette argument of tm_fill().\nNotice that the word blues is used instead of blue and the alphabet b is in uppercase.\n\n\n\n\n\n\n\ntm_shape(mpszpop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\",\n          palette = \"Blues\") +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "lesson/Lesson02/Lesson02-GeoVis.html#reference",
    "href": "lesson/Lesson02/Lesson02-GeoVis.html#reference",
    "title": "Lesson 2: Fundamental of Geospatial Data Visualisation and tmap Methods",
    "section": "Reference",
    "text": "Reference\nPrinciples, Concepts and Methods of Choropleth Maps Design\nCore Reading\n\nChoropleth Maps\nThe Basics of Data Classification\n\nAdditional Readings\n\nChoropleth Maps – A Guide to Data Classification\nBivariate Choropleth\nValue-by-alpha maps\nWhat to consider when creating choropleth maps\nChoropleth Mapping with Exploratory Data Analysis"
  },
  {
    "objectID": "lesson/Lesson02/Lesson02-GeoVis.html#references",
    "href": "lesson/Lesson02/Lesson02-GeoVis.html#references",
    "title": "Lesson 2: Fundamental of Geospatial Data Visualisation and tmap Methods",
    "section": "References",
    "text": "References\nAll About tmap package\n\ntmap: Thematic Maps in R\nDevelopment site\ntmap Reference\ntmap: get started!\ntmap: version changes\ntmap: creating thematic maps in a flexible way (useR!2015)\nExploring and presenting maps with tmap (useR!2017)"
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-GDS.html#content",
    "href": "lesson/Lesson01/Lesson01-GDS.html#content",
    "title": "Lesson 1: Geospatial Data Science with R: sf approach and methods",
    "section": "Content",
    "text": "Content\n\nAn overview of Geospatial Data Models\n\nVector and raster data model\nCoordinate systems and map projection\n\nHandling Geospatial Data in R: An Overview\nSimple features approach\n\nsf package\n\n\n\nThis lesson consists of two parts. First, I will talk about Geospatial Data Models. For students who have taken SMT201 GIS for Urban Planning, this is not new at all. However, for students who did not read SMT201, this will be new. Anyway, the focus of this section will be on R. Hence, even for students who have taken SMT201 before, this will be a good revision.\nIn part two of this lesson, I will introduce sp package. It is a relatively new R package specially developed to handle geospatial data R using tidyverse principle."
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-GDS.html#geospatial-data-models",
    "href": "lesson/Lesson01/Lesson01-GDS.html#geospatial-data-models",
    "title": "Lesson 1: Geospatial Data Science with R: sf approach and methods",
    "section": "Geospatial Data Models",
    "text": "Geospatial Data Models\nWhy should we worry about?\n\n\nIt is important for us to note that what ever data capture in a database is a model of the real world. When we say model, this means that it is a simplify version of the real world and not the real world themselves."
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-GDS.html#basic-spatial-data-models",
    "href": "lesson/Lesson01/Lesson01-GDS.html#basic-spatial-data-models",
    "title": "Lesson 1: Geospatial Data Science with R: sf approach and methods",
    "section": "Basic Spatial Data Models",
    "text": "Basic Spatial Data Models\n\nVector - implementation of discrete object conceptual model\n\nPoint, line and polygon representations.\nWidely used in cartography, and network analysis.\n\nRaster – implementation of field conceptual model\n\nArray of cells used to represent objects.\nUseful as background maps and for spatial analysis.\n\n\n\nIn general, there are two types of geospatial data models, namely vector and raster data models.\nVector data model tends to be used to store geospatial data that are discrete in nature. For example bus stop, building footprint, planning area.\nRaster data model, one the other hands, are used to store continuous fenomena such as air polution, elevation and precipitation."
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-GDS.html#coordinate-systems-and-map-projections",
    "href": "lesson/Lesson01/Lesson01-GDS.html#coordinate-systems-and-map-projections",
    "title": "Lesson 1: Geospatial Data Science with R: sf approach and methods",
    "section": "Coordinate Systems and Map Projections",
    "text": "Coordinate Systems and Map Projections\nWhat is a coordinate system?\n\n\nA coordinate system is an important property of an geospatial data. It provides a location reference to the geospatial data.\n\nThere are two common types of coordinate systems used in mapping, namely: geographic coordinate systems and projected coordinate system.\n\n\n\n\n\n\n\nFurther Reading\n\n\n\nRefer to this article and Chapter 9 Coordinate Systems to learn more about map projection.\n\n\n\n\n\n\n\n\n\n\nA coordinate system is a reference system used to represent the locations of geographic features, imagery, and observations such as GPS locations within a common geographic framework.\nEach coordinate system is defined by:\n\nIts measurement framework which is either geographic (in which spherical coordinates are measured from the earth’s center) or planimetric (in which the earth’s coordinates are projected onto a two-dimensional planar surface).\nUnit of measurement (typically feet or meters for projected coordinate systems or decimal degrees for latitude–longitude).\nThe definition of the map projection for projected coordinate systems.\nOther measurement system properties such as a spheroid of reference, a datum, and projection parameters like one or more standard parallels, a central meridian, and possible shifts in the x- and y-directions."
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-GDS.html#standard-for-geospatial-data-handling-and-analysis",
    "href": "lesson/Lesson01/Lesson01-GDS.html#standard-for-geospatial-data-handling-and-analysis",
    "title": "Lesson 1: Geospatial Data Science with R: sf approach and methods",
    "section": "Standard for Geospatial Data Handling and Analysis",
    "text": "Standard for Geospatial Data Handling and Analysis\n\n\n\n\n\n\n\nFurther Reading\n\n\nFor more information, visit this link.\n\n\n\n\nThe OGC OpenGIS Implementation Standard for Geographic Information / ISO 19125 defines:\n\nGeometric objects which can be of type point, line, polygon, multi-point, etc, and are associated to a given Coordinate Reference System;\nMethods on geometric objects return properties like dimension, boundary, area, centroid, etc;\nMethods for testing spatial relations between geometric objects equals, disjoint, intersects, touches, crosses, within, contains, overlaps and relate, which returns TRUE or FALSE;\nMethods that support spatial analysis distance, which returns a distance, and buffer, convex hull, intersection, union, difference, and symmetric difference, which returns new geometric objects.\n\nSource: www.opengeospatial.org/standards/sfa"
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-GDS.html#an-introduction-to-simple-features",
    "href": "lesson/Lesson01/Lesson01-GDS.html#an-introduction-to-simple-features",
    "title": "Lesson 1: Geospatial Data Science with R: sf approach and methods",
    "section": "An introduction to simple features",
    "text": "An introduction to simple features\n\nfeature: abstraction of real world phenomena (type or instance); has a geometry and other attributes (properties)\nsimple feature: feature with all geometric attributes described piecewise by straight line or planar interpolation between sets of points (no curves)\nIt is a hierarchical data model that simplifies geographic data by condensing a complex range of geographic forms into a single geometry class."
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-GDS.html#geospatial-data-object-framework",
    "href": "lesson/Lesson01/Lesson01-GDS.html#geospatial-data-object-framework",
    "title": "Lesson 1: Geospatial Data Science with R: sf approach and methods",
    "section": "Geospatial Data Object Framework",
    "text": "Geospatial Data Object Framework\n\nTo begin with, all contributed packages for handling spatial data in R had different representations of the data. This made it difficult to exchange data both within R between packages, and between R and external le formats and applications.\nThe first general package to provide classes and methods for spatial data types that was developed for R is called sp. It was first released on CRAN in 2005.\nIn late October 2016, sf was first released on CRAN to provide standardised support for vector data in R."
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-GDS.html#r-packages-that-support-spatial-classes",
    "href": "lesson/Lesson01/Lesson01-GDS.html#r-packages-that-support-spatial-classes",
    "title": "Lesson 1: Geospatial Data Science with R: sf approach and methods",
    "section": "R packages that support spatial classes",
    "text": "R packages that support spatial classes\nIn general, three R packages will be used to handle vector-based geospatial data in spatial classes, they are:\n\nsp provides classes and methods for dealing with spatial data in R.\nrgdal allows R to understand the structure of a geospatial data file by providing functions to read and convert geospatial data into easy-to-work-with R dataframes.\nrgeos implements the methods of the OGC standard."
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-GDS.html#introducing-sf-package",
    "href": "lesson/Lesson01/Lesson01-GDS.html#introducing-sf-package",
    "title": "Lesson 1: Geospatial Data Science with R: sf approach and methods",
    "section": "Introducing sf Package",
    "text": "Introducing sf Package\n\nsf package provides a syntax and data-structures which are coherent with the tidyverse.\nA quick introduction can be found here.\nFor more detail, visit this link."
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-GDS.html#sf-functions",
    "href": "lesson/Lesson01/Lesson01-GDS.html#sf-functions",
    "title": "Lesson 1: Geospatial Data Science with R: sf approach and methods",
    "section": "sf functions",
    "text": "sf functions\n\nGeospatial data handling\nGeometric confirmation\nGeometric operations\nGeometry creation\nGeometry operations\nGeometric measurement"
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-GDS.html#references",
    "href": "lesson/Lesson01/Lesson01-GDS.html#references",
    "title": "Lesson 1: Geospatial Data Science with R: sf approach and methods",
    "section": "References",
    "text": "References\nAll About sf package\n\n\nReference manual\nTidy spatial data analysis\n\nVignettes:\n\nSimple Features for R\nReading, Writing and Converting Simple Features\nManipulating Simple Feature Geometries\nManipulating Simple Features\nPlotting Simple Features\nMiscellaneous\nSpherical geometry in sf using s2geometry\n\nOthers:\n\nR spatial follows GDAL and PROJ development"
  },
  {
    "objectID": "git.html",
    "href": "git.html",
    "title": "Git and Github",
    "section": "",
    "text": "Happy Git and GitHub for the useR. Highly recommended to beginners."
  },
  {
    "objectID": "git.html#book",
    "href": "git.html#book",
    "title": "Git and Github",
    "section": "",
    "text": "Happy Git and GitHub for the useR. Highly recommended to beginners."
  },
  {
    "objectID": "git.html#gitgithub",
    "href": "git.html#gitgithub",
    "title": "Git and Github",
    "section": "git/github",
    "text": "git/github\n\ngithub doc\nGitHub and RStudio"
  },
  {
    "objectID": "git.html#blog-post",
    "href": "git.html#blog-post",
    "title": "Git and Github",
    "section": "Blog post",
    "text": "Blog post\n\nGetting starting with git and GitHub using RStudio\ngit\nTransform a folder as git project synchronized on Github or Gitlab\nRecovering from common Git predicaments\ngit in RStudio\nSolving git(GitHub) token issue"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "GAproject.html",
    "href": "GAproject.html",
    "title": "Geospatial Analytics Project",
    "section": "",
    "text": "The purpose of the geospatial analytics project is to provide students first hand experience on building web-based geospatial analytics tool by integrating open source web mapping API(s), data visualisation API(s) and geospatial analysis libraries. You will also learn how to collecting, processing and analysing spatially related issues using real world data. Students are encouraged to focus on research topics that are relevant to their field of study.\nThe project is team work. Students are required to form a project team of 2-3 members by the first week of the academic term. Each project teams must start thinking about their project ideas after the first lesson. They are expected to discuss their project topic and scope of works with the instructor before the end of Week 8. A project proposal in the form website edited using Quarto and publish on Netlify will be prepared and the link must be provided on eLearn by the end of Week 8.\nThe project proposal should describe the motivation of the project, problems or issues that the project will address, the relevant related work, the approach the team plans to take to solve the problem, and early prototypes or storyboards. The project teams should take advantage of this proposal as a chance to get feedback on the direction of the project from their peers.\nStudents are required to update their project websites with all the details including the final application, user guide, poster, practice research paper by the end of week 14."
  },
  {
    "objectID": "GAproject.html#overview",
    "href": "GAproject.html#overview",
    "title": "Geospatial Analytics Project",
    "section": "",
    "text": "The purpose of the geospatial analytics project is to provide students first hand experience on building web-based geospatial analytics tool by integrating open source web mapping API(s), data visualisation API(s) and geospatial analysis libraries. You will also learn how to collecting, processing and analysing spatially related issues using real world data. Students are encouraged to focus on research topics that are relevant to their field of study.\nThe project is team work. Students are required to form a project team of 2-3 members by the first week of the academic term. Each project teams must start thinking about their project ideas after the first lesson. They are expected to discuss their project topic and scope of works with the instructor before the end of Week 8. A project proposal in the form website edited using Quarto and publish on Netlify will be prepared and the link must be provided on eLearn by the end of Week 8.\nThe project proposal should describe the motivation of the project, problems or issues that the project will address, the relevant related work, the approach the team plans to take to solve the problem, and early prototypes or storyboards. The project teams should take advantage of this proposal as a chance to get feedback on the direction of the project from their peers.\nStudents are required to update their project websites with all the details including the final application, user guide, poster, practice research paper by the end of week 14."
  },
  {
    "objectID": "GAproject.html#project-theme",
    "href": "GAproject.html#project-theme",
    "title": "Geospatial Analytics Project",
    "section": "Project Theme",
    "text": "Project Theme\nStudents are required to choose one of the ASEAN country as the study area and select at least one following theme to develop the GeoSpaital Analytics Project:\n\nSpatial Point Patterns Analysis\nExploratory Spatial Data Analysis\nAnalytical Regionalisation and Geographical Segmentation\nGeographically Weighted Regression Models\nFlow and Movement Data Analytics\nGeographical Accessibility and Spatial Interaction Models\n\nPlease feel free to approach me for more details."
  },
  {
    "objectID": "GAproject.html#potentially-useful-data-source",
    "href": "GAproject.html#potentially-useful-data-source",
    "title": "Geospatial Analytics Project",
    "section": "Potentially Useful Data Source",
    "text": "Potentially Useful Data Source\n\nNational Statistical Office Thailand.\nPhilippine Statistics Authority\nVietnam Provincial Competitiveness Index\nGENERAL STATISTICS OFFICE, Vietnam.\nMyanmar Information Management Unit\nArmed Conflict Location & Event Data Project (ACLED).\nIndonesia Geospatial"
  },
  {
    "objectID": "GAproject.html#project-milestone",
    "href": "GAproject.html#project-milestone",
    "title": "Geospatial Analytics Project",
    "section": "Project Milestone",
    "text": "Project Milestone\n\nBrainstorming project ideas and consulting with course instructor between Week 2 to Week 7.\nEditing and publishing project proposal by the end of Week 8.\nSubmission of project poster: 13th November 2024 by 11:59pm (mid-night)\nTownhall presentation: 18th November 2024 (Monday), at 12:00-2:00pm. Venue: LKSLIB Basement concourse. Students are required to be at the venue latest by 11:30am to setup your project poster.\nSubmission of updated project webpage, final application, user guide, project poster and artifacts: 24th November 2024 by 11:59pm (mid-night)"
  },
  {
    "objectID": "GAproject.html#project-deliverables",
    "href": "GAproject.html#project-deliverables",
    "title": "Geospatial Analytics Project",
    "section": "Project Deliverables",
    "text": "Project Deliverables\n\nProject Website\n\nProject Github\nAt the beginning of the project, project teams are required to create a project Github. The project Github should include all the materials used to develop the project and the written materials such as proposal, poster and practice research paper. It must be used to maintain a complete project versioning including the application and project documents. The Github link must be included in the project proposal. By the end of the project, the project team must pack the final version of the Github repository and upload onto eLearn for final submission. The Github link also must be provided on eLearn.\n\n\nProject Website\nEach project team are required to create the project website by using Quatro. It will be disseminated by using Netfity web service.\nAs a first step, you should create a project summary at the course wiki that includes:\n\nThe title of your project,\nA short description of not more than 350 word summarising the motivation, objectives, main features of the application your team are going to build, and\nThe project proposal. This should in a webblog page (remember to provide a link at eLearn).\n\n\n\n\nPoster\nThe project poster should provide an overview of your project. It should include, but not limited to the following information:\n\nIssues and problems - A clear statement of the issues or/and problems your project addresses.\nMotivation - An explanation of why the issues and/or problems are interesting and what make them difficult to solve.\nApproach - A description of the techniques or algorithms you used to solve the problem.\nResults - Screenshots and a working demo of the system you built.\nFuture Work - An explanation of how the work could be extended.\n\nThe dimensions for the poster must conform to the International Standards Organization (ISO) poster size format (A1).\n\nSize = ISO A1 (594 × 841mm or 23.39 × 33.11inci)\nResolution = 300dpi or above\nFile format = jpeg\n\nPlease ensure that the poster is in high resolution.\nThe poster will be considered as a final deliverable. So don’t forget to apply good visual design principles to both your poster and project report.\nNote: The course instructor will be responsible for printing your poster. You are required to upload your poster onto the project Dropbox of eLearn according to the submission deadline provided on the project milesotne sub-section.\n\n\nPeer Evaluation\nThis component is not a typical peer evaluation of project. For Geospatial Analytics Project, students are required to evaluate at least three applications prepared by your coursemate during the townhall presentation session. A peer evaluation form will be provided. Each team only need to submit one report.\n\n\nFinal Deliverables\nThe final deliverable will include:\n\nupdated project webpage.\nartifact including the ShinyApp, data and all r modules.\nUser Guide - Step-by-step guide on how to use the Shiny Applications.\nproject poster"
  },
  {
    "objectID": "GAproject.html#grading",
    "href": "GAproject.html#grading",
    "title": "Geospatial Analytics Project",
    "section": "Grading",
    "text": "Grading\nThe geospatial analytics project will account for 30% of your final grade in the course. The distribution of marks for each stage of the project are as follows:\n\nProject website 25%\nPoster 10%\nPeer Evaluation 15%\nShinyApp 50%\n\nThe course instructor will consider strongly the novelty of the idea (If it has never been done before, you will get lots of credit!), how it addresses the problem at hand, the methodology you employ in doing the research, and your technical skill in implementing the idea.\nIn small group projects, each person will be graded individually. A good group project is a system consisting of a collection of well defined subsystems. Each subsystem should be the responsibility of one person and be clearly identified as their project. A good criteria for whether you should work in a group is whether the system as a whole is greater than the sum of its parts!\n\nGrading criteria for poster\nThe poster will be graded based on the following criteria:\n\nClear communication of key aspects of solution\nClear communication of design approaches\nClear communication of arguments for proposed solution\nCraft quality of the solution"
  },
  {
    "objectID": "GAproject.html#reference",
    "href": "GAproject.html#reference",
    "title": "Geospatial Analytics Project",
    "section": "Reference",
    "text": "Reference\n\nPaula Moraga (2017) “SpatialEpiApp : A Shiny web application for the analysis of spatial and spatio-temporal disease data”, Spatial and Spatio-temporal Epidemiology, 23, pp. 47-57. Access using SMU e-journal.\nJavier De La Hoz-M, Susana Mendes & María José Fernandez-Gómez (2022) “GeoWeightedModel : An R-Shiny package for Geographically Weighted Models”, SoftwareX, 20.\nMario Figueira, David Conesa & Antonio L´opez-Quílez (2024) “A shiny R app for spatial analysis of species distribution models”, Ecological Informatics, 80."
  },
  {
    "objectID": "GAproject.html#learning-from-past-project",
    "href": "GAproject.html#learning-from-past-project",
    "title": "Geospatial Analytics Project",
    "section": "Learning from past project",
    "text": "Learning from past project\n\nMust review project\n\nSGSAS: Simple Geo-Spatial Analysis using R-Shiny especially gwr sub-module of the application.\n\n\n\nOthers\n\nAY2018-19 Term 2\n\nSignal and the Shiny application.\n\nAY2020-2021 Term 1\n\nJ-Town and the Shiny Application\n\nAY2021-2022 Term 1\n\nGroup 1: Spatial Interaction Model\nGroup 3: Regionalisation in Reach\nGroup 5: Geographically Weighted Regression\nGroup 7: GeoeXplorer, a web-enabled geospatial analytics toolkit for detecting and visualising global spatial autocorrelation and local clusters and outliers from univariate geographically referenced data.\nGroup 8: Spatial Pointers, a web-enabled Spatial Point Patterns Analysis toolkit for visualising and analysing both spatial and network constrained point patterns.\n\nAY2022-2023 Term 1\nGroup 1: This is FINE: Fire INcidents Explorer\nGroup 2: Spatial Bros\nGroup 3: Analysis of Resale HDB Prices\nGroup 4: GeoNinjas\nGroup 5: Transit-Ability\nGroup 6: CrimeIndia\nGroup 7: The Right Space\nGroup 8: CarSpot: Geospatial Parking Analysis!!\nGroup 9: Dr Go Where\nGroup 10: Spatial Point Pattern Analysis on Seoul’s healthcare accessibility\nGroup 11: Regress Rangers\nGroup 12: Spatial Point Pattern Analysis of Airbnb listings in Beijing"
  },
  {
    "objectID": "GAproject.html#project-links",
    "href": "GAproject.html#project-links",
    "title": "Geospatial Analytics Project",
    "section": "Project Links",
    "text": "Project Links\n\nGroup 2: CrimeWatch - Vietnam Law and Order\nGroup 3: SimplyGEO-Analysing Push-Pull Factors for Public Bus Demand in Singapore Using Spatial Econometric Interaction Modelling. The project website is well designed, providing a clear and comprehensive description of the project, UI design, and User Guide. The project minutes were well documented.\nGroup 4: ELDANALYZE: Simple Geospatial Analysis Using Shiny\nGroup 5: Culinary Crossroads\nGroup 6: RecycleSG\nGroup 7: WhatTown. The Shiny application is very well designed. Able to provide a collection of useful UIs for calibrating the Accessibility models and Spatial Point Patterns Analysis.\nGroup 8: Quake Quest!\nGroup 9: Happy Hideouts\nGourp 10: ThaiRoad - Spatial Points Pattern Analysis on Road Accidents in Thailand\nGroup 11: HealthGeo Cambodia\nGroup 12: Enhancing Urban Living\nGroup 13: Project Daylight\nGroup 14: GeoMommy"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "IS415 Geospatial Analytics and Applications",
    "section": "",
    "text": "In this web site, you will find all course materials here.\nInstructor: Dr. Kam Tin Seong, Associate Professor of Information Systems (Practice)\nPiazza link.\nConsultation booking link.\nVirtual Meeting Room: Zoom\n\n\n\nSection\nTime\nVenue\n\n\n\n\nG1\n8:15-11:30\nSCIS1 SR 3-2\n\n\n\n\n\nFor the next 12 weeks, your learning journey will be very bumpy, especially come to R programming\n\nLearning R can be difficult at first—it’s like learning a new language, just like Spanish, French, or Chinese. Hadley Wickham—the chief data scientist at RStudio and the author of some amazing R packages you’ll be using like ggplot2 — made this wise observation:\n\nIt’s easy when you start out programming to get really frustrated and think, “Oh it’s me, I’m really stupid,” or, “I’m not made out to program.” But, that is absolutely not the case. Everyone gets frustrated. I still get frustrated occasionally when writing R code. It’s just a natural part of programming. So, it happens to everyone and gets less and less over time. Don’t blame yourself. Just take a break, do something fun, and then come back and try again later.\n\nEven experienced programmers find themselves bashing their heads against seemingly intractable errors.\n\nIf you’re finding yourself taking way too long hitting your head against a wall and not understanding, take a break, talk to classmates, and don’t hesitate to approach me.\nThe students who have a bad time in this course are the ones who don’t work with one another to learn. We are a learning community, and we should help each other to learn.\nIf you understand something and someone is struggling with it, try and help them. If you are struggling, take a breath, and try to pinpoint what you are struggling with.\nOur goal is to be better programmers each day, not to be the perfect programmer. There’s no such thing as a perfect programmer. I’ve been learning new things almost every day.\nI promise you can succeed in this class as long as you are willing to learn, practice, be open minded and don’t give up easily.\n\n\n\n\n\n\nAll acts of academic dishonesty (including, but not limited to, plagiarism, cheating, fabrication, facilitation of acts of academic dishonesty by others, unauthorized possession of exam questions, or tampering with the academic work of other students) are serious offences. All work (whether oral or written) submitted for purposes of assessment must be the student’s own work. Penalties for violation of the policy range from zero marks for the component assessment to expulsion, depending on the nature of the offense. When in doubt, students should consult the instructors of the course. Details on the SMU Code of Academic Integrity may be accessed at http://www.smuscd.org/resources.html\n\n\n\nCourse materials obtained during your course of study at SMU are meant for personal use only, namely, for the purposes of studying and research. You are strictly not permitted to make copies of or print additional copies or distribute such copies of the course materials or any parts thereof, for commercial gain or exchange. For example, offering such materials on the Internet through CourseHero, Carousell and the like, is strictly prohibited.\nThe selling of these materials and/or any copies thereof are strictly prohibited under Singapore copyright laws. Printed materials and electronic materials are both protected by copyright laws. All students are subject to Singapore copyright laws and must strictly adhere to SMU’s procedures and requirements relating to copyright.\nPlease also note that for some materials, the publishers may specifically state that each copy is for the personal use of one individual only and no further reprographic reproduction is allowed, including for personal use. These restrictions are spelt out clearly on these specific sets of resources and students are required to adhere to these rules.\nStudents who infringe any of the aforesaid rules, laws and requirements shall be liable to disciplinary action by SMU. In addition, such students may also leave themselves open to suits by copyright owners who are entitled to take legal action against persons who infringe their copyright.\nWe strongly urge all students to respect the copyright laws and abide by SMU’s procedures and requirements relating to copyright.\n\n\n\nSMU strives to make learning experiences accessible for all. If you anticipate or experience physical or academic barriers due to disability, please let the instructor know immediately. You are also welcome to contact the university’s disability support team if you have questions or concerns about academic accommodations:included@smu.edu.sg\nPlease be aware that the accessible tables in our seminar room should remain available for students who require them.\n\n\n\nAs part of emergency preparedness, Instructors may conduct lessons online via the Zoom platform during the term, to prepare students for online learning. During an actual emergency, students will be notified to access the Zoom platform for their online lessons. The class schedule will mirror the current face-to-face class timetable unless otherwise stated."
  },
  {
    "objectID": "index.html#words-of-encouragement",
    "href": "index.html#words-of-encouragement",
    "title": "IS415 Geospatial Analytics and Applications",
    "section": "",
    "text": "For the next 12 weeks, your learning journey will be very bumpy, especially come to R programming\n\nLearning R can be difficult at first—it’s like learning a new language, just like Spanish, French, or Chinese. Hadley Wickham—the chief data scientist at RStudio and the author of some amazing R packages you’ll be using like ggplot2 — made this wise observation:\n\nIt’s easy when you start out programming to get really frustrated and think, “Oh it’s me, I’m really stupid,” or, “I’m not made out to program.” But, that is absolutely not the case. Everyone gets frustrated. I still get frustrated occasionally when writing R code. It’s just a natural part of programming. So, it happens to everyone and gets less and less over time. Don’t blame yourself. Just take a break, do something fun, and then come back and try again later.\n\nEven experienced programmers find themselves bashing their heads against seemingly intractable errors.\n\nIf you’re finding yourself taking way too long hitting your head against a wall and not understanding, take a break, talk to classmates, and don’t hesitate to approach me.\nThe students who have a bad time in this course are the ones who don’t work with one another to learn. We are a learning community, and we should help each other to learn.\nIf you understand something and someone is struggling with it, try and help them. If you are struggling, take a breath, and try to pinpoint what you are struggling with.\nOur goal is to be better programmers each day, not to be the perfect programmer. There’s no such thing as a perfect programmer. I’ve been learning new things almost every day.\nI promise you can succeed in this class as long as you are willing to learn, practice, be open minded and don’t give up easily."
  },
  {
    "objectID": "index.html#other-important-information",
    "href": "index.html#other-important-information",
    "title": "IS415 Geospatial Analytics and Applications",
    "section": "",
    "text": "All acts of academic dishonesty (including, but not limited to, plagiarism, cheating, fabrication, facilitation of acts of academic dishonesty by others, unauthorized possession of exam questions, or tampering with the academic work of other students) are serious offences. All work (whether oral or written) submitted for purposes of assessment must be the student’s own work. Penalties for violation of the policy range from zero marks for the component assessment to expulsion, depending on the nature of the offense. When in doubt, students should consult the instructors of the course. Details on the SMU Code of Academic Integrity may be accessed at http://www.smuscd.org/resources.html\n\n\n\nCourse materials obtained during your course of study at SMU are meant for personal use only, namely, for the purposes of studying and research. You are strictly not permitted to make copies of or print additional copies or distribute such copies of the course materials or any parts thereof, for commercial gain or exchange. For example, offering such materials on the Internet through CourseHero, Carousell and the like, is strictly prohibited.\nThe selling of these materials and/or any copies thereof are strictly prohibited under Singapore copyright laws. Printed materials and electronic materials are both protected by copyright laws. All students are subject to Singapore copyright laws and must strictly adhere to SMU’s procedures and requirements relating to copyright.\nPlease also note that for some materials, the publishers may specifically state that each copy is for the personal use of one individual only and no further reprographic reproduction is allowed, including for personal use. These restrictions are spelt out clearly on these specific sets of resources and students are required to adhere to these rules.\nStudents who infringe any of the aforesaid rules, laws and requirements shall be liable to disciplinary action by SMU. In addition, such students may also leave themselves open to suits by copyright owners who are entitled to take legal action against persons who infringe their copyright.\nWe strongly urge all students to respect the copyright laws and abide by SMU’s procedures and requirements relating to copyright.\n\n\n\nSMU strives to make learning experiences accessible for all. If you anticipate or experience physical or academic barriers due to disability, please let the instructor know immediately. You are also welcome to contact the university’s disability support team if you have questions or concerns about academic accommodations:included@smu.edu.sg\nPlease be aware that the accessible tables in our seminar room should remain available for students who require them.\n\n\n\nAs part of emergency preparedness, Instructors may conduct lessons online via the Zoom platform during the term, to prepare students for online learning. During an actual emergency, students will be notified to access the Zoom platform for their online lessons. The class schedule will mirror the current face-to-face class timetable unless otherwise stated."
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-Introduction_to_GAA.html#demystifying-geospatial-analytics",
    "href": "lesson/Lesson01/Lesson01-Introduction_to_GAA.html#demystifying-geospatial-analytics",
    "title": "Lesson 1: Introduction to Geospatial Analytics and Applications",
    "section": "",
    "text": "A Geographical Information System (GIS) is a toolkit for creating, managing, analysing, visualising, and sharing data of any kind according to where it’s located.\n\n\n\n\n\n\n\nGeospatial analytics is more than a GIS."
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-Introduction_to_GAA.html#demystifying-geospatial-analytics-1",
    "href": "lesson/Lesson01/Lesson01-Introduction_to_GAA.html#demystifying-geospatial-analytics-1",
    "title": "Lesson 1: Introduction to Geospatial Analytics and Applications",
    "section": "",
    "text": "Geospatial analytics is more than data visualisation\n\n\n\n\n\nSource: Singapore’s first disease map delivers real-time information on infectious diseases"
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-Introduction_to_GAA.html#demystifying-geospatial-analytics-2",
    "href": "lesson/Lesson01/Lesson01-Introduction_to_GAA.html#demystifying-geospatial-analytics-2",
    "title": "Lesson 1: Introduction to Geospatial Analytics and Applications",
    "section": "",
    "text": "Geospatial analytics is more than just mapping."
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-Introduction_to_GAA.html#motivation-of-geospatial-analytics",
    "href": "lesson/Lesson01/Lesson01-Introduction_to_GAA.html#motivation-of-geospatial-analytics",
    "title": "Lesson 1: Introduction to Geospatial Analytics and Applications",
    "section": "",
    "text": "About 80% of all data maintained by organisations around the world has a location component.\n\n\n\n\n\n\n\n(Source: BusinessWeek Research Services, 2006)"
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-Introduction_to_GAA.html#motivation-of-geospatial-analytics-1",
    "href": "lesson/Lesson01/Lesson01-Introduction_to_GAA.html#motivation-of-geospatial-analytics-1",
    "title": "Lesson 1: Introduction to Geospatial Analytics and Applications",
    "section": "",
    "text": "Geospatial information in Smart Nation.\n\n\n\n\n\n\n\nSee more at this link"
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-Introduction_to_GAA.html#motivation-of-geospatial-analytics-2",
    "href": "lesson/Lesson01/Lesson01-Introduction_to_GAA.html#motivation-of-geospatial-analytics-2",
    "title": "Lesson 1: Introduction to Geospatial Analytics and Applications",
    "section": "",
    "text": "The explosion in the availability of open geospatial data from both the public and private sectors at national and international levels."
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-Introduction_to_GAA.html#motivation-of-geospatial-analytics-3",
    "href": "lesson/Lesson01/Lesson01-Introduction_to_GAA.html#motivation-of-geospatial-analytics-3",
    "title": "Lesson 1: Introduction to Geospatial Analytics and Applications",
    "section": "",
    "text": "The national geospatial master plan.\n\n\n\n\n\n\n\nSource: Singapore Geospatial Master Plan"
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-Introduction_to_GAA.html#the-role-of-geospatial-analytics",
    "href": "lesson/Lesson01/Lesson01-Introduction_to_GAA.html#the-role-of-geospatial-analytics",
    "title": "Lesson 1: Introduction to Geospatial Analytics and Applications",
    "section": "",
    "text": "Uncovering insights not found in statistical graphs and tables.\n\n\n\nSource: SGSAS - Simple Geo-Spatial Analysis using R Shiny"
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-Introduction_to_GAA.html#the-role-of-geospatial-analytics-1",
    "href": "lesson/Lesson01/Lesson01-Introduction_to_GAA.html#the-role-of-geospatial-analytics-1",
    "title": "Lesson 1: Introduction to Geospatial Analytics and Applications",
    "section": "",
    "text": "To reveal the untapped property of spatial contiguity in geographic knowledge discovery in databases.\n\n\n\n\n\n\n\nSource: SGSAS - Simple Geo-Spatial Analysis using R Shiny"
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-Introduction_to_GAA.html#the-role-of-geospatial-analytics-2",
    "href": "lesson/Lesson01/Lesson01-Introduction_to_GAA.html#the-role-of-geospatial-analytics-2",
    "title": "Lesson 1: Introduction to Geospatial Analytics and Applications",
    "section": "",
    "text": "To uncover the complexity of the real world relationship.\n\n\n\n\n\n\n\nSource: SGSAS - Simple Geo-Spatial Analysis using R Shiny"
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-Introduction_to_GAA.html#the-role-of-geospatial-analytics-3",
    "href": "lesson/Lesson01/Lesson01-Introduction_to_GAA.html#the-role-of-geospatial-analytics-3",
    "title": "Lesson 1: Introduction to Geospatial Analytics and Applications",
    "section": "",
    "text": "To model spatial interactions and flows.\n\n\n\n\n\n\n\nSource: IS415 Bus Rider Flow Project"
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-Introduction_to_GAA.html#geospatial-analytics-and-social-consciousness",
    "href": "lesson/Lesson01/Lesson01-Introduction_to_GAA.html#geospatial-analytics-and-social-consciousness",
    "title": "Lesson 1: Introduction to Geospatial Analytics and Applications",
    "section": "",
    "text": "The true power of geospatial analytics is to provide decision makers and planners with data-driven and process information for better problem solving and more efficient use of resources."
  },
  {
    "objectID": "lesson/Lesson03/Lesson03-SPPA-2.html#content",
    "href": "lesson/Lesson03/Lesson03-SPPA-2.html#content",
    "title": "Lesson 3: Spatial Point Patterns Analysis",
    "section": "",
    "text": "Introducing Spatial Point Patterns\n\nThe basic concepts of spatial point patterns\n1st Order versus 2nd Order\nSpatial Point Patterns in real world\n\n1st Order Spatial Point Patterns Analysis\n\nKernel Density Estimation (KDE)\nNetwork Constrained Kernel Density Estimation (NKDE)\nTemporal Network Kernel Density Estimation (TNKDE)\n\n\n\n\n2nd Order Spatial Point Patterns Analysis\n\nNearest Neighbour Index\nG-function\nF-function\nK-function\nL-function\n\n\n\n\n\nWelcome to Lesson 4: Spatial Point Pattern Analysis, a family of spatial statistics specially developed to model and to test distribution of spatial point events.\nThe lesson proceeding is divided into three main section. First, I will share with you what are real world spatial point events. This is followed by explaining the concepts and methods of 1st-order and 2nd-order spatial point patterns analysis."
  },
  {
    "objectID": "lesson/Lesson03/Lesson03-SPPA-2.html#what-is-spatial-point-patterns",
    "href": "lesson/Lesson03/Lesson03-SPPA-2.html#what-is-spatial-point-patterns",
    "title": "Lesson 3: Spatial Point Patterns Analysis",
    "section": "What is Spatial Point Patterns",
    "text": "What is Spatial Point Patterns\n\nPoints as Events\nMapped pattern\n\nNot a sample\nSelection bias\n\nEvents are mapped, but non-events are not\n\n\n\nSpatial Point Patterns in Real World\n\nDistribution of dieses such as dengue fever.\n\n\n\n\n\nSpatial Point Patterns in Real World\n\nDistribution of car collisions.\n\n\n\n\n\n\n\n\n\nSpatial Point Patterns in Real World\n\nDistribution of crime incidents.\n\n\n\n\n\n\n\n\n\nSpatial Point Patterns in Real World\n\nDistribution of public services such as education institutions.\n\n\n\n\n\nSpatial Point Patterns in Real World\n\nLocations of the different channel stores.\n\n\n\n\n\n\n\n\n\nSpatial Point Patterns in Real World\n\nDistribution of social media data such as tweets."
  },
  {
    "objectID": "lesson/Lesson03/Lesson03-SPPA-2.html#real-world-question",
    "href": "lesson/Lesson03/Lesson03-SPPA-2.html#real-world-question",
    "title": "Lesson 3: Spatial Point Patterns Analysis",
    "section": "Real World Question",
    "text": "Real World Question\n\nLocation only\n\nare points randomly located or patterned\n\nLocation and value\n\nmarked point pattern\nis combination of location and value random or patterned\n\nWhat is the underlying process?\n\n\nIt is important note that SPPA is exploratory and confirmatory in nature. They are specially developed for describing the spatial point pattern and for confirming the observed patterns statistically. However, they are explanatory nor for prediction."
  },
  {
    "objectID": "lesson/Lesson03/Lesson03-SPPA-2.html#points-on-a-plane",
    "href": "lesson/Lesson03/Lesson03-SPPA-2.html#points-on-a-plane",
    "title": "Lesson 3: Spatial Point Patterns Analysis",
    "section": "Points on a Plane",
    "text": "Points on a Plane\n\nClassic point pattern analysis\n\npoints on an isotropic plane\nno effect of translation and rotation\nclassic examples: tree seedlings, rocks, etc\n\nDistance\n\nstraight line only\n\n\n\n\nReal world spatial point patterns\n\nIs this a random distribution?\n\n\n\n\n\nReal world spatial point patterns\n\nIs this a random distribution?"
  },
  {
    "objectID": "lesson/Lesson03/Lesson03-SPPA-2.html#spatial-point-patterns-analysis",
    "href": "lesson/Lesson03/Lesson03-SPPA-2.html#spatial-point-patterns-analysis",
    "title": "Lesson 3: Spatial Point Patterns Analysis",
    "section": "Spatial Point Patterns Analysis",
    "text": "Spatial Point Patterns Analysis\n\nPoint pattern analysis (PPA) is the study of the spatial arrangements of points in (usually 2-dimensional) space.\nThe simplest formulation is a set X = {x ∈ D} where D, which can be called the study region, is a subset of Rn, a n-dimensional Euclidean space.\nA fundamental problem of PPA is inferring whether a given arrangement is merely random or the result of some process."
  },
  {
    "objectID": "lesson/Lesson03/Lesson03-SPPA-2.html#spatial-point-patterns-analysis-techniques",
    "href": "lesson/Lesson03/Lesson03-SPPA-2.html#spatial-point-patterns-analysis-techniques",
    "title": "Lesson 3: Spatial Point Patterns Analysis",
    "section": "Spatial Point Patterns Analysis Techniques",
    "text": "Spatial Point Patterns Analysis Techniques\n\nFirst-order vs Second-order Analysis of spatial point patterns.\n\n\nReference: 11.4 First and second order effects of Intro to GIS and Spatial Analysis\n\nThe first-order properties describe the way in which the expected value (mean or average) of the spatial point pattern varies across space (i.e., the intensity of the spatial point pattern). Such properties are usually measured with the so-called quadrat analysis, nearest neighbour index and kernel estimation. Second-order properties describe the covariance (or correlation) between values of the spatial point pattern at different regions in space and are usually measured with the G function, K function and L function. Applied to point event data, both properties could be used to explore the spatial variation in the risk of being victimized by a crime, spatial and space-time clustering of criminal activities, and the raised incidence of criminal activities around point sources, such as robberies around ATM machines, subway entrances and exits, etc.\n\n\n\nFirst-order Spatial Point Patterns Analysis Techniques\n\nDensity-based\n\nKernel density estimation\nQuadrat analysis,\n\nDistance-based\n\nNearest Neighbour Index\n\n\n\n\n\nBasic concept of density-based measures"
  },
  {
    "objectID": "lesson/Lesson03/Lesson03-SPPA-2.html#kernel-density-estimation-silverman-1986",
    "href": "lesson/Lesson03/Lesson03-SPPA-2.html#kernel-density-estimation-silverman-1986",
    "title": "Lesson 3: Spatial Point Patterns Analysis",
    "section": "Kernel density estimation (Silverman 1986)",
    "text": "Kernel density estimation (Silverman 1986)\n\nA method to compute the intensity of a point distribution.\n\n\n\nThe general formula:\n\n\nGraphically\n\n\n\n\n\nKDE Step 1: Computing point intensity\n\n\n\n\nKDE Step 2: Spatial interpolation using kernel function\n\n\n\n\nKDE Map of Childcare Services, Singapore\n\n\n\n\nAdaptive Bandwidth\n\n\nAdaptive schemes adjust itself according to the density of data: - Shorter bandwidths where data are dense and longer where sparse.\n\nFinding nearest neighbors are one of the often used approaches.\n\n\n\n\n\n\n\n\nFixed bandwidth\n\n\n\nMight produce large estimate variances where data are sparse, while mask subtle local variations where data are dense.\nIn extreme condition, fixed schemes might not be able to calibrate in local areas where data are too sparse to satisfy the calibration requirements (observations must be more than parameters)."
  },
  {
    "objectID": "lesson/Lesson03/Lesson03-SPPA-2.html#network-constrained-kernel-density-estimation-nkde",
    "href": "lesson/Lesson03/Lesson03-SPPA-2.html#network-constrained-kernel-density-estimation-nkde",
    "title": "Lesson 3: Spatial Point Patterns Analysis",
    "section": "Network Constrained Kernel Density Estimation (NKDE)",
    "text": "Network Constrained Kernel Density Estimation (NKDE)\nMany real world point event are not randomly distributed. Their distribution, on the other hand, are constrained by network such as roads, rivers, and fault lines just to name a few of them.\n\n\n\nNetwork Constrained Kernel Density Estimation (NKDE) method\nTo calculate a Network Kernel Density Estimate (NKDE), it is possible to:\n\nuse lixels instead of pixels. A lixel is a linear equivalent of a pixel on a network. The lines of the network are split into lixels according to a chosen resolution. The centres of the lixels are sampling points for which the density will be estimated.\ncalculate network distances between objects instead of Euclidean distances.\nadjust the kernel function to deal with the anisotropic space\n\n\n\n\nNKDE Method\n\nTo perform a NKDE, the events must be snapped on the network. The snapped events are shown here in green.\n\n\n\n\n\n\n\n\n\n\n\nNKDE Method\n\n\nThe mass of each event can be seen as a third dimension and is evaluated by a selected kernel function (K) within a specified bandwidth. The kernel function must satisfy the following conditions:\n\nThe total mass of an event is 1, and is spread according to the function K within the bandwidth.\n\nIn the figure below, we can see that the “influence” of each point is limited within the bandwidth and decreases when we move away from the event.\n\n\n\n\n\n\nNKDE Method\n\n\nWith this method, one can evaluate the density of the studied phenomenon at each location on the network. In the figure below, 3 sampling points (s1, s2 and s3) are added in blue.\n\n\nWhere S1, S2, and S3 are calculated by using the formulas below:\n\nand the general formular will be defined as:\n\n\n\n\n\nwith 𝒅𝑠𝑖 the density estimated at the sample point 𝑠𝒊, 𝒃𝑤 the bandwidth and 𝑒𝑗 an event.\n\n\n\n\n\nNKDE Method\n\n\nThe proposed kernel functions in the spNetwork package are:"
  },
  {
    "objectID": "lesson/Lesson03/Lesson03-SPPA-2.html#temporal-network-kernel-density-estimate",
    "href": "lesson/Lesson03/Lesson03-SPPA-2.html#temporal-network-kernel-density-estimate",
    "title": "Lesson 3: Spatial Point Patterns Analysis",
    "section": "Temporal Network Kernel Density Estimate",
    "text": "Temporal Network Kernel Density Estimate\n\n\nEvents recorded on a network often have a temporal dimension. In that context, one could estimate the density of events in both time and network spaces.\nThe spatio-temporal kernel is calculated as the product of the network kernel density and the time kernel density.\n\nFor a sample point at location l and time t, the Temporal Network Kernel Density Estimate (TNKDE) is calculated as follows:\n\n\n\n\n\n\nQuadrat Analysis – Step 1\n\n\n\nDivide the study area into subregion of equal size,\n\noften squares, but don’t have to be.\n\n\n\n\n\n\n\n\nQuadrat Analysis – Step 2\n\n\n\nCount the frequency of events in each region.\n\n\n\n\n\n\n\nQuadrat Analysis – Step 3\n\n\n\nCalculate the intensity of events in each region.\n\n\n\n\n\n\n\nQuadrat Analysis – Step 4\n\nCalculate the quadrat statistics and perform CSR test.\n\n\n\n\n\nQuadrat Analysis – Variance-Mean Ratio (VMR)\n\nFor an uniform distribution, the variance is zero, - therefore, we expect a variance-mean ratio close to 0.\nFor a random distribution, the variance and mean are the same,\n\ntherefore, we expect a variance-mean ratio close to 1.\n\nFor a cluster distribution, the variance is relatively large,\n\ntherefore, we expect a variance-mean ratio greater than 1.]\n\n\n\n\n\nComplete Spatial Randomness (CSR)\n\n\n\nCSR/IRP satisfy two conditions: - Any event has equal probability of being in any location, a 1st order effect.\n\nThe location of one event is independent of the location of another event, a 2nd order effect.\n\n\nReference: Chapter 12 Hypothesis testing of Intro to GIS and Spatial Analysis\n\n\n\n\n\n\n\nQuadrat Analysis: The interpretation\n\n\n\n\n\n\n\n\nWeaknesses of quadrat analysis\n\n\n\nIt is sensitive to the quadrat size.\n\nIf the quadrat size is too small, they may contain only a couple of points, and\nIf the quadrat size is too large, they may contain too many points.\n\nIt is a measure of dispersion rather than a measure of pattern.\nIt results in a single measure for the entire distribution, so variation within the region are not recognised."
  },
  {
    "objectID": "lesson/Lesson03/Lesson03-SPPA-2.html#distance-based-nearest-neighbour-index",
    "href": "lesson/Lesson03/Lesson03-SPPA-2.html#distance-based-nearest-neighbour-index",
    "title": "Lesson 3: Spatial Point Patterns Analysis",
    "section": "Distance-based: Nearest Neighbour Index",
    "text": "Distance-based: Nearest Neighbour Index\n\nWhat is Nearest Neighbour?\nDirect distance from a point to its nearest neighbour."
  },
  {
    "objectID": "lesson/Lesson03/Lesson03-SPPA-2.html#nearest-neighbour-index",
    "href": "lesson/Lesson03/Lesson03-SPPA-2.html#nearest-neighbour-index",
    "title": "Lesson 3: Spatial Point Patterns Analysis",
    "section": "Nearest Neighbour Index",
    "text": "Nearest Neighbour Index\nThe Nearest Neighbour Index is expressed as the ratio of the Observed Mean Distance to the Expected Mean Distance.\n\n\n\nCalculating Nearest Neighbour Index\n\n\n\n\nInterpreting Nearest Neighbour Index\nThe expected distance is the average distance between neighbours in a hypothetical random distribution.\n\nIf the index is less than 1, the pattern exhibits clustering,\nIf the index is equal to 1, the patterns exhibits random, and\nIf the index is greater than 1, the trend is toward dispersion or competition.\n\n\n\n\n\n\n\n\n\nThe test statistics\n\n\n\nNull Hypothesis: Points are randomly distributed\nTest statistics:\n\n\n\nReject the null hypothesis if the z-score is large and p-value is smaller than the alpha value.\n\n\n\n\n\n\nInterpreting Nearest Neighbour Index\n\n\n\nThe p-value is smaller than 0.05 =&gt; Reject the null hypothesis that the point patterns are randomly distributed."
  },
  {
    "objectID": "lesson/Lesson03/Lesson03-SPPA-2.html#g-function",
    "href": "lesson/Lesson03/Lesson03-SPPA-2.html#g-function",
    "title": "Lesson 3: Spatial Point Patterns Analysis",
    "section": "G function",
    "text": "G function\n\n\nThe formula\n\n\n\n\n\n\n\nInterpretation of G-function\n\n\nThe shape of G-function tells us the way the events are spaced in a point pattern.\n\nClustered: G increases rapidly at short distance.\nEvenness: G increases slowly up to distance where most events spaced, then increases rapidly.\n\n\n\n\n\n\n\n\nHow do we tell if G is significant?\n\n\n\nThe significant of any departure from CSR (either cluster or regularity) can be evaluated using simulated “confidence envelopes”\n\n\n\n\n\n\n\n\nMonte Carlo simulation test of CSR\n\nPerform m independent simulation of n events (i.e. 999) in the study region.\nFor each simulated point pattern, estimate G(r) and use the maximum (95th) and minimum (5th) of these functions for the simulated patterns to define an upper and lower simulation envelope.\nIf the estimated G(r) lies above the upper envelope or below the lower envelope, the estimated G(r) is statistically significant.\n\n\n\n\nThe significant test of G-function"
  },
  {
    "objectID": "lesson/Lesson03/Lesson03-SPPA-2.html#f-function",
    "href": "lesson/Lesson03/Lesson03-SPPA-2.html#f-function",
    "title": "Lesson 3: Spatial Point Patterns Analysis",
    "section": "F function",
    "text": "F function\n\nSelect a sample of point locations anywhere in the study region at random\n\nDetermine minimum distance from each point to any event in the study area.\n\nThree steps:\n\nRandomly select m points (p1, p2, ….., pn),\nCalculate dmin(pi,s) as the minimum distance from location pi to any event in the point patterns, and\nCalculate F(d).\n\n\n\n\nThe F function formula\n\n\n\n\n\n\n\n\nInterpretation of F-function\n\nClustered = F(r) rises slowly at first, but more rapidly at longer distances.\nEvenness = F(r) rises rapidly at first, then slowly at longer distances.\n\n\n\n\nThe significant test of F-function\n\n\n\n\nComparison between G and F"
  },
  {
    "objectID": "lesson/Lesson03/Lesson03-SPPA-2.html#ripleys-k-function-ripley-1981",
    "href": "lesson/Lesson03/Lesson03-SPPA-2.html#ripleys-k-function-ripley-1981",
    "title": "Lesson 3: Spatial Point Patterns Analysis",
    "section": "Ripley’s K function (Ripley, 1981)",
    "text": "Ripley’s K function (Ripley, 1981)\n\nLimitation of nearest neighbor distance method is that it uses only nearest distance\nConsiders only the shortest scales of variation.\nK function uses more points.\n\nProvides an estimate of spatial dependence over a wider range of scales.\nBased on all the distances between events in the study area.\nAssumes isotropy over the region.\n\n\n\n\nCalculating the K function\n\n\n\nConstruct a circle of radius h around each point event(i).\nCount the number of other events (j) that fall inside this circle.\nRepeat these two steps for all points (i) and sum results.\nIncrement h by a small amount and repeat the calculation.\n\n\n\n\n\n\n\n\nK function\n\n\nThe formula:\n\n\n\n\n\n\nThe K function complete spatial randomness test\n\nK(h) can be plotted against different values of h.\nBut what should K look like for no spatial dependence?\nConsider what K(h) should look like for a random point process (CSR)\n\nThe probability of an event at any point in R is independent of what other events have occurred and equally likely anywhere in R\n\n\n\n\n\nInterpreting the K function complete spatial randomness test\n\n\nUnder the assumption of CSR, the expected number of events within distance h of an event is:\n\nCompare K(h) to 𝜋ℎ^2\n\nK(h) &lt; 𝜋ℎ^2 if point pattern is regular\nK(h) &gt; 𝜋ℎ^2 if point pattern is clustered\n\n\n\n\nAbove the envelop: significant cluster pattern - Below the envelop: significant regular\nInside the envelop: CSR"
  },
  {
    "objectID": "lesson/Lesson03/Lesson03-SPPA-2.html#the-l-function-besag-1977",
    "href": "lesson/Lesson03/Lesson03-SPPA-2.html#the-l-function-besag-1977",
    "title": "Lesson 3: Spatial Point Patterns Analysis",
    "section": "The L function (Besag 1977)",
    "text": "The L function (Besag 1977)\n\n\nIn practice, K function will be normalised to obtained a benchmark of zero.\nThe formula:\n\n\n\n\n\nInterpreting the L function complete spatial randomness test\n\n\n\n\nWhen an observed L value is greater than its corresponding L(theo)(i.e. red break line) value for a particular distance and above the upper confidence envelop, spatial clustering for that distance is statistically significant (e.g. distance beyond C).\nWhen an observed L value is greater than its corresponding L(theo) value for a particular distance and lower than the upper confidence envelop, spatial clustering for that distance is statistically NOT significant (e.g. distance between B and C).\nWhen an observed L value is smaller than its corresponding L(theo) value for a particular distance and beyond the lower confidence envelop, spatial dispersion for that distance is statistically significant. - When an observed L value is smaller than its corresponding L(theo) value for a particular distance and within the lower confidence envelop, spatial dispersion for that distance is statistically NOT significant (e.g. distance between A and B).\n\n\n\n\n\nThe grey zone indicates the confident envelop (i.e. 95%).\n\n\n\n\n\n\n\n\nThe L function (Besag 1977)\n\n\nThe modified L function\n\n\nL(r)&gt;0 indicates that the observed distribution is geographically concentrated.\nL(r)&lt;0 implies dispersion.\nL(r)=0 indicates complete spatial randomness (CRS)."
  },
  {
    "objectID": "lesson/Lesson03/Lesson03-SPPA-2.html#references",
    "href": "lesson/Lesson03/Lesson03-SPPA-2.html#references",
    "title": "Lesson 3: Spatial Point Patterns Analysis",
    "section": "References",
    "text": "References\n\nChapter 11 Point Pattern Analysis of Intro to GIS and Spatial Analysis. Section 11.2, 11.3, 11.3.1 and 11.4\nGIS&T Body of Knowledge AM-07-Point Pattern Analysis\nGIS&T Body of Knowledge AM-08-Kernels and Density Estimation\nAnalyzing Patterns in Business Point Data, Directions Magazine March 17, 2005.\nOkabe, Atsuyuki, and Kokichi Sugihara. 2012. “Spatial Analysis Along Networks: Statistical and Computational Methods”. John Wiley & Sons.\nSugihara, Kokichi, Toshiaki Satoh, and Atsuyuki Okabe. 2010. “Simple and Unbiased Kernel Function for Network Analysis.” In 2010 10th International Symposium on Communications and Information Technologies, 827–32. IEEE.\nXie, Zhixiao, and Jun Yan. 2008. “Kernel Density Estimation of Traffic Accidents in a Network Space.” Computers, Environment and Urban Systems 32 (5): 396–406. Contents"
  },
  {
    "objectID": "lesson.html",
    "href": "lesson.html",
    "title": "Weekly Lesson Plan",
    "section": "",
    "text": "Week\nDate\nTopic\nRemark\n\n\n\n\n1\n19/8/2024\nLesson 1: Introduction to Geospatial Analytics\n\n\n\n2\n26/8/2024\nLesson 2: Fundamental of Geospatial Data Visualisation and tmap Methods\n\n\n\n3\n2/9/2024\nLesson 3: Spatial Point Patterns Analysis\n\n\n\n4\n9/9/2024\nLesson 4: Spatio-Temporal Point Patterns Analysis\n\n\n\n5\n16/9/2024\nLesson 5: Spatial Weights and Applications\n\n\n\n6\n23/9/2024\nLesson 6: Measures of Global and Local Spatial Association\n\n\n\n7\n30/9/2024\nLesson 7: Shiny for Building Web-enabled Geospatial Analytics Application\n\n\n\n8\n7/10/2024\nRecess week\n\n\n\n9\n14/10/2024\nLesson 8: Hierarchical Clustering for Multivariate Geographically Referenced Data\n\n\n\n10\n21/10/2024\nLesson 9: Spatially Constrained Clustering Analysis\n\n\n\n11\n28/10/2024\nLesson 10: Regression Modelling of Geographically Reference Data\n\n\n\n12\n4/11/2024\nLesson 11: Geographically Weighted Regression\n\n\n\n13\n11/11/2024\nLesson 12: Geographically Weighted Predictive Modelling\n\n\n\n14\n18/11/2024\nTownhall Presentation\n\n\n\n15"
  },
  {
    "objectID": "outline/Lesson02_outline.html",
    "href": "outline/Lesson02_outline.html",
    "title": "Lesson 2: Fundamental of Geospatial Data Visualisation and tmap Methods",
    "section": "",
    "text": "In this lesson, the basic principles and concepts of thematic mapping and geovisualisation will be introduced. You will also gain hands-on experience on using tmap package to build cartographic quality thematic maps."
  },
  {
    "objectID": "outline/Lesson02_outline.html#overview",
    "href": "outline/Lesson02_outline.html#overview",
    "title": "Lesson 2: Fundamental of Geospatial Data Visualisation and tmap Methods",
    "section": "",
    "text": "In this lesson, the basic principles and concepts of thematic mapping and geovisualisation will be introduced. You will also gain hands-on experience on using tmap package to build cartographic quality thematic maps."
  },
  {
    "objectID": "outline/Lesson02_outline.html#content",
    "href": "outline/Lesson02_outline.html#content",
    "title": "Lesson 2: Fundamental of Geospatial Data Visualisation and tmap Methods",
    "section": "Content",
    "text": "Content\n\nGeospatial Visualisation\n\nClassification of maps\nPrinciples of map design\nThematic mapping techniques\nAnalytical mapping techniques"
  },
  {
    "objectID": "outline/Lesson02_outline.html#lesson-slides",
    "href": "outline/Lesson02_outline.html#lesson-slides",
    "title": "Lesson 2: Fundamental of Geospatial Data Visualisation and tmap Methods",
    "section": "Lesson Slides",
    "text": "Lesson Slides\n\nLesson 2: Fundamental of Geospatial Data Visualisation and tmap Methods slides."
  },
  {
    "objectID": "outline/Lesson02_outline.html#hands-on-exercise",
    "href": "outline/Lesson02_outline.html#hands-on-exercise",
    "title": "Lesson 2: Fundamental of Geospatial Data Visualisation and tmap Methods",
    "section": "Hands-on Exercise",
    "text": "Hands-on Exercise\n\nChoropleth Mapping with R\n\n\nSelf-reading Before Meet-up\nTo read before class:\n\nProportional Symbols\nChoropleth Maps\nThe Basics of Data Classification\nChoropleth Mapping with Exploratory Data Analysis\nTennekes, M. (2018) “tmap: Thematic Maps in R”, Journal of Statistical Software, Vol 84:6, 1-39."
  },
  {
    "objectID": "outline/Lesson02_outline.html#all-about-r",
    "href": "outline/Lesson02_outline.html#all-about-r",
    "title": "Lesson 2: Fundamental of Geospatial Data Visualisation and tmap Methods",
    "section": "All About R",
    "text": "All About R\n\ntmap: thematic maps in R package especially:\n\ntmap: get started!,\ntmap: version changes, and\nChapter 8 Making maps with R of Geocomputation with R."
  },
  {
    "objectID": "outline/Lesson02_outline.html#reference",
    "href": "outline/Lesson02_outline.html#reference",
    "title": "Lesson 2: Fundamental of Geospatial Data Visualisation and tmap Methods",
    "section": "Reference",
    "text": "Reference\n\nThe Concept of Map Symbols\nChoropleth map\nChoropleth Maps – A Guide to Data Classification\nBivariate Choropleth\nValue-by-alpha maps\nWhat to consider when creating choropleth maps"
  },
  {
    "objectID": "outline/Lesson04_outline.html",
    "href": "outline/Lesson04_outline.html",
    "title": "Lesson 4: Spatio-Temporal Point Patterns Analysis",
    "section": "",
    "text": "A spatio-temporal point process (also called space-time or spatial-temporal point process) is a random collection of points, where each point represents the time and location of an event. Examples of events include incidence of disease, sightings or births of a species, or the occurrences of fires, earthquakes, lightning strikes, tsunamis, or volcanic eruptions. In this lesson, you will learn the basic concepts and methods of Spatio-temporal Point Patterns Analysis. You will also gain hands-on experience on using these methods to discover real-world point processes."
  },
  {
    "objectID": "outline/Lesson04_outline.html#content",
    "href": "outline/Lesson04_outline.html#content",
    "title": "Lesson 4: Spatio-Temporal Point Patterns Analysis",
    "section": "Content",
    "text": "Content\n\nSpatio-temporal Point Processes\n\nBasic concepts of spatio-temporal point processes.\nExamples of real-world spatio-temporal point processes.\n\nSpatio-temporal Point Patterns Analysis methods\n\nSpatio-temporal Kernel Density Estimation (STKDE)\nSpatio-temporal K-functions\n\n\n\nLesson Slides and Hands-on Notes\n\nLesson 4 slides\nHands-on Exercise 4: Spatio-Temporal Point Patterns Analysis."
  },
  {
    "objectID": "outline/Lesson04_outline.html#references",
    "href": "outline/Lesson04_outline.html#references",
    "title": "Lesson 4: Spatio-Temporal Point Patterns Analysis",
    "section": "References",
    "text": "References\n\nJonatan A. González, et. al. (2016) “Spatio-temporal point process statistics: A review”, Spatial Statistics, Volume 18, Part B, November 2016, Pages 505-544.\nAlexander Hohl et. al.“Spatiotemporal Point Pattern Analysis Using Ripley’s K” in Geospatial Data Science Techniques and Applications.\nTonini, Marj et. al. (2017) “Evolution of forest fires in Portugal: from spatio-temporal point events to smoothed density maps”, Natural hazards (Dordrecht), 2017-02, Vol.85 (3), p.1489-1510. Available at SMU eJournal.\nJuan, P et. al. (2012) “Pinpointing spatio-temporal interactions in wildfire patterns”, Stochastic environmental research and risk assessment, 2012-12, Vol.26 (8), p.1131-1150. Available at SMU eJournal."
  },
  {
    "objectID": "outline/Lesson04_outline.html#all-about-r",
    "href": "outline/Lesson04_outline.html#all-about-r",
    "title": "Lesson 4: Spatio-Temporal Point Patterns Analysis",
    "section": "All About R",
    "text": "All About R\n\nGonzález & Moraga, “Non-Parametric Analysis of Spatial and Spatio-Temporal Point Patterns”, The R Journal, 2023\nstpp\n\nstpp: An R Package for Plotting, Simulating and Analyzing Spatio-Temporal Point Patterns, Journal of Statistical Software (JSS), April 2013, Volume 53, Issue 2.\n\nsparr: Spatial and Spatiotemporal Relative Risk\nstopp\n\nstpp: An R Package for Plotting, Simulating and Analysing Spatio-Temporal Point Patterns"
  },
  {
    "objectID": "outline/Lesson05_outline.html",
    "href": "outline/Lesson05_outline.html",
    "title": "Lesson 5: Spatial Weights and Applications",
    "section": "",
    "text": "Computing spatial weight is an essential step toward measuring the strength of the spatial relationships between objects. In this lesson, the basic concept of spatial weight will be introduced. This is followed by a discussion of methods to compute spatial weights."
  },
  {
    "objectID": "outline/Lesson05_outline.html#content",
    "href": "outline/Lesson05_outline.html#content",
    "title": "Lesson 5: Spatial Weights and Applications",
    "section": "Content",
    "text": "Content\n\nTobler’s First law of Geography\nPrinciples of Spatial Autocorrelation\nConcepts of Spatial Proximity and Spatial Weights\n\nContiguity-Based Spatial Weights: Rook’s & Queen’s\nDistance-Band Spatial Weights: fixed and adaptive\n\nApplications of Spatial Weights\n\nSpatially Lagged Variables\nGeographically Weighted Summary Statistics"
  },
  {
    "objectID": "outline/Lesson05_outline.html#lesson-slides-and-hands-on-notes",
    "href": "outline/Lesson05_outline.html#lesson-slides-and-hands-on-notes",
    "title": "Lesson 5: Spatial Weights and Applications",
    "section": "Lesson Slides and Hands-on Notes",
    "text": "Lesson Slides and Hands-on Notes\n\nLesson 5 slides.\nHands-on Exercise 5\n\n\nSelf-reading Before Meet-up\nTo read before class:\n\nChapter 2. Codifying the neighbourhood structure of Handbook of Spatial Analysis: Theory and Application with R.\n\nAlternatively\n\nChapter 9: Modelling Areal Data of Applied Spatial Data Analysis with R (2nd Edition). This book is available in smu digital library. Until section 9.3.1."
  },
  {
    "objectID": "outline/Lesson05_outline.html#references",
    "href": "outline/Lesson05_outline.html#references",
    "title": "Lesson 5: Spatial Weights and Applications",
    "section": "References",
    "text": "References\n\nFrançois Bavaud (2010) “Models for Spatial Weights: A Systematic Look” Geographical Analysis, Vol. 30, No.2, pp 153-171.\nTony H. Grubesic and Andrea L. Rosso (2014) “The Use of Spatially Lagged Explanatory Variables for Modeling Neighborhood Amenities and Mobility in Older Adults”, Cityscape, Vol. 16, No. 2, pp. 205-214."
  },
  {
    "objectID": "outline/Lesson05_outline.html#all-about-r",
    "href": "outline/Lesson05_outline.html#all-about-r",
    "title": "Lesson 5: Spatial Weights and Applications",
    "section": "All About R",
    "text": "All About R\n\nspdep package\nsfdep package\nrgeoda package"
  },
  {
    "objectID": "outline/Lesson07_outline.html",
    "href": "outline/Lesson07_outline.html",
    "title": "Lesson 7: Building Web-enabled Geospatial Analytics Applications with R Shiny",
    "section": "",
    "text": "Motivation of developing web-enabled applications\n\nBasic principles of web applications\n\nIntroducing R Shiny\n\nGetting to know Shiny\nArchitecture of Shiny\nBuilding block of Shiny app\n\nAdvanced R Shiny\n\nReactive feature of Shiny\nBuild interactive Shiny application by using plotly R\nBuild static, interactive and reactive geovisualisation application by using tmap\n\nManaging R Shiny Project\n\nThe basic development cycle of creating apps\nDebug errors in the codes\nBuild complex Shiny application using module\nImprove the productivity of Shiny applications development\n\nDeploying Shiny Application\n\nDeploy to the cloud: shinyapps.io\nDeploy on-premises (open source): Shiny Server\nDeploy on-premises (commercial): RStudio Connect"
  },
  {
    "objectID": "outline/Lesson07_outline.html#content",
    "href": "outline/Lesson07_outline.html#content",
    "title": "Lesson 7: Building Web-enabled Geospatial Analytics Applications with R Shiny",
    "section": "",
    "text": "Motivation of developing web-enabled applications\n\nBasic principles of web applications\n\nIntroducing R Shiny\n\nGetting to know Shiny\nArchitecture of Shiny\nBuilding block of Shiny app\n\nAdvanced R Shiny\n\nReactive feature of Shiny\nBuild interactive Shiny application by using plotly R\nBuild static, interactive and reactive geovisualisation application by using tmap\n\nManaging R Shiny Project\n\nThe basic development cycle of creating apps\nDebug errors in the codes\nBuild complex Shiny application using module\nImprove the productivity of Shiny applications development\n\nDeploying Shiny Application\n\nDeploy to the cloud: shinyapps.io\nDeploy on-premises (open source): Shiny Server\nDeploy on-premises (commercial): RStudio Connect"
  },
  {
    "objectID": "outline/Lesson07_outline.html#lesson-slides-and-hands-on-notes",
    "href": "outline/Lesson07_outline.html#lesson-slides-and-hands-on-notes",
    "title": "Lesson 7: Building Web-enabled Geospatial Analytics Applications with R Shiny",
    "section": "Lesson Slides and Hands-on Notes",
    "text": "Lesson Slides and Hands-on Notes\n\nIntroducing R Shiny slides in html format.\nAdvanced R Shiny I slides in html format.\nManaging R Shiny Project slides in html format."
  },
  {
    "objectID": "outline/Lesson07_outline.html#must-do",
    "href": "outline/Lesson07_outline.html#must-do",
    "title": "Lesson 7: Building Web-enabled Geospatial Analytics Applications with R Shiny",
    "section": "Must Do!",
    "text": "Must Do!\n\nView the three parts series of Shiny Tutorial at this link."
  },
  {
    "objectID": "outline/Lesson07_outline.html#readings",
    "href": "outline/Lesson07_outline.html#readings",
    "title": "Lesson 7: Building Web-enabled Geospatial Analytics Applications with R Shiny",
    "section": "Readings",
    "text": "Readings\n\nCore Readings\n\nShiny reference guide at CRAN.\nHadley Wickham (2021) Mastering Shiny, O’Reilly Media. This is a highly recommended book. You can find the online version with this link.\nPaula Moraga (2020) Geospatial Health Data: Modeling and Visualization with R-INLA and Shiny, Chapman & Hall/CRC. Chapter 13, 14 and 15.\n\n\n\nAdditional references\n\nColin Fay, Sébastien Rochette, Vincent Guyader, Cervan Girard (2020), Engineering Production-Grade Shiny Apps, Chapman & Hall. You can find the online version with this link.\nOutstanding User Interfaces with Shiny.\nHow to Build a Shiny Application from Scratch.\n\n\n\nDeploying Shiny Application on shinyapps.io\n\nGetting started with shinyapps.io\nshinyapps.io user guide"
  },
  {
    "objectID": "outline/Lesson07_outline.html#gallery",
    "href": "outline/Lesson07_outline.html#gallery",
    "title": "Lesson 7: Building Web-enabled Geospatial Analytics Applications with R Shiny",
    "section": "Gallery",
    "text": "Gallery\n\nWinners of the 1st Shiny Contest\nWinners of the 2nd Annual Shiny Contest\nWinners of the 3rd annual Shiny Contest\nShiny Gallery\nFifteen New Zealand government Shiny web apps\nShinyApps Gallery\ndreamRs shiny gallery\nTools for Teaching Quantitative Thinking"
  },
  {
    "objectID": "outline/Lesson09_outline.html",
    "href": "outline/Lesson09_outline.html",
    "title": "Lesson 8: Geographically Weighted Regression",
    "section": "",
    "text": "In this lesson, you will learn the basic concepts and methods of geographically weighted regression."
  },
  {
    "objectID": "outline/Lesson09_outline.html#content",
    "href": "outline/Lesson09_outline.html#content",
    "title": "Lesson 9: Spatially Constrained Cluster Analysis",
    "section": "Content",
    "text": "Content\n\nBasic concepts of spatially constrained cluster analysis\nApproaches for clustering geographically referenced data\n\nMinimum spanning trees\nClustGeo method"
  },
  {
    "objectID": "outline/Lesson09_outline.html#lesson-slides-and-hands-on-notes",
    "href": "outline/Lesson09_outline.html#lesson-slides-and-hands-on-notes",
    "title": "Lesson 8: Geographically Weighted Regression",
    "section": "Lesson Slides and Hands-on Notes",
    "text": "Lesson Slides and Hands-on Notes\n\nLesson 8 slides.\nHands-on Exercise 8."
  },
  {
    "objectID": "outline/Lesson09_outline.html#self-reading-before-meet-up",
    "href": "outline/Lesson09_outline.html#self-reading-before-meet-up",
    "title": "Lesson 8: Geographically Weighted Regression",
    "section": "Self-reading Before Meet-up",
    "text": "Self-reading Before Meet-up\nTo read before class:\n\nBrunsdon, C., Fotheringham, A.S., and Charlton, M. (2002) “Geographically weighted regression: A method for exploring spatial nonstationarity”. Geographical Analysis, 28: 281-289.\nBrunsdon, C., Fotheringham, A.S. and Charlton, M., (1999) “Some Notes on Parametric Significance Tests for Geographically Weighted Regression”. Journal of Regional Science, 39(3), 497-524."
  },
  {
    "objectID": "outline/Lesson09_outline.html#references",
    "href": "outline/Lesson09_outline.html#references",
    "title": "Lesson 8: Geographically Weighted Regression",
    "section": "References",
    "text": "References\n\nMennis, Jeremy (2006) “Mapping the Results of Geographically Weighted Regression”, The Cartographic Journal, Vol.43 (2), p.171-179.\nStephen A. Matthews ; Tse-Chuan Yang (2012) “Mapping the results of local statistics: Using geographically weighted regression”, Demographic Research, Vol.26, p.151-166."
  },
  {
    "objectID": "outline/Lesson09_outline.html#all-about-r",
    "href": "outline/Lesson09_outline.html#all-about-r",
    "title": "Lesson 8: Geographically Weighted Regression",
    "section": "All About R",
    "text": "All About R\n\nGWmodel package, especially\n\nGollini, I et. al. (2015) “GWmodel: An R Package for Exploring Spatial Heterogeneity Using Geographically Weighted Models”, Journal of Statistical Software, Volume 63, Issue 17 and\nBinbin Lu, Paul Harris, Martin Charlton & Chris Brunsdon (2014) “The GWmodel R package: further topics for exploring spatial heterogeneity using geographically weighted models”, Geo-spatial Information Science, 17:2, 85-101, DOI: 10.1080/10095020.2014.917453.\n\nlctools package especially gw() and gwr() related functions.\nspgwr implements of geographically weighted regression methods for exploring possible non-stationarity.\ngwrr: its geographically weighted regression (GWR) models and has tools to diagnose and remediate collinearity in the GWR models. Also fits geographically weighted ridge regression (GWRR) and geographically weighted lasso (GWL) models."
  },
  {
    "objectID": "outline/Lesson11_outline.html",
    "href": "outline/Lesson11_outline.html",
    "title": "Lesson 11: Geographically Weighted Regression",
    "section": "",
    "text": "In this lesson, you will learn the basic concepts and methods of geographically weighted regression."
  },
  {
    "objectID": "outline/Lesson11_outline.html#content",
    "href": "outline/Lesson11_outline.html#content",
    "title": "Lesson 11: Geographically Weighted Regression",
    "section": "Content",
    "text": "Content\n\nIntroducing Geographically Weighted Regression\n\nWeighting functions (kernel)\nWeighting schemes\nBandwidth\nInterpreting and Visualising"
  },
  {
    "objectID": "outline/Lesson11_outline.html#lesson-slides-and-hands-on-notes",
    "href": "outline/Lesson11_outline.html#lesson-slides-and-hands-on-notes",
    "title": "Lesson 11: Geographically Weighted Regression",
    "section": "Lesson Slides and Hands-on Notes",
    "text": "Lesson Slides and Hands-on Notes\n\nLesson 11 slides.\nHands-on Exercise 11."
  },
  {
    "objectID": "outline/Lesson11_outline.html#self-reading-before-meet-up",
    "href": "outline/Lesson11_outline.html#self-reading-before-meet-up",
    "title": "Lesson 11: Geographically Weighted Regression",
    "section": "Self-reading Before Meet-up",
    "text": "Self-reading Before Meet-up\nTo read before class:\n\nBrunsdon, C., Fotheringham, A.S., and Charlton, M. (2002) “Geographically weighted regression: A method for exploring spatial nonstationarity”. Geographical Analysis, 28: 281-289.\nBrunsdon, C., Fotheringham, A.S. and Charlton, M., (1999) “Some Notes on Parametric Significance Tests for Geographically Weighted Regression”. Journal of Regional Science, 39(3), 497-524."
  },
  {
    "objectID": "outline/Lesson11_outline.html#reference",
    "href": "outline/Lesson11_outline.html#reference",
    "title": "Lesson 10: Modelling Geographic of Accessibility",
    "section": "Reference",
    "text": "Reference\n\nSection on “Transportation and Accessibility” in The Geography of Transport Systems.\nRich, D.C. (1980) Potential Models in Human Geography.\nOrpana, T./Lampinen, J. (2003) “Building spatial choice models from aggregate data”. Journal of Regional Science,43, 2, p. 319-347.\nTwo-step floating catchment area method.\nCheng, Gang et. al. (2016) “Spatial difference analysis for accessibility to high level hospitals based on travel time in Shenzhen, China” Habitat International, Vol.53, p.485-494.\nPolzin, Pierre ; Borges, José ; Coelho, António (2014) “An Extended Kernel Density Two-Step Floating Catchment Area Method to Analyze Access to Health Care” Environment and planning. B, Planning & design, Vol.41 (4), p.717-735."
  },
  {
    "objectID": "outline/Lesson11_outline.html#all-about-r",
    "href": "outline/Lesson11_outline.html#all-about-r",
    "title": "Lesson 11: Geographically Weighted Regression",
    "section": "All About R",
    "text": "All About R\n\nGWmodel package, especially\n\nGollini, I et. al. (2015) “GWmodel: An R Package for Exploring Spatial Heterogeneity Using Geographically Weighted Models”, Journal of Statistical Software, Volume 63, Issue 17 and\nBinbin Lu, Paul Harris, Martin Charlton & Chris Brunsdon (2014) “The GWmodel R package: further topics for exploring spatial heterogeneity using geographically weighted models”, Geo-spatial Information Science, 17:2, 85-101, DOI: 10.1080/10095020.2014.917453.\n\nlctools package especially gw() and gwr() related functions.\nspgwr implements of geographically weighted regression methods for exploring possible non-stationarity.\ngwrr: its geographically weighted regression (GWR) models and has tools to diagnose and remediate collinearity in the GWR models. Also fits geographically weighted ridge regression (GWRR) and geographically weighted lasso (GWL) models."
  },
  {
    "objectID": "Quarto.html",
    "href": "Quarto.html",
    "title": "All About Quarto",
    "section": "",
    "text": "Getting Started\nRStudio IDE\nUsing R\nHTML Basics"
  },
  {
    "objectID": "Quarto.html#overview",
    "href": "Quarto.html#overview",
    "title": "All About Quarto",
    "section": "",
    "text": "Getting Started\nRStudio IDE\nUsing R\nHTML Basics"
  },
  {
    "objectID": "Quarto.html#create-websites-and-blogs",
    "href": "Quarto.html#create-websites-and-blogs",
    "title": "All About Quarto",
    "section": "Create websites and blogs",
    "text": "Create websites and blogs\n\nCreating a Website\nWebsite Navigation\nDocument Listings"
  },
  {
    "objectID": "Quarto.html#authoring-guide",
    "href": "Quarto.html#authoring-guide",
    "title": "All About Quarto",
    "section": "Authoring Guide",
    "text": "Authoring Guide\n\nMarkdown Basics\nFigures\nTables\nDiagrams\nCitations & Footnotes\nCross References\nArticle Layout"
  },
  {
    "objectID": "Quarto.html#useful-web-resources",
    "href": "Quarto.html#useful-web-resources",
    "title": "All About Quarto",
    "section": "Useful web resources",
    "text": "Useful web resources\n\nAwesome Quarto. This github repository provides a comprehensive listing of Quarto resources. It should be the second stop (after Quarto homepage) if you need to look for revelent materials about Quarto."
  },
  {
    "objectID": "ShinyResources.html",
    "href": "ShinyResources.html",
    "title": "Shiny Resources",
    "section": "",
    "text": "Hadley Wickham (2020) Mastering Shiny. Everything you need to know about Shiny can be found here. It is not an easy to read book but worth investing time and effort to read.\nShiny from R Studio\nLearn Shiny\nFunction reference\nThe Shiny Cheat sheet\nColin Fay, Sébastien Rochette, Vincent Guyader, Cervan Girard (2020) Engineering Production-Grade Shiny Apps\nDavid Granjon (2020) Outstanding User Interfaces with Shiny\nHow to Build a Shiny Application from Scratch\nShiny 入門\n\n\n\n\n\nGallery\nShiny Contest Winners 2019 - Full List\nFifteen New Zealand government Shiny web apps\nIntroducing the New Zealand Trade Intelligence Dashboard"
  },
  {
    "objectID": "ShinyResources.html#shiny-web-based-visual-analytics-development-tool-in-r",
    "href": "ShinyResources.html#shiny-web-based-visual-analytics-development-tool-in-r",
    "title": "Shiny Resources",
    "section": "",
    "text": "Hadley Wickham (2020) Mastering Shiny. Everything you need to know about Shiny can be found here. It is not an easy to read book but worth investing time and effort to read.\nShiny from R Studio\nLearn Shiny\nFunction reference\nThe Shiny Cheat sheet\nColin Fay, Sébastien Rochette, Vincent Guyader, Cervan Girard (2020) Engineering Production-Grade Shiny Apps\nDavid Granjon (2020) Outstanding User Interfaces with Shiny\nHow to Build a Shiny Application from Scratch\nShiny 入門\n\n\n\n\n\nGallery\nShiny Contest Winners 2019 - Full List\nFifteen New Zealand government Shiny web apps\nIntroducing the New Zealand Trade Intelligence Dashboard"
  },
  {
    "objectID": "ShinyResources.html#others",
    "href": "ShinyResources.html#others",
    "title": "Shiny Resources",
    "section": "Others",
    "text": "Others\n\nAwesome Shiny Extensions"
  },
  {
    "objectID": "ShinyWorkshop/Shiny2/Shiny2.html#overview",
    "href": "ShinyWorkshop/Shiny2/Shiny2.html#overview",
    "title": "Building Web-enabled Geospatial Analytics Application with Shiny: Beyond the basic",
    "section": "Overview",
    "text": "Overview\nIn this lesson, selected advanced methods of Shiny will be discussed. You will also gain hands-on experiences on using these advanced methods to build Shiny applications.\nBy the end of this lesson, you will be able to:\n\ngain further understanding of the reactive feature of Shiny and Shiny’s functions that support reactive flow,\nbuild interactive Shiny application by using plotly R and\nbuild static, interactive and reactive geovisualisation application by using tmap"
  },
  {
    "objectID": "ShinyWorkshop/Shiny2/Shiny2.html#reactive-flow",
    "href": "ShinyWorkshop/Shiny2/Shiny2.html#reactive-flow",
    "title": "Building Web-enabled Geospatial Analytics Application with Shiny: Beyond the basic",
    "section": "Reactive Flow",
    "text": "Reactive Flow\nBy default, Shiny application is Reactive!"
  },
  {
    "objectID": "ShinyWorkshop/Shiny2/Shiny2.html#in-class-exercise-building-a-reactive-scatter-plot-using-shiny",
    "href": "ShinyWorkshop/Shiny2/Shiny2.html#in-class-exercise-building-a-reactive-scatter-plot-using-shiny",
    "title": "Building Web-enabled Geospatial Analytics Application with Shiny: Beyond the basic",
    "section": "In-class Exercise: Building a reactive scatter plot using Shiny",
    "text": "In-class Exercise: Building a reactive scatter plot using Shiny\n\n\nIn this in-class exercise, you are going to explore advanced reactive features.\nTo get started, you need to do the followings:\n\nstart a new Shiny Application\nload the necessary R package, namely Shiny and tidyverse\nimport the data file (i.e. Exam_data.csv )\nbuild a basic scatterplot look similar to the figure on the right."
  },
  {
    "objectID": "ShinyWorkshop/Shiny2/Shiny2.html#embedding-interactive-graphs-in-r-shiny",
    "href": "ShinyWorkshop/Shiny2/Shiny2.html#embedding-interactive-graphs-in-r-shiny",
    "title": "Building Web-enabled Geospatial Analytics Application with Shiny: Beyond the basic",
    "section": "Embedding Interactive Graphs in R Shiny",
    "text": "Embedding Interactive Graphs in R Shiny\nThe plotly way\n\n\nTwo approaches: - Using plotly directly, or - Plot the basic visualisation using ggplot2, then wrap the visualisation object into plotly object using ggplotly().\nReference: - Plotly R Open Source Graphing Library - 17 Server-side linking with shiny of Interactive web-based data visualization with R, plotly, and shiny"
  },
  {
    "objectID": "ShinyWorkshop/Shiny2/Shiny2.html#in-class-exercise-embedding-a-drill-down-bar-chart-in-shiny",
    "href": "ShinyWorkshop/Shiny2/Shiny2.html#in-class-exercise-embedding-a-drill-down-bar-chart-in-shiny",
    "title": "Building Web-enabled Geospatial Analytics Application with Shiny: Beyond the basic",
    "section": "In-class Exercise: Embedding a drill-down bar chart in Shiny",
    "text": "In-class Exercise: Embedding a drill-down bar chart in Shiny\nIn this exercise, you will learn how to embed a drill-down bar chart in Shiny by using event_data() of plotly."
  },
  {
    "objectID": "ShinyWorkshop/Shiny2/Shiny2.html#in-class-exercise-embedding-a-static-map-in-shiny",
    "href": "ShinyWorkshop/Shiny2/Shiny2.html#in-class-exercise-embedding-a-static-map-in-shiny",
    "title": "Building Web-enabled Geospatial Analytics Application with Shiny: Beyond the basic",
    "section": "In-class Exercise: Embedding A Static Map in Shiny",
    "text": "In-class Exercise: Embedding A Static Map in Shiny\nIn this exercise, you will learn how to embed a static map in Shiny by using renderPlot(). By the end of this exercise, you will be able to plot a static map on Shiny display as shown below."
  },
  {
    "objectID": "ShinyWorkshop/Shiny2/Shiny2.html#in-class-exercise-building-a-choropleth-mapping-application",
    "href": "ShinyWorkshop/Shiny2/Shiny2.html#in-class-exercise-building-a-choropleth-mapping-application",
    "title": "Building Web-enabled Geospatial Analytics Application with Shiny: Beyond the basic",
    "section": "In-class Exercise: Building a choropleth mapping application",
    "text": "In-class Exercise: Building a choropleth mapping application\nIn this exercise, you will learn how to build a choropleth mapping application by using tmap and Shiny."
  },
  {
    "objectID": "ShinyWorkshop/Shiny2/Shiny2.html#embedding-an-interactive-map-in-shiny",
    "href": "ShinyWorkshop/Shiny2/Shiny2.html#embedding-an-interactive-map-in-shiny",
    "title": "Building Web-enabled Geospatial Analytics Application with Shiny: Beyond the basic",
    "section": "Embedding an interactive map in Shiny",
    "text": "Embedding an interactive map in Shiny\nIn this exercise, you will learn how to embed an interactive map in Shiny by using renderTmap() and tmapOutput() of tmap package. The interactive map is a proportional symbol map showing distribution of winnings by branches/outlets."
  },
  {
    "objectID": "ShinyWorkshop/Shiny2/Shiny2.html#in-class-exercise-reactive-map-in-r-shiny",
    "href": "ShinyWorkshop/Shiny2/Shiny2.html#in-class-exercise-reactive-map-in-r-shiny",
    "title": "Building Web-enabled Geospatial Analytics Application with Shiny: Beyond the basic",
    "section": "In-class Exercise: Reactive Map in R Shiny",
    "text": "In-class Exercise: Reactive Map in R Shiny\nIn this exercise, you will learn how to create reactive map in Shiny. The output will look similar to the figure below."
  },
  {
    "objectID": "ShinyWorkshop/Shiny2/Shiny2.html#references",
    "href": "ShinyWorkshop/Shiny2/Shiny2.html#references",
    "title": "Building Web-enabled Geospatial Analytics Application with Shiny: Beyond the basic",
    "section": "References",
    "text": "References\n\nHadley Wickham (2020) Mastering Shiny: Build Interactive Apps, Reports, and Dashboards Powered by R online version\n\nChapter 3 Basic reactivity\nChapter 13 Why reactivity?\nChapter 14 The reactive graph\nChapter 15 Reactive building blocks\nChapter 16 Escaping the graph\n\nCarson Sievert (2019) Interactive web-based data visualization with R, plotly, and shiny, online version.\n\nChapter 17 Server-side linking with shiny"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Where should the next new business outlet be located in order to optimise the profit? What are the location factors that affect the resale prices of HDB housing units? Which are the economic or service activities such as IT professional firms, car workshops, fast food chains (ie. KFC, McDonalds), coffee outlets (Starbucks, Ya Kun Kaya Toast, Toast Box) that tend to be located close to one another and which are the ones that tend to be a distance apart? Do these observed patterns and processes occur at random or are they being constrained by geographical factors? These and many other related questions are the challenges faced by data scientists and data analysts today especially when geographical data are used.\nGeospatial analytics offers the solutions to these questions by providing data scientists and analysts a problem-driven and data-centric analysis framework focusing on discovering actionable understanding from geographically referenced data. It makes extensive use of geospatial data wrangling, geoprocessing, spatial statistical, geospatial machine learning and spatial data visualisation techniques to support decision- and strategy-making.\nThis course provides students with an introduction to the concepts, principles and methods of geospatial analytics and their practical applications of geospatial analytics in real world operations. Emphasis will be placed on\n\nperforming geospatial data science tasks such as importing, tidying, manipulating, transforming, projecting and processing geospatial data programmatically,\nvisualising, analysing and describing geographical patterns and process using appropriate geovisualisation and thematic mapping techniques,\nConducting geospatial analysis by using appropriate spatial statistics and machine learning methods and\nbuilding web-based geospatial analytics applications."
  },
  {
    "objectID": "syllabus.html#synopsis",
    "href": "syllabus.html#synopsis",
    "title": "Syllabus",
    "section": "",
    "text": "Where should the next new business outlet be located in order to optimise the profit? What are the location factors that affect the resale prices of HDB housing units? Which are the economic or service activities such as IT professional firms, car workshops, fast food chains (ie. KFC, McDonalds), coffee outlets (Starbucks, Ya Kun Kaya Toast, Toast Box) that tend to be located close to one another and which are the ones that tend to be a distance apart? Do these observed patterns and processes occur at random or are they being constrained by geographical factors? These and many other related questions are the challenges faced by data scientists and data analysts today especially when geographical data are used.\nGeospatial analytics offers the solutions to these questions by providing data scientists and analysts a problem-driven and data-centric analysis framework focusing on discovering actionable understanding from geographically referenced data. It makes extensive use of geospatial data wrangling, geoprocessing, spatial statistical, geospatial machine learning and spatial data visualisation techniques to support decision- and strategy-making.\nThis course provides students with an introduction to the concepts, principles and methods of geospatial analytics and their practical applications of geospatial analytics in real world operations. Emphasis will be placed on\n\nperforming geospatial data science tasks such as importing, tidying, manipulating, transforming, projecting and processing geospatial data programmatically,\nvisualising, analysing and describing geographical patterns and process using appropriate geovisualisation and thematic mapping techniques,\nConducting geospatial analysis by using appropriate spatial statistics and machine learning methods and\nbuilding web-based geospatial analytics applications."
  },
  {
    "objectID": "syllabus.html#course-objectives",
    "href": "syllabus.html#course-objectives",
    "title": "Syllabus",
    "section": "Course Objectives",
    "text": "Course Objectives\nUpon completion of the course, students will be able to:\n\nProvide accurate explanation of the mathematical and input data requirements of the analysis method(s) used,\nImport, extract, process, transform and assemble geospatial analytical sandbox programmatically to meet the analysis needs,\nApply appropriate geospatial analysis methods in addressing specific analysis tasks,\nCommunicate the analysis procedures in a reproducible manner,\nCommunicate the analysis results effectively and in an easy to understand manner with the help of appropriate geo- and data visualisation techniques, and\nDesign and implement web-enabled geospatial analytics applications."
  },
  {
    "objectID": "syllabus.html#competencies",
    "href": "syllabus.html#competencies",
    "title": "Syllabus",
    "section": "Competencies",
    "text": "Competencies\n\nDefining Geospatial Analytics and describing the applications of geospatial analytics by using real-world examples.\nExplaining the differences between Geospatial Analytics and Geographic Information Systems (GIS).\nImporting, wrangling and transforming aspatial data by using tidyverse family of packages\nImporting, wrangling and transforming geographical data by sing sf package.\nGeocoding and georeferencing geographical data programmatically.\nPerforming geoprocessing operations programmatically by using sf packages.\nPerforming Exploratory Data Analysis (EDA) using ggplot2 and Confirmatory Data Analysis using ggstatsplot.\nPreparing cartographic quality analytical maps by using tmap package.\nVisualising and analysing multivariate geospatial data using corrplot and heatmaply packages.\nExplaining the principles and methods of spatial point patterns and complete spatial randomness.\nPerforming spatial point pattern analysis by using appropriate R package(s) and providing accurate interpretation of the analysis results.\nExplaining the principles and methods of spatial weights.\nComputing spatial weights by using appropriate functions of spdep package.\nExplaining the concepts of spatial autocorrelation, spatial clusters, hot spot and cold spot areas.\nComputing localised geospatial statistics by using appropriate R package(s) and providing accurate interpretation of the analysis results.\nExplaining the principles of explanatory modelling and methods of geographically weighted regression.\nCalibrating geographically weighted regression models by using appropriate R package(s) and providing accurate interpretation of the analysis results.\nExplaining the concept of geographic segmentation and methods of spatially constrained clustering.\nPerforming spatially constrained cluster analysis by using appropriate R package(s) and providing accurate interpretation of the analysis results.\nExplaining the concept of spatial dependency and methods of spatial interpolation.\nPerforming spatial interpolation by using appropriate R package(s) and providing accurate interpretation of the analysis results.\nDefining geographic accessibility and explaining the methods of accessibility analysis.\nPerforming geographic accessibility analysis by using appropriate R package(s) and providing accurate interpretation of the analysis results.\nDesigning geospatial application programmatically by using free and open source software and packages (i.e. R, R packages and Shiny)."
  },
  {
    "objectID": "syllabus.html#prerequisitesco-requisites",
    "href": "syllabus.html#prerequisitesco-requisites",
    "title": "Syllabus",
    "section": "Prerequisites/Co-requisites",
    "text": "Prerequisites/Co-requisites\nNIL. (But a basic working knowledge of computer and numeracy will be assumed. Students are expected to understand Windows-based operating systems and to manage files and disk space responsibly. More importantly, students taking this course must be willing to learn R programming.)"
  },
  {
    "objectID": "syllabus.html#course-assessments",
    "href": "syllabus.html#course-assessments",
    "title": "Syllabus",
    "section": "Course Assessments",
    "text": "Course Assessments\n\n\n\nAssessment Categories\nWeighting (%)\n\n\n\n\nHands-on Exercise (2% x 10)\n20%\n\n\nIn-class Exercise (2% x 10)\n20%\n\n\nTake-home Exercise (10% x 3)\n30%\n\n\nGeospatial Analytics Project\n30%"
  },
  {
    "objectID": "syllabus.html#course-assessment-details",
    "href": "syllabus.html#course-assessment-details",
    "title": "Syllabus",
    "section": "Course Assessment Details",
    "text": "Course Assessment Details\n\nPre-lesson Learning and Class Participation\nPre-lesson learning materials, namely: videos, slides, recommended readings and hands-on exercises will be released one week before the weekly lesson starts. A strict requirement for each class meeting is to complete the assigned pre-learning materials before coming to class. Students are required write down at least one question or issue encountered while viewing the video, reading the recommended articles or working on the hands-on exercises and post them on Piazza for discussion.\nStudent sharing of insights from readings of assigned materials on Piazza will form a large part of the learning in this course.\n\n\nHands-on Exercise (20%)\nHands-on exercises aim to provide students to gain hands-on experience on using selected R packages to perform geospatial analysis with real world use cases. It is important for students to complete the hands-on exercises before class.\n\n\nIn-class Exercise (20%)\nIn-class exercise and discussion will extend the methods learned from the hands-on exercise to advanced modelling. The in-class discussion will also focus on how to interpret the analysis results and to communicate the analysis results by using appropriate map and data visualisation techniques. Students may also be quizzed in class and thereby contribute to in-class exercise.\n\n\nTake-home Exercise (30%)\nThere are three take-home exercises that are due throughout the term. They aim to provide students the opportunities to apply the methods learned in class by working through mini real-world cases. Each take-home exercise is an extension of the hands-on and in-class exercises. What this means is that, for example, in Lesson 1, students will learn the concept of spatial point processes and the hands-on exercise will provide students step-by-step guide on how to use R packages to perform spatial point patterns analysis. The in-class discussion, beside clarification of the concepts and usage of R packages syntax and argument, it will focus more on how to interpret and communicate the analysis results. Then the take-home exercise will require students to synthesise what they have learned from the readings, hands-on exercise and in-class exercise. The estimated workload will be about 6-8 hours per week.\nEach take-home exercise will carry a same weightage of 10%. The deliverable format of the take-home exercises and marking rubric of will be provided on the handout of the take-home exercise. Feedback on take-home exercise will be provided weekly before the weekly lesson starts. This is to ensure that students will learn from mistakes made in the earlier take-home exercise and improve their work progressively in the subsequent take-home exercises.\nStudents may work together to help one another with computer or geospatial issues and discuss the materials that constitute the take-home exercise. However, each student is required to prepare and submit the take-home exercise (including any computer work) on their own. Cheating is strictly prohibited. Cheating includes but not limited to: plagiarism and submission of work that is not the student’s.\n\n\nGeospatial Analytics Project (30%)\nThe purpose of the geospatial analytics project is to provide students first hand experience on building web-based geospatial analytics tool by integrating open source web mapping API(s), data visualisation API(s) and geospatial analysis libraries. You will also learn how to collecting, processing and analysing spatially related issues using real world data. Students are encouraged to focus on research topics that are relevant to their field of study.\n\n\nFinal Exam\nThere will be no final examination for this course."
  },
  {
    "objectID": "lesson/Lesson02/Lesson02-GeoVis.html#what-is-a-map",
    "href": "lesson/Lesson02/Lesson02-GeoVis.html#what-is-a-map",
    "title": "Lesson 2: Fundamental of Geospatial Data Visualisation and tmap Methods",
    "section": "What is a Map?",
    "text": "What is a Map?\nA model of real world depict by a collection of cartographic symbols or/and visual abstraction."
  },
  {
    "objectID": "lesson/Lesson02/Lesson02-GeoVis.html#typology-of-maps",
    "href": "lesson/Lesson02/Lesson02-GeoVis.html#typology-of-maps",
    "title": "Lesson 2: Fundamental of Geospatial Data Visualisation and tmap Methods",
    "section": "Typology of Maps",
    "text": "Typology of Maps"
  },
  {
    "objectID": "lesson/Lesson02/Lesson02-GeoVis.html#thematic-mapping-principles-and-methods",
    "href": "lesson/Lesson02/Lesson02-GeoVis.html#thematic-mapping-principles-and-methods",
    "title": "Lesson 2: Fundamental of Geospatial Data Visualisation and tmap Methods",
    "section": "Thematic Mapping: Principles and Methods",
    "text": "Thematic Mapping: Principles and Methods\n\nDisplaying\n\nQualitative data\nQuantitative data\n\nChoosing -Appropriate classification method for displaying data\n\nAppropriate number of classes\n\nTechniques in data analysis\n\nUsing the classification histogram\nNormalizing data"
  },
  {
    "objectID": "lesson/Lesson02/Lesson02-GeoVis.html#qualitative-thematic-maps",
    "href": "lesson/Lesson02/Lesson02-GeoVis.html#qualitative-thematic-maps",
    "title": "Lesson 2: Fundamental of Geospatial Data Visualisation and tmap Methods",
    "section": "Qualitative Thematic Maps",
    "text": "Qualitative Thematic Maps\nVisual Variables and Cartographic Symbols\n\n\n\nQualitative visual variables are used for nominal scale data.\nThe goal of qualitative visual variables is to show how entities differ from each other.\nThe visual variables that do a good job of showing ordinal differences are: colour value, colour saturation, size and texture/grain.\n\nFigure on the right for examples of these four ordinal visual variables used each in point, linear and areal symbols."
  },
  {
    "objectID": "lesson/Lesson02/Lesson02-GeoVis.html#qualitative-thematic-map",
    "href": "lesson/Lesson02/Lesson02-GeoVis.html#qualitative-thematic-map",
    "title": "Lesson 2: Fundamental of Geospatial Data Visualisation and tmap Methods",
    "section": "Qualitative Thematic Map",
    "text": "Qualitative Thematic Map\nPoint symbol map\n\n\n\nDifferent point symbols are used to represent school types."
  },
  {
    "objectID": "lesson/Lesson02/Lesson02-GeoVis.html#qualitative-thematic-map-1",
    "href": "lesson/Lesson02/Lesson02-GeoVis.html#qualitative-thematic-map-1",
    "title": "Lesson 2: Fundamental of Geospatial Data Visualisation and tmap Methods",
    "section": "Qualitative Thematic Map",
    "text": "Qualitative Thematic Map\nLine symbol map\n\n\n\nA road map is an example of a thematic map. It shows the road network of an area. In this map, lines with different colour intensity and tickness are used to differentiate hierarchy of roads."
  },
  {
    "objectID": "lesson/Lesson02/Lesson02-GeoVis.html#qualitative-thematic-map-2",
    "href": "lesson/Lesson02/Lesson02-GeoVis.html#qualitative-thematic-map-2",
    "title": "Lesson 2: Fundamental of Geospatial Data Visualisation and tmap Methods",
    "section": "Qualitative Thematic Map",
    "text": "Qualitative Thematic Map\nArea map\n\n\n\nLand use map below is a good example of a discrete thematic map. In this map, different colours are use to represent different land use types."
  },
  {
    "objectID": "lesson/Lesson02/Lesson02-GeoVis.html#quantitative-thematic-map",
    "href": "lesson/Lesson02/Lesson02-GeoVis.html#quantitative-thematic-map",
    "title": "Lesson 2: Fundamental of Geospatial Data Visualisation and tmap Methods",
    "section": "Quantitative Thematic Map",
    "text": "Quantitative Thematic Map\nVisual Variables and Cartographic Symbols\n\n\n\nQuantitative visual variables are used to display ordinal, interval or ratio scale data.\n\nThe goal of the quantitative visual variable is to show relative magnitude or order between entities.\nThe visual variables that do a good job of showing ordinal differences are: colour value, colour saturation, size and texture/grain.\n\nFigure on the right shows of these four ordinal visual variables used each in point, linear and areal symbols."
  },
  {
    "objectID": "lesson/Lesson02/Lesson02-GeoVis.html#proportional-symbol-map",
    "href": "lesson/Lesson02/Lesson02-GeoVis.html#proportional-symbol-map",
    "title": "Lesson 2: Fundamental of Geospatial Data Visualisation and tmap Methods",
    "section": "Proportional Symbol Map",
    "text": "Proportional Symbol Map\n\nThe proportional symbol technique uses symbols of different sizes to represent data associated with different areas or locations within the map.\n\n\n\nThe proportional symbol technique uses symbols of different sizes to represent data associated with different areas or locations within the map. For example, the proportional maps above use circle with different sizes to represent millions of people. There are two types of point features that are typically depicted with proportional symbols: features for which the data represents a geographic position directly (e.g., gallons of oil from individual oil wells), and features that are geographic areas to which data are aggregated and the data magnitudes are assigned to a representative point within the area (e.g., the geographic centroid of a state as in the examples above). In either case, the area of the symbol is scaled to represent the data magnitude, sometimes with a bit of exaggeration to adjust for a general tendency of human vision to underestimate differences in area. A variant on this direct data-to-symbol scaling groups values into categories first, then scales the symbol to represent the mean for the category, assigning a symbol to each place to represent the category range that the mean for the place falls within"
  },
  {
    "objectID": "lesson/Lesson02/Lesson02-GeoVis.html#dot-density-map",
    "href": "lesson/Lesson02/Lesson02-GeoVis.html#dot-density-map",
    "title": "Lesson 2: Fundamental of Geospatial Data Visualisation and tmap Methods",
    "section": "Dot Density Map",
    "text": "Dot Density Map\nA dot-density map is a type of thematic map that uses dots or other symbols on the map to show the values of one or more numeric data fields. Each dot on a dot-density map represents some amount of data.\n\n\nOne dot represent 100 households.\n\n\nReference: Dot distribution map at wiki and Dot Density Maps"
  },
  {
    "objectID": "lesson/Lesson03/Lesson03-SPPA-2.html",
    "href": "lesson/Lesson03/Lesson03-SPPA-2.html",
    "title": "Lesson 3: Spatial Point Patterns Analysis",
    "section": "",
    "text": "Introducing Spatial Point Patterns\n\nThe basic concepts of spatial point patterns\n1st Order versus 2nd Order\nSpatial Point Patterns in real world\n\n1st Order Spatial Point Patterns Analysis\n\nKernel Density Estimation (KDE)\nNetwork Constrained Kernel Density Estimation (NKDE)\nTemporal Network Kernel Density Estimation (TNKDE)\n\n\n\n\n2nd Order Spatial Point Patterns Analysis\n\nNearest Neighbour Index\nG-function\nF-function\nK-function\nL-function\n\n\n\n\n\nWelcome to Lesson 4: Spatial Point Pattern Analysis, a family of spatial statistics specially developed to model and to test distribution of spatial point events.\nThe lesson proceeding is divided into three main section. First, I will share with you what are real world spatial point events. This is followed by explaining the concepts and methods of 1st-order and 2nd-order spatial point patterns analysis."
  },
  {
    "objectID": "lesson/Lesson04/Lesson04-STPPA.html#content",
    "href": "lesson/Lesson04/Lesson04-STPPA.html#content",
    "title": "Lesson 4: Spatio-Temporal Point Patterns Analysis (STPPA)",
    "section": "Content",
    "text": "Content\n\nBasic concepts of Spatio-Temporal Point Process\nSpatio-Temporal Kernel Density Estimation"
  },
  {
    "objectID": "lesson/Lesson04/Lesson04-STPPA.html#what-is-a-spatio-temporal-point-process",
    "href": "lesson/Lesson04/Lesson04-STPPA.html#what-is-a-spatio-temporal-point-process",
    "title": "Lesson 4: Spatio-Temporal Point Patterns Analysis (STPPA)",
    "section": "What is a Spatio-Temporal Point Process",
    "text": "What is a Spatio-Temporal Point Process\nA spatio-temporal point process (also called space-time or spatial-temporal point process) is a random collection of points, where each point represents the time and location of an event. Examples of events include incidence of disease, sightings or births of a species, or the occurrences of fires, earthquakes, lightning strikes, tsunamis, or volcanic eruptions. Typically the spatio-temporal point events are recorded in three-dimension, namely: longitude, latitude, and time as shown in the figure below."
  },
  {
    "objectID": "lesson/Lesson04/Lesson04-STPPA.html#spatio-temporal-kde-stkde",
    "href": "lesson/Lesson04/Lesson04-STPPA.html#spatio-temporal-kde-stkde",
    "title": "Lesson 4: Spatio-Temporal Point Patterns Analysis (STPPA)",
    "section": "Spatio-Temporal KDE (STKDE)",
    "text": "Spatio-Temporal KDE (STKDE)\nMathematically, STKDE is defined as"
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-Introduction_to_GAA.html",
    "href": "lesson/Lesson01/Lesson01-Introduction_to_GAA.html",
    "title": "Lesson 1: Introduction to Geospatial Analytics and Applications",
    "section": "",
    "text": "Demystifying Geospatial Analytics\nMotivation of Geospatial Analytics\nThe Role of Geospatial Analytics\nGeospatial Analytics and Social Consciousness\n\n\n\n\nA Geographical Information System (GIS) is a toolkit for creating, managing, analysing, visualising, and sharing data of any kind according to where it’s located.\n\n\n\n\n\n\n\nGeospatial analytics is more than a GIS.\n\n\n\n\n\nGeospatial analytics is more than data visualisation\n\n\n\n\n\nSource: Singapore’s first disease map delivers real-time information on infectious diseases\n\n\n\n\n\n\n\n\n\nGeospatial analytics is more than just mapping.\n\n\n\n\n\n\nAbout 80% of all data maintained by organisations around the world has a location component.\n\n\n\n\n\n\n\n(Source: BusinessWeek Research Services, 2006)\n\n\n\n\n\nGeospatial information in Smart Nation.\n\n\n\n\n\n\n\nSee more at this link\n\n\n\n\n\nThe explosion in the availability of open geospatial data from both the public and private sectors at national and international levels.\n\n\n\n\n\n\n\n\n\n\nThe national geospatial master plan.\n\n\n\n\n\n\n\nSource: Singapore Geospatial Master Plan\n\n\n\n\n\nUncovering insights not found in statistical graphs and tables.\n\n\n\nSource: SGSAS - Simple Geo-Spatial Analysis using R Shiny\n\n\n\n\n\nTo reveal the untapped property of spatial contiguity in geographic knowledge discovery in databases.\n\n\n\n\n\n\n\nSource: SGSAS - Simple Geo-Spatial Analysis using R Shiny\n\n\n\n\n\nTo uncover the complexity of the real world relationship.\n\n\n\n\n\n\n\nSource: SGSAS - Simple Geo-Spatial Analysis using R Shiny\n\n\n\n\n\nTo model spatial interactions and flows.\n\n\n\n\n\n\n\nSource: IS415 Bus Rider Flow Project\n\n\n\n\nThe true power of geospatial analytics is to provide decision makers and planners with data-driven and process information for better problem solving and more efficient use of resources."
  },
  {
    "objectID": "lesson/Lesson05/Lesson05-Spatial_Weights.html#what-is-geographically-referenced-attribute",
    "href": "lesson/Lesson05/Lesson05-Spatial_Weights.html#what-is-geographically-referenced-attribute",
    "title": "Lesson 5: Spatial Weights and Applications",
    "section": "What is geographically referenced attribute?",
    "text": "What is geographically referenced attribute?\n\n\nRows: 323\nColumns: 12\n$ SUBZONE_N        &lt;chr&gt; \"MARINA SOUTH\", \"PEARL'S HILL\", \"BOAT QUAY\", \"HENDERS…\n$ SUBZONE_C        &lt;fct&gt; MSSZ01, OTSZ01, SRSZ03, BMSZ08, BMSZ03, BMSZ07, BMSZ0…\n$ PLN_AREA_N       &lt;fct&gt; MARINA SOUTH, OUTRAM, SINGAPORE RIVER, BUKIT MERAH, B…\n$ PLN_AREA_C       &lt;fct&gt; MS, OT, SR, BM, BM, BM, BM, SR, QT, QT, QT, BM, ME, R…\n$ REGION_N         &lt;fct&gt; CENTRAL REGION, CENTRAL REGION, CENTRAL REGION, CENTR…\n$ REGION_C         &lt;fct&gt; CR, CR, CR, CR, CR, CR, CR, CR, CR, CR, CR, CR, CR, C…\n$ YOUNG            &lt;dbl&gt; NA, 1100, 0, 2620, 2840, 2910, 2850, 0, 1120, 30, NA,…\n$ `ECONOMY ACTIVE` &lt;dbl&gt; NA, 3420, 50, 7500, 6260, 7560, 8340, 50, 2750, 210, …\n$ AGED             &lt;dbl&gt; NA, 2110, 20, 3260, 1630, 3310, 3590, 10, 560, 50, NA…\n$ TOTAL            &lt;dbl&gt; NA, 6630, 70, 13380, 10730, 13780, 14780, 60, 4430, 2…\n$ DEPENDENCY       &lt;dbl&gt; NA, 0.9385965, 0.4000000, 0.7840000, 0.7140575, 0.822…\n$ geometry         &lt;MULTIPOLYGON [m]&gt; MULTIPOLYGON (((31495.56 30..., MULTIPOL…\n\n\n\nA kind of data that is very similar to an ordinary data. The only difference is that each observation is associated with some form of geography such as numbers of aged population by planning subzone."
  },
  {
    "objectID": "lesson/Lesson05/Lesson05-Spatial_Weights.html#what-are-spatial-weights-wij",
    "href": "lesson/Lesson05/Lesson05-Spatial_Weights.html#what-are-spatial-weights-wij",
    "title": "Lesson 5: Spatial Weights and Applications",
    "section": "What are Spatial Weights (wij)",
    "text": "What are Spatial Weights (wij)\n\nA way to define spatial neighbourhood.\n\n\n\nBefore we can perform statistics test of spatial randomness, we need to understand how spatial relationship among geographical areas can be defined mathematically."
  },
  {
    "objectID": "lesson/Lesson05/Lesson05-Spatial_Weights.html#applications-of-spatial-weights",
    "href": "lesson/Lesson05/Lesson05-Spatial_Weights.html#applications-of-spatial-weights",
    "title": "Lesson 5: Spatial Weights and Applications",
    "section": "Applications of Spatial Weights",
    "text": "Applications of Spatial Weights\nFormally, for observation i, the spatial lag of yi, referred to as [Wy]i (the variable Wy observed for location i) is:\n\nwhere the weights wij consist of the elements of the i-th row of the matrix W, matched up with the corresponding elements of the vector y.\n\nWith a neighbor structure defined by the non-zero elements of the spatial weights matrix W, a spatially lagged variable is a weighted sum or a weighted average of the neighboring values for that variable. In most commonly used notation, the spatial lag of y is then expressed as Wy."
  },
  {
    "objectID": "lesson/Lesson05/Lesson05-Spatial_Weights.html#references",
    "href": "lesson/Lesson05/Lesson05-Spatial_Weights.html#references",
    "title": "Lesson 5: Spatial Weights and Applications",
    "section": "References",
    "text": "References\n\nChapter 2. Codifying the neighbourhood structure of Handbook of Spatial Analysis: Theory and Application with R.\nFrançois Bavaud (2010) “Models for Spatial Weights: A Systematic Look” Geographical Analysis, Vol. 30, No.2, pp 153-171.\nTony H. Grubesic and Andrea L. Rosso (2014) “The Use of Spatially Lagged Explanatory Variables for Modeling Neighborhood Amenities and Mobility in Older Adults”, Cityscape, Vol. 16, No. 2, pp. 205-214."
  },
  {
    "objectID": "lesson/Lesson06/Lesson05-GLSA.html#content",
    "href": "lesson/Lesson06/Lesson05-GLSA.html#content",
    "title": "Lesson 6: Global and Local Measures of Spatial Association",
    "section": "Content",
    "text": "Content\n\n\n\nWhat is Spatial Autocorrelation\n\nMeasures of Global Spatial Autocorrelation\nMeasures of Global High/Low Clustering\n\nIntroducing Localised Geospatial Analysis\n\nLocal Indicators of Spatial Association (LISA)\n\nCluster and Outlier Analysis\n\nLocal Moran and Local Geary\nMoran scatterplot\nLISA Cluster Map\n\n\n\n\nHot Spot and Cold Spot Areas Analysis\n\nGetis and Ord’s G-statistics\n\nEmerging Hot Spot Analysis (EHSA)\n\nSpacetime and spacetime cubes\nMann-Kendall Test\nEHSA map"
  },
  {
    "objectID": "lesson/Lesson06/Lesson05-GLSA.html#what-is-spatial-autocorrelation",
    "href": "lesson/Lesson06/Lesson05-GLSA.html#what-is-spatial-autocorrelation",
    "title": "Lesson 6: Global and Local Measures of Spatial Association",
    "section": "What is Spatial Autocorrelation",
    "text": "What is Spatial Autocorrelation\n\nToble’s First Law of Geography\nSpatial Dependency\nSpatial Autocorrelation\n\nPositive autocorrelation\nNegative autocorrelation"
  },
  {
    "objectID": "lesson/Lesson06/Lesson05-GLSA.html#measures-of-global-spatial-autocorrelation",
    "href": "lesson/Lesson06/Lesson05-GLSA.html#measures-of-global-spatial-autocorrelation",
    "title": "Lesson 6: Global and Local Measures of Spatial Association",
    "section": "Measures of Global Spatial Autocorrelation",
    "text": "Measures of Global Spatial Autocorrelation\n\nMoran’s I\nGeary’s c"
  },
  {
    "objectID": "lesson/Lesson06/Lesson05-GLSA.html#measures-of-global-highlow-clustering-getis-ord-global-g",
    "href": "lesson/Lesson06/Lesson05-GLSA.html#measures-of-global-highlow-clustering-getis-ord-global-g",
    "title": "Lesson 6: Global and Local Measures of Spatial Association",
    "section": "Measures of Global High/Low Clustering: Getis-Ord Global G",
    "text": "Measures of Global High/Low Clustering: Getis-Ord Global G\n\n\n\nGetis-Ord Global G statistic is concerned with the overall concentration or lack of concentration in all pairs that are neighbours given the definition of neighbouring areas.\nThe variable must contain only positive values to be used.\n\n\n\nSource: Getis, A., & Ord, K. (1992). “The Analysis of Spatial Association by Use of Distance Statistics”. Geographical Analysis, 24, 189–206."
  },
  {
    "objectID": "lesson/Lesson06/Lesson05-GLSA.html#local-spatial-autocorrelation-statistics",
    "href": "lesson/Lesson06/Lesson05-GLSA.html#local-spatial-autocorrelation-statistics",
    "title": "Lesson 6: Global and Local Measures of Spatial Association",
    "section": "Local Spatial Autocorrelation Statistics",
    "text": "Local Spatial Autocorrelation Statistics\n\nA collection of geospatial statistical analysis methods for analysing the location related tendency (clusters or outliers) in the attributes of geographically referenced data (points or areas).\nCan be indecies decomposited from their global measures such as local Moran’s I, local Geary’s c, and Getis-Ord Gi*.\nThese spatial statistics are well suited for:\n\ndetecting clusters or outliers;\nidentifying hot spot or cold spot areas;\nassessing the assumptions of stationarity; and\nidentifying distances beyond which no discernible association obtains."
  },
  {
    "objectID": "lesson/Lesson06/Lesson05-GLSA.html#local-indicator-of-spatial-association-lisa",
    "href": "lesson/Lesson06/Lesson05-GLSA.html#local-indicator-of-spatial-association-lisa",
    "title": "Lesson 6: Global and Local Measures of Spatial Association",
    "section": "Local Indicator of Spatial Association (LISA)",
    "text": "Local Indicator of Spatial Association (LISA)\n\nA subset of localised geospatial statistics methods.\nAny spatial statistics that satisfies the following two requirements (Anselin, L. 1995):\n\nthe LISA for each observation gives an indication of the extent of significant spatial clustering of similar values around that observation;\nthe sum of LISAs for all observations is proportional to a global indicator of spatial association."
  },
  {
    "objectID": "lesson/Lesson06/Lesson05-GLSA.html#local-morans-i",
    "href": "lesson/Lesson06/Lesson05-GLSA.html#local-morans-i",
    "title": "Lesson 6: Global and Local Measures of Spatial Association",
    "section": "Local Moran’s I",
    "text": "Local Moran’s I\nGiven a geographically referenced attribute field, X the formula of local Moran’s I is:"
  },
  {
    "objectID": "lesson/Lesson06/Lesson05-GLSA.html#detecting-hot-and-cold-spot-areas",
    "href": "lesson/Lesson06/Lesson05-GLSA.html#detecting-hot-and-cold-spot-areas",
    "title": "Lesson 6: Global and Local Measures of Spatial Association",
    "section": "Detecting hot and cold spot areas",
    "text": "Detecting hot and cold spot areas\n\n\n\nGiven a set of geospatial features (i.e. points or polygons) and an analysis field, the spatial statistics tell you where features with either high (i.e. hot spots) or low values (cold spots) cluster spatially.\nThe spatial statistic used is called Getis-Ord Gi* statistic (pronounced G-i-star).\nGetis and Ord (1992) define the local G and G∗ statistics for region i (i=1,···,n) as:"
  },
  {
    "objectID": "lesson/Lesson06/Lesson05-GLSA.html#fixed-weighting-scheme",
    "href": "lesson/Lesson06/Lesson05-GLSA.html#fixed-weighting-scheme",
    "title": "Lesson 6: Global and Local Measures of Spatial Association",
    "section": "Fixed weighting scheme",
    "text": "Fixed weighting scheme\n\n\n\nThings to consider if fixed distance is used: - All features should have at least one neighbour.\n\nNo feature should have all other features as neighbours.\nEspecially if the values for the input field are skewed, you want features to have about eight neighbors each.\nMight produce large estimate variances where data are sparse, while mask subtle local variations where data are dense.\nIn extreme condition, fixed schemes might not be able to calibrate in local areas where data are too sparse to satisfy the calibration requirements (observations must be more than parameters)."
  },
  {
    "objectID": "lesson/Lesson06/Lesson05-GLSA.html#adaptive-weighting-schemes",
    "href": "lesson/Lesson06/Lesson05-GLSA.html#adaptive-weighting-schemes",
    "title": "Lesson 6: Global and Local Measures of Spatial Association",
    "section": "Adaptive weighting schemes",
    "text": "Adaptive weighting schemes\n\n\n\nAdaptive schemes adjust itself according to the density of data\n\nShorter bandwidths where data are dense and longer where sparse.\nFinding nearest neighbors are one of the often used approaches.\n\n\n\n]"
  },
  {
    "objectID": "lesson/Lesson06/Lesson05-GLSA.html#best-practice-guidelines",
    "href": "lesson/Lesson06/Lesson05-GLSA.html#best-practice-guidelines",
    "title": "Lesson 6: Global and Local Measures of Spatial Association",
    "section": "Best practice guidelines",
    "text": "Best practice guidelines\n\nResults are only reliable if the input feature class contains at least 30 features.\nThe input field mst be in continuous data type such as a count, rate, or other numeric measurement, no categorical attribute field is allowed."
  },
  {
    "objectID": "lesson/Lesson06/Lesson05-GLSA.html#emerging-hot-spot-analysis-ehsa",
    "href": "lesson/Lesson06/Lesson05-GLSA.html#emerging-hot-spot-analysis-ehsa",
    "title": "Lesson 6: Global and Local Measures of Spatial Association",
    "section": "Emerging Hot Spot Analysis (EHSA)",
    "text": "Emerging Hot Spot Analysis (EHSA)\n\nA technique that falls under exploratory spatial data analysis (ESDA).\nIt combines the traditional ESDA technique of hot spot analysis using the Getis-Ord Gi* statistic with the traditional time-series Mann-Kendall test for monotonic trends.\nThe goal of EHSA is to evaluate how hot and cold spots are changing over time. It helps us answer the questions: are they becoming increasingly hotter, are they cooling down, or are they staying the same?"
  },
  {
    "objectID": "lesson/Lesson06/Lesson05-GLSA.html#in-colclusion",
    "href": "lesson/Lesson06/Lesson05-GLSA.html#in-colclusion",
    "title": "Lesson 6: Global and Local Measures of Spatial Association",
    "section": "In colclusion",
    "text": "In colclusion\nSpatial statistics methods are not a blackbox. Before performing the analysis, a geospatial analyst should consider the followings:\n\nWhat is the geographical question?\nWhat is the geospatial feature?\nWhat is the analysis field?\nWhich conceptualization of spatial relationships is appropriate?"
  },
  {
    "objectID": "lesson/Lesson06/Lesson05-GLSA.html#references",
    "href": "lesson/Lesson06/Lesson05-GLSA.html#references",
    "title": "Lesson 6: Global and Local Measures of Spatial Association",
    "section": "References",
    "text": "References\n\nMoran, P. A. P. (1950). “Notes on Continuous Stochastic Phenomena”. Biometrika. 37 (1): 17–23.\nGeary, R.C. (1954) “The Contiguity Ratio and Statistical Mapping”. The Incorporated Statistician, Vol. 5, No. 3, pp. 115-127. - Moran’s I\nGeary’s c - Getis, A., & Ord, K. (1992). “The Analysis of Spatial Association by Use of Distance Statistics”. Geographical Analysis, 24, 189–206.\nAnselin, L. (1995). “Local indicators of spatial association – LISA”. Geographical Analysis, 27(4): 93-115.\nGetis, A. and Ord, J.K. (1992) “The analysis of spatial association by use of distance statistics”. Geographical Analysis, 24(3): 189-206.\nOrd, J.K. and Getis, A. (2010) “Local spatial autocorrelation statistics: Distributional issues and an application”. Geographical Analysis, 27(4): 286-306."
  },
  {
    "objectID": "lesson/Lesson06/Lesson06-GLSA.html#content",
    "href": "lesson/Lesson06/Lesson06-GLSA.html#content",
    "title": "Lesson 6: Global and Local Measures of Spatial Association",
    "section": "Content",
    "text": "Content\n\n\n\nWhat is Spatial Autocorrelation\n\nMeasures of Global Spatial Autocorrelation\nMeasures of Global High/Low Clustering\n\nIntroducing Localised Geospatial Analysis\n\nLocal Indicators of Spatial Association (LISA)\n\nCluster and Outlier Analysis\n\nLocal Moran and Local Geary\nMoran scatterplot\nLISA Cluster Map\n\n\n\n\nHot Spot and Cold Spot Areas Analysis\n\nGetis and Ord’s G-statistics\n\nEmerging Hot Spot Analysis (EHSA)\n\nSpacetime and spacetime cubes\nMann-Kendall Test\nEHSA map"
  },
  {
    "objectID": "lesson/Lesson06/Lesson06-GLSA.html#what-is-spatial-autocorrelation",
    "href": "lesson/Lesson06/Lesson06-GLSA.html#what-is-spatial-autocorrelation",
    "title": "Lesson 6: Global and Local Measures of Spatial Association",
    "section": "What is Spatial Autocorrelation",
    "text": "What is Spatial Autocorrelation\n\nToble’s First Law of Geography\nSpatial Dependency\nSpatial Autocorrelation\n\nPositive autocorrelation\nNegative autocorrelation"
  },
  {
    "objectID": "lesson/Lesson06/Lesson06-GLSA.html#measures-of-global-spatial-autocorrelation",
    "href": "lesson/Lesson06/Lesson06-GLSA.html#measures-of-global-spatial-autocorrelation",
    "title": "Lesson 6: Global and Local Measures of Spatial Association",
    "section": "Measures of Global Spatial Autocorrelation",
    "text": "Measures of Global Spatial Autocorrelation\n\nMoran’s I\nGeary’s c"
  },
  {
    "objectID": "lesson/Lesson06/Lesson06-GLSA.html#measures-of-global-highlow-clustering-getis-ord-global-g",
    "href": "lesson/Lesson06/Lesson06-GLSA.html#measures-of-global-highlow-clustering-getis-ord-global-g",
    "title": "Lesson 6: Global and Local Measures of Spatial Association",
    "section": "Measures of Global High/Low Clustering: Getis-Ord Global G",
    "text": "Measures of Global High/Low Clustering: Getis-Ord Global G\n\n\n\nGetis-Ord Global G statistic is concerned with the overall concentration or lack of concentration in all pairs that are neighbours given the definition of neighbouring areas.\nThe variable must contain only positive values to be used.\n\n\n\nSource: Getis, A., & Ord, K. (1992). “The Analysis of Spatial Association by Use of Distance Statistics”. Geographical Analysis, 24, 189–206."
  },
  {
    "objectID": "lesson/Lesson06/Lesson06-GLSA.html#local-spatial-autocorrelation-statistics",
    "href": "lesson/Lesson06/Lesson06-GLSA.html#local-spatial-autocorrelation-statistics",
    "title": "Lesson 6: Global and Local Measures of Spatial Association",
    "section": "Local Spatial Autocorrelation Statistics",
    "text": "Local Spatial Autocorrelation Statistics\n\nA collection of geospatial statistical analysis methods for analysing the location related tendency (clusters or outliers) in the attributes of geographically referenced data (points or areas).\nCan be indecies decomposited from their global measures such as local Moran’s I, local Geary’s c, and Getis-Ord Gi*.\nThese spatial statistics are well suited for:\n\ndetecting clusters or outliers;\nidentifying hot spot or cold spot areas;\nassessing the assumptions of stationarity; and\nidentifying distances beyond which no discernible association obtains."
  },
  {
    "objectID": "lesson/Lesson06/Lesson06-GLSA.html#local-indicator-of-spatial-association-lisa",
    "href": "lesson/Lesson06/Lesson06-GLSA.html#local-indicator-of-spatial-association-lisa",
    "title": "Lesson 6: Global and Local Measures of Spatial Association",
    "section": "Local Indicator of Spatial Association (LISA)",
    "text": "Local Indicator of Spatial Association (LISA)\n\nA subset of localised geospatial statistics methods.\nAny spatial statistics that satisfies the following two requirements (Anselin, L. 1995):\n\nthe LISA for each observation gives an indication of the extent of significant spatial clustering of similar values around that observation;\nthe sum of LISAs for all observations is proportional to a global indicator of spatial association."
  },
  {
    "objectID": "lesson/Lesson06/Lesson06-GLSA.html#local-morans-i",
    "href": "lesson/Lesson06/Lesson06-GLSA.html#local-morans-i",
    "title": "Lesson 6: Global and Local Measures of Spatial Association",
    "section": "Local Moran’s I",
    "text": "Local Moran’s I\nGiven a geographically referenced attribute field, X the formula of local Moran’s I is:"
  },
  {
    "objectID": "lesson/Lesson06/Lesson06-GLSA.html#detecting-hot-and-cold-spot-areas",
    "href": "lesson/Lesson06/Lesson06-GLSA.html#detecting-hot-and-cold-spot-areas",
    "title": "Lesson 6: Global and Local Measures of Spatial Association",
    "section": "Detecting hot and cold spot areas",
    "text": "Detecting hot and cold spot areas\n\n\n\nGiven a set of geospatial features (i.e. points or polygons) and an analysis field, the spatial statistics tell you where features with either high (i.e. hot spots) or low values (cold spots) cluster spatially.\nThe spatial statistic used is called Getis-Ord Gi* statistic (pronounced G-i-star).\nGetis and Ord (1992) define the local G and G∗ statistics for region i (i=1,···,n) as:"
  },
  {
    "objectID": "lesson/Lesson06/Lesson06-GLSA.html#fixed-weighting-scheme",
    "href": "lesson/Lesson06/Lesson06-GLSA.html#fixed-weighting-scheme",
    "title": "Lesson 6: Global and Local Measures of Spatial Association",
    "section": "Fixed weighting scheme",
    "text": "Fixed weighting scheme\n\n\n\nThings to consider if fixed distance is used: - All features should have at least one neighbour.\n\nNo feature should have all other features as neighbours.\nEspecially if the values for the input field are skewed, you want features to have about eight neighbors each.\nMight produce large estimate variances where data are sparse, while mask subtle local variations where data are dense.\nIn extreme condition, fixed schemes might not be able to calibrate in local areas where data are too sparse to satisfy the calibration requirements (observations must be more than parameters)."
  },
  {
    "objectID": "lesson/Lesson06/Lesson06-GLSA.html#adaptive-weighting-schemes",
    "href": "lesson/Lesson06/Lesson06-GLSA.html#adaptive-weighting-schemes",
    "title": "Lesson 6: Global and Local Measures of Spatial Association",
    "section": "Adaptive weighting schemes",
    "text": "Adaptive weighting schemes\n\n\n\nAdaptive schemes adjust itself according to the density of data\n\nShorter bandwidths where data are dense and longer where sparse.\nFinding nearest neighbors are one of the often used approaches.\n\n\n\n]"
  },
  {
    "objectID": "lesson/Lesson06/Lesson06-GLSA.html#best-practice-guidelines",
    "href": "lesson/Lesson06/Lesson06-GLSA.html#best-practice-guidelines",
    "title": "Lesson 6: Global and Local Measures of Spatial Association",
    "section": "Best practice guidelines",
    "text": "Best practice guidelines\n\nResults are only reliable if the input feature class contains at least 30 features.\nThe input field mst be in continuous data type such as a count, rate, or other numeric measurement, no categorical attribute field is allowed."
  },
  {
    "objectID": "lesson/Lesson06/Lesson06-GLSA.html#emerging-hot-spot-analysis-ehsa",
    "href": "lesson/Lesson06/Lesson06-GLSA.html#emerging-hot-spot-analysis-ehsa",
    "title": "Lesson 6: Global and Local Measures of Spatial Association",
    "section": "Emerging Hot Spot Analysis (EHSA)",
    "text": "Emerging Hot Spot Analysis (EHSA)\n\nA technique that falls under exploratory spatial data analysis (ESDA).\nIt combines the traditional ESDA technique of hot spot analysis using the Getis-Ord Gi* statistic with the traditional time-series Mann-Kendall test for monotonic trends.\nThe goal of EHSA is to evaluate how hot and cold spots are changing over time. It helps us answer the questions: are they becoming increasingly hotter, are they cooling down, or are they staying the same?"
  },
  {
    "objectID": "lesson/Lesson06/Lesson06-GLSA.html#in-colclusion",
    "href": "lesson/Lesson06/Lesson06-GLSA.html#in-colclusion",
    "title": "Lesson 6: Global and Local Measures of Spatial Association",
    "section": "In colclusion",
    "text": "In colclusion\nSpatial statistics methods are not a blackbox. Before performing the analysis, a geospatial analyst should consider the followings:\n\nWhat is the geographical question?\nWhat is the geospatial feature?\nWhat is the analysis field?\nWhich conceptualization of spatial relationships is appropriate?"
  },
  {
    "objectID": "lesson/Lesson06/Lesson06-GLSA.html#references",
    "href": "lesson/Lesson06/Lesson06-GLSA.html#references",
    "title": "Lesson 6: Global and Local Measures of Spatial Association",
    "section": "References",
    "text": "References\n\nMoran, P. A. P. (1950). “Notes on Continuous Stochastic Phenomena”. Biometrika. 37 (1): 17–23.\nGeary, R.C. (1954) “The Contiguity Ratio and Statistical Mapping”. The Incorporated Statistician, Vol. 5, No. 3, pp. 115-127. - Moran’s I\nGeary’s c - Getis, A., & Ord, K. (1992). “The Analysis of Spatial Association by Use of Distance Statistics”. Geographical Analysis, 24, 189–206.\nAnselin, L. (1995). “Local indicators of spatial association – LISA”. Geographical Analysis, 27(4): 93-115.\nGetis, A. and Ord, J.K. (1992) “The analysis of spatial association by use of distance statistics”. Geographical Analysis, 24(3): 189-206.\nOrd, J.K. and Getis, A. (2010) “Local spatial autocorrelation statistics: Distributional issues and an application”. Geographical Analysis, 27(4): 286-306."
  },
  {
    "objectID": "lesson/Lesson03/Lesson03-SPPA.html",
    "href": "lesson/Lesson03/Lesson03-SPPA.html",
    "title": "Lesson 3: Spatial Point Patterns Analysis",
    "section": "",
    "text": "Introducing Spatial Point Patterns\n\nThe basic concepts of spatial point patterns\n1st Order versus 2nd Order\nSpatial Point Patterns in real world\n\n1st Order Spatial Point Patterns Analysis\n\nQuadrat analysis\nKernel density estimation\n\n\n\n\n2nd Order Spatial Point Patterns Analysis\n\nNearest Neighbour Index\nG-function\nF-function\nK-function\nL-function\n\n\n\n\n\nWelcome to Lesson 4: Spatial Point Pattern Analysis, a family of spatial statistics specially developed to model and to test distribution of spatial point events.\nThe lesson proceeding is divided into three main section. First, I will share with you what are real world spatial point events. This is followed by explaining the concepts and methods of 1st-order and 2nd-order spatial point patterns analysis."
  },
  {
    "objectID": "th_ex2.html",
    "href": "th_ex2.html",
    "title": "Take-home Exercise 2: Application of Geospatial Analysis Methods to Discover Thailand Drug Abuse at the Province Level",
    "section": "",
    "text": "Important\n\n\n\nThis handout provides the context, the task, the expectation and the grading criteria of Take-home Exercise 2. Students must review and understand them before getting started with the take-home exercise."
  },
  {
    "objectID": "th_ex2.html#setting-the-scene",
    "href": "th_ex2.html#setting-the-scene",
    "title": "Take-home Exercise 2: Application of Geospatial Analysis Methods to Discover Thailand Drug Abuse at the Province Level",
    "section": "Setting the Scene",
    "text": "Setting the Scene\nDrug abuse is associated with significant negative health, financial and social consequences. Yet, illicit drug consumption remains highly prevalent and continues to be a growing problem worldwide. In 2021, 1 in 17 people aged 15–64 in the world had used a drug in the past 12 months. Notwithstanding population growth, the estimated number of drug users grew from 240 million in 2011 to 296 million in 2021.\nThe geopolitics of Thailand which is near the Golden Triangle of Indochina, the largest drug production site in Asia, and the constant transportation infrastructure development made Thailand became market and transit routes for drug trafficking to the third countries.\nIn Thailand, drug abuse is one of the major social issue. There are about 2.7 million youths using drugs in Thailand. Among youths aged between 15 and 19 years, there are about 300,000 who have needs for drug treatment. Most of Thai youths involved with drugs are vocational-school students, which nearly doubles in number compared to secondary-school students.\nFigure below shows geographic distribution of drug use cases by province and by year."
  },
  {
    "objectID": "th_ex2.html#objectives",
    "href": "th_ex2.html#objectives",
    "title": "Take-home Exercise 2: Application of Geospatial Analysis Methods to Discover Thailand Drug Abuse at the Province Level",
    "section": "Objectives",
    "text": "Objectives\nAs a curious geospatial analytics green horn, you are interested to discover:\n\nif the key indicators of drug abuse of Thailand are independent from space.\nIf the indicators of drug abuse is indeed spatial dependent, then, you would like to detect where are the clusters and outliers, and the hotspots.\nLast but not least, you are also interested to investigate how the observation above evolve over time."
  },
  {
    "objectID": "th_ex2.html#the-task",
    "href": "th_ex2.html#the-task",
    "title": "Take-home Exercise 2: Application of Geospatial Analysis Methods to Discover Thailand Drug Abuse at the Province Level",
    "section": "The Task",
    "text": "The Task\nThe specific tasks of this take-home exercise are as follows:\n\nUsing appropriate function of sf and tidyverse, preparing the following geospatial data layer:\n\na study area layer in sf polygon features. It must be at province level (including Bangkok) of Thailand.\na drug abuse indicators layer within the study area in sf polygon features.\n\nUsing the extracted data, perform global spatial autocorrelation analysis by using sfdep methods.\nUsing the extracted data, perform local spatial autocorrelation analysis by using sfdep methods.\nDescribe the spatial patterns revealed by the analysis above.\n\n\n\n\n\n\n\nNote\n\n\n\nA quality research should go beyond pre-defined scope of work. Students are encouraged to extend your investigate by using the suggested. You are also encouraged to explore appropriate method(s) not mentioned above."
  },
  {
    "objectID": "th_ex2.html#the-data",
    "href": "th_ex2.html#the-data",
    "title": "Take-home Exercise 2: Application of Geospatial Analysis Methods to Discover Thailand Drug Abuse at the Province Level",
    "section": "The Data",
    "text": "The Data\nFor the purpose of this take-home exercise, two data sets shall be used, they are:\n\nThailand Drug Offenses [2017-2022] at Kaggle.\nThailand - Subnational Administrative Boundaries at HDX. You are required to use the province boundary data set."
  },
  {
    "objectID": "th_ex2.html#grading-criteria",
    "href": "th_ex2.html#grading-criteria",
    "title": "Take-home Exercise 2: Application of Geospatial Analysis Methods to Discover Thailand Drug Abuse at the Province Level",
    "section": "Grading Criteria",
    "text": "Grading Criteria\nThis exercise will be graded by using the following criteria:\n\nGeospatial Data Wrangling (20 marks): This is an important aspect of geospatial analytics. You will be assessed on your ability:\n\nto employ appropriate R functions from various R packages specifically designed for modern data science such as readxl, tidyverse (tidyr, dplyr, ggplot2), sf just to mention a few of them, to perform the import and extract the data.\nto clean and derive appropriate variables for meeting the analysis need.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nAll data are like vast grassland full of land mines. Your job is to clear those mines and not to step on them.\n\n\n\nGeospatial Analysis (30 marks): In this exercise, you are expected to use the appropriate global and local measures of spatial autocorrelation to perform the analysis. You will be assessed on your ability:\n\nto describe the methods used correctly, and\nto provide accurate interpretation of the analysis results.\n\nGeovisualisation and geocommunication (20 marks): In this section, you will be assessed on your ability to communicate the results in business friendly visual representations. This course is geospatial centric, hence, it is important for you to demonstrate your competency in using appropriate geovisualisation techniques to reveal and communicate the findings of your analysis.\nReproducibility (15 marks): This is an important learning outcome of this exercise. You will be assessed on your ability to provide a comprehensive documentation of the analysis procedures in the form of code chunks of Quarto. It is important to note that it is not enough by merely providing the code chunk without any explanation on the purpose and R function(s) used.\nBonus (15 marks): Demonstrate your ability to employ methods beyond what you had learned in class to gain insights from the data."
  },
  {
    "objectID": "th_ex2.html#submission-instructions",
    "href": "th_ex2.html#submission-instructions",
    "title": "Take-home Exercise 2: Application of Geospatial Analysis Methods to Discover Thailand Drug Abuse at the Province Level",
    "section": "Submission Instructions",
    "text": "Submission Instructions\n\nThe write-up of the take-home exercise must be in Quarto html document format. You are required to publish the write-up on Netlify.\nZip the take-home exercise folder and upload it onto eLearn. If the size of the zip file is beyond the capacity of eLearn, you can upload it on SMU OneDrive and provide the download link on eLearn.."
  },
  {
    "objectID": "th_ex2.html#due-date",
    "href": "th_ex2.html#due-date",
    "title": "Take-home Exercise 2: Application of Geospatial Analysis Methods to Discover Thailand Drug Abuse at the Province Level",
    "section": "Due Date",
    "text": "Due Date\n6 13th October 2024 (Sunday), 11.59pm (midnight)."
  },
  {
    "objectID": "th_ex2.html#learning-from-senior",
    "href": "th_ex2.html#learning-from-senior",
    "title": "Take-home Exercise 2: Application of Geospatial Analysis Methods to Discover Thailand Drug Abuse at the Province Level",
    "section": "Learning from senior",
    "text": "Learning from senior\nYou are advised to review these sample submissions prepared by your seniors.\n\nKHANT MIN NAING: Very well done in all the five grading criteria especially the ability to provide a comprehensive overview of the analysis methods used and discussion on the analysis results.\nMATTHEW HO YIWEN Able to provide a clear and comprehensive discussion on the geospatial data wrangling process and to communicate the analysis results by using appropriate geovisualisation and data visualisation methods."
  },
  {
    "objectID": "th_ex2.html#learning-from-is415",
    "href": "th_ex2.html#learning-from-is415",
    "title": "Take-home Exercise 2: Application of Geospatial Analysis Methods to Discover Thailand Drug Abuse at the Province Level",
    "section": "Learning from IS415",
    "text": "Learning from IS415\n\nKHANT MIN NAING: Very well done in all the five grading criteria especially the ability to provide a comprehensive overview of the analysis methods used and discussion on the analysis results.\nMATTHEW HO YIWEN Able to provide a clear and comprehensive discussion on the geospatial data wrangling process and to communicate the analysis results by using appropriate geovisualisation and data visualisation methods."
  },
  {
    "objectID": "th_ex2.html#q-a",
    "href": "th_ex2.html#q-a",
    "title": "Take-home Exercise 2: Application of Geospatial Analysis Methods to Discover Thailand Drug Abuse at the Province Level",
    "section": "Q & A",
    "text": "Q & A\nPlease submit your questions or queries related to this take-home exercise on Piazza."
  },
  {
    "objectID": "th_ex2.html#peer-learning",
    "href": "th_ex2.html#peer-learning",
    "title": "Take-home Exercise 2: Application of Geospatial Analysis Methods to Discover Thailand Drug Abuse at the Province Level",
    "section": "Peer Learning",
    "text": "Peer Learning"
  },
  {
    "objectID": "th_ex2.html#reference",
    "href": "th_ex2.html#reference",
    "title": "Take-home Exercise 2: Application of Geospatial Analysis Methods to Discover Thailand Drug Abuse at the Province Level",
    "section": "Reference",
    "text": "Reference\n\nWorld Drug Report 2023\nThailand Country Report 2023\nKhan, D. et. al. (2017) “Hot spots, cluster detection and spatial outlier analysis of teen birth rates in the U.S., 2003–2012”, Spatial and Spatio-temporal Epidemiology, Vol. 21, pp. 67–75.\nMuhammad Arif & Didit Purnomo (2017) “Measuring Spatial Cluster for Leading Industries in Surakarta with Exploratory Spatial Data Analysis (ESDA)”, Jurnal Ekonomi Pembangunan, Vol. 18 (1), pp. 64-81.\nStamatis Kalogirou (2012) “Testing local versions of correlation coefficients”, Review of Regional Research, Vol., pp. 45–61. SMU e-journal."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html",
    "title": "In-class Exercise 1",
    "section": "",
    "text": "The taskThe code\n\n\nFor the purpose of this in-class exercise, tidyverse, sf and ggstatsplot packages will be used. Write a code chunk to check if these two packages have been installed in R. If yes, load them in R environment.\n\n\n\npacman::p_load(tidyverse, sf, ggstatsplot)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#getting-started",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#getting-started",
    "title": "In-class Exercise 1",
    "section": "",
    "text": "The taskThe code\n\n\nFor the purpose of this in-class exercise, tidyverse, sf and ggstatsplot packages will be used. Write a code chunk to check if these two packages have been installed in R. If yes, load them in R environment.\n\n\n\npacman::p_load(tidyverse, sf, ggstatsplot)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#working-with-master-plan-planning-sub-zone-data",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#working-with-master-plan-planning-sub-zone-data",
    "title": "In-class Exercise 1",
    "section": "Working with Master Plan Planning Sub-zone Data",
    "text": "Working with Master Plan Planning Sub-zone Data\n\nThe taskThe code\n\n\n\nCreate a sub-folder called data in In-class_Ex01 folder.\nIf necessary visit data.gov.sg and download Master Plan 2014 Subzone Boundary (Web) from the portal. You are required to download both the ESRI shapefile and kml file.\nWrite a code chunk to import Master Plan 2014 Subzone Boundary (Web) in shapefile and kml save them in sf simple features data frame.\n\n\n\n\nmpsz_shp &lt;- st_read(dsn = \"data/\",\n                layer = \"MPSZ-2019\")\n\nReading layer `MPSZ-2019' from data source \n  `D:\\tskam\\IS415_AY2024-25T1\\In-class_Ex\\In-class_Ex01\\data' \n  using driver `ESRI Shapefile'\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nGeodetic CRS:  WGS 84\n\nmpsz_kml &lt;- st_read(\"data/MP19_SUBZONE_WEB_PL.kml\")\n\nReading layer `URA_MP19_SUBZONE_NO_SEA_PL' from data source \n  `D:\\tskam\\IS415_AY2024-25T1\\In-class_Ex\\In-class_Ex01\\data\\MP19_SUBZONE_WEB_PL.kml' \n  using driver `KML'\nSimple feature collection with 332 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY, XYZ\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#working-with-pre-school-location-data",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#working-with-pre-school-location-data",
    "title": "In-class Exercise 1",
    "section": "Working with Pre-school Location Data",
    "text": "Working with Pre-school Location Data\n\nThe taskThe code\n\n\n\nIf necessary visit data.gov.sg and download Pre-Schools Location from the portal. You are required to download both the kml and geojson files.\nWrite a code chunk to import Pre-Schools Location in kml geojson save them in sf simple features data frame.\n\n\n\n\npreschool_kml &lt;- st_read(\"data/PreSchoolsLocation.kml\")\n\nReading layer `PRESCHOOLS_LOCATION' from data source \n  `D:\\tskam\\IS415_AY2024-25T1\\In-class_Ex\\In-class_Ex01\\data\\PreSchoolsLocation.kml' \n  using driver `KML'\nSimple feature collection with 2290 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6878 ymin: 1.247759 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\npreschool_geojson &lt;- st_read(\"data/PreSchoolsLocation.geojson\") \n\nReading layer `PreSchoolsLocation' from data source \n  `D:\\tskam\\IS415_AY2024-25T1\\In-class_Ex\\In-class_Ex01\\data\\PreSchoolsLocation.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 2290 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6878 ymin: 1.247759 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#handling-coordinate-systems",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#handling-coordinate-systems",
    "title": "In-class Exercise 1",
    "section": "Handling Coordinate Systems",
    "text": "Handling Coordinate Systems\n\nChecking coordinate system\n\nThe taskThe code\n\n\nWrite a code chunk to check the project of the imported sf objects.\n\n\n\nst_crs(mpsz_shp)\n\nCoordinate Reference System:\n  User input: WGS 84 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4326]]\n\n\n\n\n\n\n\nTransforming coordinate system\n\nThe taskThe code\n\n\nRe-write the code chunk to import the Master Plan Sub-zone 2019 and Pre-schools Location with proper transformation\n\n\n\nmpsz &lt;- st_read(dsn = \"data/\",\n                layer = \"MPSZ-2019\") %&gt;%\n  st_transform(crs = 3414)\n\nReading layer `MPSZ-2019' from data source \n  `D:\\tskam\\IS415_AY2024-25T1\\In-class_Ex\\In-class_Ex01\\data' \n  using driver `ESRI Shapefile'\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nGeodetic CRS:  WGS 84\n\npreschool &lt;- st_read(\"data/PreSchoolsLocation.kml\") %&gt;%\n  st_transform(crs = 3414)\n\nReading layer `PRESCHOOLS_LOCATION' from data source \n  `D:\\tskam\\IS415_AY2024-25T1\\In-class_Ex\\In-class_Ex01\\data\\PreSchoolsLocation.kml' \n  using driver `KML'\nSimple feature collection with 2290 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6878 ymin: 1.247759 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#geospatial-data-wrangling",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#geospatial-data-wrangling",
    "title": "In-class Exercise 1",
    "section": "Geospatial Data Wrangling",
    "text": "Geospatial Data Wrangling\n\nPoint-in-Polygon count\n\nThe taskThe code\n\n\nWrite a code chunk to count the number of pre-schools in each planning sub-zone.\n\n\n\nmpsz &lt;- mpsz %&gt;%\n  mutate(`PreSch Count` = lengths(st_intersects(mpsz, preschool)))\n\n\n\n\n\n\nComputing density\n\nThe taskThe code\n\n\nWrite a single line code to perform the following tasks:\n\nDerive the area of each planning sub-zone.\nDrop the unit of measurement of the area (i.e. m^2)\nCalculate the density of pre-school at the planning sub-zone level.\n\n\n\n\nmpsz &lt;- mpsz %&gt;%\n  mutate(Area = units::drop_units(\n    st_area(.)),\n    `PreSch Density` = `PreSch Count` / Area * 1000000\n  )"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#statistical-analysis",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#statistical-analysis",
    "title": "In-class Exercise 1",
    "section": "Statistical Analysis",
    "text": "Statistical Analysis\n\nThe taskThe codeThe plot\n\n\nUsing appropriate Exploratory Data Analysis (EDA) and Confirmatory Data Analysis (CDA) methods to explore and confirm the statistical relationship between Pre-school Density and Pre-school count.\nTip: Refer to ggscatterstats() of ggstatsplot package.\n\n\n\nmpsz$`PreSch Density` &lt;- as.numeric(as.character(mpsz$`PreSch Density`))\nmpsz$`PreSch Count` &lt;- as.numeric(as.character(mpsz$`PreSch Count`)) \nmpsz &lt;- as.data.frame(mpsz)\n\nggscatterstats(data = mpsz,\n               x = `PreSch Density`,\n               y = `PreSch Count`,\n               type = \"parametric\")\n\n\n\n\n\n\n\n\n\n\n\n\nCommet:"
  },
  {
    "objectID": "ShinyWorkshop/Shiny1/Shiny1.html",
    "href": "ShinyWorkshop/Shiny1/Shiny1.html",
    "title": "Building Web-enabled Geospatial Analytics Applications with Shiny: Shiny basic",
    "section": "",
    "text": "What is a Web-enabled Geospatial Analytics Application?\nWhy building Web-enabled Geospatial Analytical Application?\nEvolution of web-based Technology\nGetting to Know Shiny"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/data/MPSZ-2019.html",
    "href": "In-class_Ex/In-class_Ex01/data/MPSZ-2019.html",
    "title": "IS415 AY2022-23T2",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n        0 0     false"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#getting-started",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#getting-started",
    "title": "In-class Exercise 2",
    "section": "Getting started",
    "text": "Getting started\n\nThe taskThe code\n\n\nFor the purpose of this in-class exercise, tidyverseand sf packages will be used. Write a code chunk to check if these two packages have been installed in R. If yes, load them in R environment.\n\n\n\n\npacman::p_load(tidyverse, sf)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#working-with-master-plan-planning-sub-zone-data",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#working-with-master-plan-planning-sub-zone-data",
    "title": "In-class Exercise 2",
    "section": "Working with Master Plan Planning Sub-zone Data",
    "text": "Working with Master Plan Planning Sub-zone Data\n\nThe taskThe code\n\n\n\nCreate a sub-folder called data in In-class_Ex02 folder.\nIf necessary visit data.gov.sg and download Master Plan 2014 Subzone Boundary (Web) from the portal. You are required to download both the ESRI shapefile and kml file.\nWrite a code chunk to import Master Plan 2014 Subzone Boundary (Web) in shapefile and kml save them in sf simple features data frame.\n\n\n\n\nThis code chunk imports shapefile.\n\nmpsz14_shp &lt;- st_read(dsn = \"data/\",\n                layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `D:\\tskam\\IS415_AY2024-25T1\\In-class_Ex\\In-class_Ex02\\data' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nThis code chunk imports kml file.\n\nmpsz14_kml &lt;- st_read(\"data/MasterPlan2014SubzoneBoundaryWebKML.kml\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#working-with-master-plan-planning-sub-zone-data-1",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#working-with-master-plan-planning-sub-zone-data-1",
    "title": "In-class Exercise 2",
    "section": "Working with Master Plan Planning Sub-zone Data",
    "text": "Working with Master Plan Planning Sub-zone Data\n\nThe taskThe code\n\n\n\nWrite a code chunk to export mpsz14_shp sf data.frame into kml file save the output in data sub-folder. Name the output file MP14_SUBZONE_WEB_PL.\n\n\n\n\n\nst_write(mpsz14_shp, \n         \"data/MP14_SUBZONE_WEB_PL.kml\",\n         delete_dsn = TRUE)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#working-with-pre-school-location-data",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#working-with-pre-school-location-data",
    "title": "In-class Exercise 2",
    "section": "Working with Pre-school Location Data",
    "text": "Working with Pre-school Location Data\n\nThe taskThe code\n\n\n\nIf necessary visit data.gov.sg and download Pre-Schools Location from the portal. You are required to download both the kml and geojson files.\nWrite a code chunk to import Pre-Schools Location in kml geojson save them in sf simple features data frame.\n\n\n\n\nThis code chunk imports kml file.\n\npreschool_kml &lt;- st_read(\"data/PreSchoolsLocation.kml\")\n\nThis code chunk imports geojson file.\n\npreschool_geojson &lt;- st_read(\"data/PreSchoolsLocation.geojson\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#working-with-master-plan-2019-subzone-boundary-data",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#working-with-master-plan-2019-subzone-boundary-data",
    "title": "In-class Exercise 2",
    "section": "Working with Master Plan 2019 Subzone Boundary Data",
    "text": "Working with Master Plan 2019 Subzone Boundary Data\n\nThe taskTo import shapefileTo import kml\n\n\n\nVisit data.gov.sg and download Master Plan 2019 Subzone Boundary (No Sea) from the portal. You are required to download both the kml file.\nMove MPSZ-2019 shapefile provided for In-class Exercise 1 folder on elearn to data sub-folder of In-class_Ex02.\nWrite a code chunk to import Master Plan 2019 Subzone Boundary (No SEA) kml and MPSZ-2019 into sf simple feature data.frame.\n\n\n\n\n\nmpsz19_shp &lt;- st_read(dsn = \"data/\",\n                layer = \"MPSZ-2019\")\n\nReading layer `MPSZ-2019' from data source \n  `D:\\tskam\\IS415_AY2024-25T1\\In-class_Ex\\In-class_Ex02\\data' \n  using driver `ESRI Shapefile'\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nGeodetic CRS:  WGS 84\n\n\n\n\n\n\n\nmpsz19_kml &lt;- st_read(\"data/MasterPlan2019SubzoneBoundaryNoSeaKML.kml\")\n\nReading layer `URA_MP19_SUBZONE_NO_SEA_PL' from data source \n  `D:\\tskam\\IS415_AY2024-25T1\\In-class_Ex\\In-class_Ex02\\data\\MasterPlan2019SubzoneBoundaryNoSeaKML.kml' \n  using driver `KML'\nSimple feature collection with 332 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY, XYZ\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#handling-coordinate-systems",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#handling-coordinate-systems",
    "title": "In-class Exercise 2",
    "section": "Handling Coordinate Systems",
    "text": "Handling Coordinate Systems\nChecking coordinate system\n\nThe taskThe code\n\n\nWrite a code chunk to check the project of the imported sf objects.\n\n\n\n\nst_crs(mpsz19_shp)\n\nCoordinate Reference System:\n  User input: WGS 84 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4326]]"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#handling-coordinate-systems-1",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#handling-coordinate-systems-1",
    "title": "In-class Exercise 2",
    "section": "Handling Coordinate Systems",
    "text": "Handling Coordinate Systems\nTransforming coordinate system\n\nThe taskTo import MPSZ-2019To import PreSchoolsLocation.kml\n\n\nRe-write the code chunk to import the Master Plan Sub-zone 2019 and Pre-schools Location with proper transformation\n\n\n\n\nmpsz19_shp &lt;- st_read(dsn = \"data/\",\n                layer = \"MPSZ-2019\") %&gt;%\n  st_transform(crs = 3414)\n\nReading layer `MPSZ-2019' from data source \n  `D:\\tskam\\IS415_AY2024-25T1\\In-class_Ex\\In-class_Ex02\\data' \n  using driver `ESRI Shapefile'\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nGeodetic CRS:  WGS 84\n\n\n\n\n\n\n\npreschool &lt;- st_read(\"data/PreSchoolsLocation.kml\") %&gt;%\n  st_transform(crs = 3414)\n\nReading layer `PRESCHOOLS_LOCATION' from data source \n  `D:\\tskam\\IS415_AY2024-25T1\\In-class_Ex\\In-class_Ex02\\data\\PreSchoolsLocation.kml' \n  using driver `KML'\nSimple feature collection with 2290 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6878 ymin: 1.247759 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#geospatial-data-wrangling",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#geospatial-data-wrangling",
    "title": "In-class Exercise 2",
    "section": "Geospatial Data Wrangling",
    "text": "Geospatial Data Wrangling\nPoint-in-Polygon count\n\nThe taskThe code\n\n\nWrite a code chunk to count the number of pre-schools in each planning sub-zone.\n\n\n\n\nmpsz19_shp &lt;- mpsz19_shp %&gt;%\n  mutate(`PreSch Count` = lengths(\n    st_intersects(mpsz19_shp, preschool)))"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#geospatial-data-wrangling-1",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#geospatial-data-wrangling-1",
    "title": "In-class Exercise 2",
    "section": "Geospatial Data Wrangling",
    "text": "Geospatial Data Wrangling\nComputing density\n\nThe taskThe code\n\n\nWrite a single line code to perform the following tasks:\n\nDerive the area of each planning sub-zone.\nDrop the unit of measurement of the area (i.e. m^2)\nCalculate the density of pre-school at the planning sub-zone level.\n\n\n\n\n\nmpsz19_shp &lt;- mpsz19_shp %&gt;%\n  mutate(Area = units::drop_units(\n    st_area(.)),\n    `PreSch Density` = `PreSch Count` / Area * 1000000\n  )"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#statistical-analysis",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#statistical-analysis",
    "title": "In-class Exercise 2",
    "section": "Statistical Analysis",
    "text": "Statistical Analysis\n\nThe taskThe codeThe plot\n\n\nUsing appropriate Exploratory Data Analysis (EDA) and Confirmatory Data Analysis (CDA) methods to explore and confirm the statistical relationship between Pre-school Density and Pre-school count.\nTip: Refer to ggscatterstats() of ggstatsplot package.\n\n\n\n\nmpsz$`PreSch Density` &lt;- as.numeric(as.character(mpsz19_shp$`PreSch Density`))\nmpsz$`PreSch Count` &lt;- as.numeric(as.character(mpsz19_shp$`PreSch Count`)) \nmpsz19_shp &lt;- as.data.frame(mpsz19_shp)\n\nggscatterstats(data = mpsz19_shp,\n               x = `PreSch Density`,\n               y = `PreSch Count`,\n               type = \"parametric\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#working-with-population-data",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#working-with-population-data",
    "title": "In-class Exercise 2",
    "section": "Working with Population Data",
    "text": "Working with Population Data\n\nThe taskThe code\n\n\n\nVisit and extract the latest Singapore Residents by Planning Area / Subzone, Age Group, Sex and Type of Dwelling from Singstat homepage.\n\n\n\n\n\npopdata &lt;- read_csv(\"data/respopagesextod2023.csv\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#data-wrangling",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#data-wrangling",
    "title": "In-class Exercise 2",
    "section": "Data Wrangling",
    "text": "Data Wrangling\n\nThe taskThe code\n\n\n\nWrite a code chunk to prepare a data.frame showing population by Planning Area and Planning subzone\n\n\n\n\n\npopdata2023 &lt;- popdata %&gt;% \n  group_by(PA, SZ, AG) %&gt;% \n  summarise(`POP`=sum(`Pop`)) %&gt;%  \n  ungroup() %&gt;% \n  pivot_wider(names_from=AG,\n              values_from = POP)\n\ncolnames(popdata2023)\n\n [1] \"PA\"          \"SZ\"          \"0_to_4\"      \"10_to_14\"    \"15_to_19\"   \n [6] \"20_to_24\"    \"25_to_29\"    \"30_to_34\"    \"35_to_39\"    \"40_to_44\"   \n[11] \"45_to_49\"    \"50_to_54\"    \"55_to_59\"    \"5_to_9\"      \"60_to_64\"   \n[16] \"65_to_69\"    \"70_to_74\"    \"75_to_79\"    \"80_to_84\"    \"85_to_89\"   \n[21] \"90_and_Over\""
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#data-processing",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#data-processing",
    "title": "In-class Exercise 2",
    "section": "Data Processing",
    "text": "Data Processing\n\nThe taskThe code\n\n\nWrite a code chunk to derive a tibble data.framewith the following fields PA, SZ, YOUNG, ECONOMY ACTIVE, AGED, TOTAL, DEPENDENCY where by:\n\nYOUNG: age group 0 to 4 until age groyup 20 to 24,\nECONOMY ACTIVE: age group 25-29 until age group 60-64,\nAGED: age group 65 and above,\nTOTAL: all age group, and\nDEPENDENCY: the ratio between young and aged against economy active group.\n\n\n\n\n\npopdata2023 &lt;- popdata2023 %&gt;%\n  mutate(YOUNG=rowSums(.[3:6]) # Aged 0 - 24, 10 - 24\n         +rowSums(.[14])) %&gt;% # Aged 5 - 9\n  mutate(`ECONOMY ACTIVE` = rowSums(.[7:13])+ # Aged 25 - 59\n  rowSums(.[15])) %&gt;%  # Aged 60 -64\n  mutate(`AGED`=rowSums(.[16:21])) %&gt;%\n  mutate(`TOTAL`=rowSums(.[3:21])) %&gt;%\n  mutate(`DEPENDENCY`=(`YOUNG` + `AGED`)\n  / `ECONOMY ACTIVE`) %&gt;% \n  select(`PA`, `SZ`, `YOUNG`, \n         `ECONOMY ACTIVE`, `AGED`,\n         `TOTAL`, `DEPENDENCY`)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#joining-popdata2023-and-mpsz19_shp",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#joining-popdata2023-and-mpsz19_shp",
    "title": "In-class Exercise 2",
    "section": "Joining popdata2023 and mpsz19_shp",
    "text": "Joining popdata2023 and mpsz19_shp\nThe code chunk below is used to change data in the PA and SZ fields into uppercase.\n\n\npopdata2023 &lt;- popdata2023 %&gt;%\n  mutate_at(.vars = vars(PA, SZ), \n          .funs = list(toupper)) \n\n\nThe code chunk below is used to perform left-join whereby the join fields are SUBZONE_N from the mpsz19_shp sf data.frame and SZ from the popdata2023 data.frame.\n\n\nmpsz_pop2023 &lt;- left_join(mpsz19_shp, popdata2023,\n                          by = c(\"SUBZONE_N\" = \"SZ\"))"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#choropleth-map-of-dependency-ratio-by-planning-subzone",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02.html#choropleth-map-of-dependency-ratio-by-planning-subzone",
    "title": "In-class Exercise 2",
    "section": "Choropleth Map of Dependency Ratio by Planning Subzone",
    "text": "Choropleth Map of Dependency Ratio by Planning Subzone\n\nThe mapThe code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2023)+\n  \n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"Dependency ratio\") +\n  \n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.title.size = 1,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            bg.color = \"#E4D5C9\",\n            frame = F) +\n  \n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 1.5) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics (DOS)\", \n             position = c(\"left\", \"bottom\"))"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/data/MPSZ-2019.html",
    "href": "In-class_Ex/In-class_Ex02/data/MPSZ-2019.html",
    "title": "IS415 AY2024-25T1",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n        0 0     false"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class Ex04-stpp.html",
    "href": "In-class_Ex/In-class_Ex04/In-class Ex04-stpp.html",
    "title": "Spatio-Temporal Point Patterns Analysis",
    "section": "",
    "text": "A spatio-temporal point process (also called space-time or spatial-temporal point process) is a random collection of points, where each point represents the time and location of an event. Examples of events include incidence of disease, sightings or births of a species, or the occurrences of fires, earthquakes, lightning strikes, tsunamis, or volcanic eruptions.\nThe analysis of spatio-temporal point patterns is becoming increasingly necessary, given the rapid emergence of geographically and temporally indexed data in a wide range of fields. Several spatio-temporal point patterns analysis methods have been introduced and implemented in R in the last ten years. This chapter shows how various R packages can be combined to run a set of spatio-temporal point pattern analyses in a guided and intuitive way. A real world forest fire events in Kepulauan Bangka Belitung, Indonesia from 1st January 2023 to 31st December 2023 is used to illustrate the methods, procedures and interpretations."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class Ex04-stpp.html#overview",
    "href": "In-class_Ex/In-class_Ex04/In-class Ex04-stpp.html#overview",
    "title": "Spatio-Temporal Point Patterns Analysis",
    "section": "",
    "text": "A spatio-temporal point process (also called space-time or spatial-temporal point process) is a random collection of points, where each point represents the time and location of an event. Examples of events include incidence of disease, sightings or births of a species, or the occurrences of fires, earthquakes, lightning strikes, tsunamis, or volcanic eruptions.\nThe analysis of spatio-temporal point patterns is becoming increasingly necessary, given the rapid emergence of geographically and temporally indexed data in a wide range of fields. Several spatio-temporal point patterns analysis methods have been introduced and implemented in R in the last ten years. This chapter shows how various R packages can be combined to run a set of spatio-temporal point pattern analyses in a guided and intuitive way. A real world forest fire events in Kepulauan Bangka Belitung, Indonesia from 1st January 2023 to 31st December 2023 is used to illustrate the methods, procedures and interpretations."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class Ex04-stpp.html#learning-outcome",
    "href": "In-class_Ex/In-class_Ex04/In-class Ex04-stpp.html#learning-outcome",
    "title": "Spatio-Temporal Point Patterns Analysis",
    "section": "Learning Outcome",
    "text": "Learning Outcome\n\nThe research questions\nThe specific questions we would like to answer are:\n\nare the locations of forest fire in Kepulauan Bangka Belitung spatial and spatio-temporally independent?\nif the answer is NO, where and when the observed forest fire locations tend to cluster?"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class Ex04-stpp.html#the-data",
    "href": "In-class_Ex/In-class_Ex04/In-class Ex04-stpp.html#the-data",
    "title": "Spatio-Temporal Point Patterns Analysis",
    "section": "The data",
    "text": "The data\nFor the purpose of this exercise, two data sets will be used, they are:\n\nforestfires, a csv file provides locations of forest fire detected from the Moderate Resolution Imaging Spectroradiometer (MODIS) sensor data. The data are downloaded from Fire Information for Resource Management System. For the purpose of this exercise, only forest fires within Kepulauan Bangka Belitung will be used.\n\nKepulauan_Bangka_Belitung, an ESRI shapefile showing the sub-district (i.e. kelurahan) boundary of Kepulauan Bangka Belitung. The data set was downloaded from Indonesia Geospatial portal. The original data covers the whole Indonesia. For the purpose of this exercise, only sub-districts within Kepulauan Bangka Belitung are extracted."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class Ex04-stpp.html#installing-and-loading-the-r-packages",
    "href": "In-class_Ex/In-class_Ex04/In-class Ex04-stpp.html#installing-and-loading-the-r-packages",
    "title": "Spatio-Temporal Point Patterns Analysis",
    "section": "Installing and Loading the R packages",
    "text": "Installing and Loading the R packages\nFor the purpose of this study, six R packages will be used. They are:\n\nsf provides functions for importing processing and wrangling geospatial data,,\nraster for handling raster data in R,\nspatstat for performing Spatial Point Patterns Analysis such as kcross, Lcross, etc.,\nsparr provides functions to estimate fixed and adaptive kernel-smoothed spatial relative risk surfaces via the density-ratio method and perform subsequent inference. Fixed-bandwidth spatiotemporal density and relative risk estimation is also supported\ntmap provides functions to produce cartographic quality thematic maps, and\ntidyverse, a family of R packages that provide functions to perform common data science tasks including and not limited to data import, data transformation, data wrangling and data visualisation.\n\n\nDIYThe solution\n\n\nUsing the steps you learned from previous chapter, write a code chunk to load the packages above onto R environment.\n\n\n\npacman::p_load(sf, raster, spatstat, sparr, tmap, stopp, tidyverse, stpp)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class Ex04-stpp.html#importing-and-preparing-study-area",
    "href": "In-class_Ex/In-class_Ex04/In-class Ex04-stpp.html#importing-and-preparing-study-area",
    "title": "Spatio-Temporal Point Patterns Analysis",
    "section": "Importing and Preparing Study Area",
    "text": "Importing and Preparing Study Area\n\nImporting study area\nCode chunk below is used import study area (i.e. Kepulauan Bangka Belitung) into R environment.\n\nkbb &lt;- st_read(dsn=\"data/rawdata\",\n               layer = \"Kepulauan_Bangka_Belitung\") %&gt;%\n  st_transform(crs = 32748)\n\nThe revised code chunk.\n\nkbb_sf &lt;- st_read(dsn=\"data/rawdata\",\n               layer = \"Kepulauan_Bangka_Belitung\") %&gt;%\n  st_union() %&gt;%\n  st_zm(drop = TRUE, what = \"ZM\") %&gt;%\n  st_transform(crs = 32748)\n\nReading layer `Kepulauan_Bangka_Belitung' from data source \n  `D:\\tskam\\IS415_AY2024-25T1\\In-class_Ex\\In-class_Ex04\\data\\rawdata' \n  using driver `ESRI Shapefile'\nSimple feature collection with 298 features and 27 fields\nGeometry type: POLYGON\nDimension:     XYZ\nBounding box:  xmin: 105.1085 ymin: -3.116593 xmax: 106.8488 ymax: -1.501603\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\n\n\nConverting OWIN\nNext, as.owin() is used to convert kbb into an owin object.\n\nkbb_owin &lt;- as.owin(kbb_sf)\nkbb_owin\n\nwindow: polygonal boundary\nenclosing rectangle: [512066.8, 705559.4] x [9655398, 9834006] units\n\n\nNext, class() is used to confirm if the output is indeed an owin object.\n\nclass(kbb_owin)\n\n[1] \"owin\"\n\n\n\nwrite_rds(kbb_owin, \"data/rds/kbb_owin\")\n\n\nkbb_owin &lt;- read_rds(\"data/rds/kbb_owin\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class Ex04-stpp.html#importing-and-preparing-forest-fire-data",
    "href": "In-class_Ex/In-class_Ex04/In-class Ex04-stpp.html#importing-and-preparing-forest-fire-data",
    "title": "Spatio-Temporal Point Patterns Analysis",
    "section": "Importing and Preparing Forest Fire data",
    "text": "Importing and Preparing Forest Fire data\nNext, we will import the forest fire data set (i.e. forestfires.csv) into R environment.\n\nfire_sf &lt;- read_csv(\"data/rawdata/forestfires.csv\") %&gt;%\n  st_as_sf(coords = c(\"longitude\", \"latitude\"),\n                       crs = 4326) %&gt;%\n  st_transform(crs = 32748)\n\nBecause ppp object only accept numerical or character as mark. The code chunk below is used to convert data type of acq_date to numeric.\n\nfire_sf &lt;- fire_sf %&gt;% \n  mutate(DayofYear = yday(acq_date)) %&gt;%\n  mutate(Month_num = month(acq_date)) %&gt;%\n  mutate(Month_fac = month(acq_date, \n                           label = TRUE, \n                           abbr = FALSE))"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class Ex04-stpp.html#visualising-the-fire-points",
    "href": "In-class_Ex/In-class_Ex04/In-class Ex04-stpp.html#visualising-the-fire-points",
    "title": "Spatio-Temporal Point Patterns Analysis",
    "section": "Visualising the Fire Points",
    "text": "Visualising the Fire Points\n\nOverall plot\n\nDIYThe code\n\n\nUsing the steps you learned in Hands-on Exercise 2, prepare a point symbol map showing the distribution of fire points. The map should look similar to the figure below.\n\n\n\n\n\n\n\n\n\n\n\n\ntm_shape(kbb_sf)+\n  tm_polygons() +\ntm_shape(fire_sf) +\n  tm_dots()\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisuaising geographic distribution of forest fires by month\n\nDIYThe code\n\n\nUsing the steps you learned in Hands-on Exercise 2, prepare a point symbol map showing the monthly geographic distribution of forest fires in 2023. The map should look similar to the figure below.\n\n\n\n\n\n\n\n\n\n\n\n\ntm_shape(kbb_sf)+\n  tm_polygons() +\ntm_shape(fire_sf) +\n  tm_dots(size = 0.1) +\ntm_facets(by=\"Month_fac\", \n            free.coords=FALSE, \n            drop.units = TRUE)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class Ex04-stpp.html#computing-stkde-by-month",
    "href": "In-class_Ex/In-class_Ex04/In-class Ex04-stpp.html#computing-stkde-by-month",
    "title": "Spatio-Temporal Point Patterns Analysis",
    "section": "Computing STKDE by Month",
    "text": "Computing STKDE by Month\nIn this section, you will learn how to compute STKDE by using spattemp.density() of sparr package. Before using the function, it is highly recommended you read the function’s reference guide in detail in order to understand the input data requirements and the output object generated.\n\nExtracting forest fires by month\nThe code chunk below is used to remove the unwanted fields from fire_sf sf data.frame. This is because as.ppp() only need the mark field and geometry field from the input sf data.frame.\n\nfire_month &lt;- fire_sf %&gt;% \n  select(Month_num)\n\n\n\nCreating ppp\nThe code chunk below is used to derive a ppp object called fire_month from fire_month sf data.frame.\n\nfire_month_ppp &lt;- as.ppp(fire_month)\nfire_month_ppp\n\nMarked planar point pattern: 741 points\nmarks are numeric, of storage type  'double'\nwindow: rectangle = [521564.1, 695791] x [9658137, 9828767] units\n\n\nThe code chunk below is used to check the output is in the correct object class.\n\nsummary(fire_month_ppp)\n\nMarked planar point pattern:  741 points\nAverage intensity 2.49258e-08 points per square unit\n\nCoordinates are given to 10 decimal places\n\nmarks are numeric, of type 'double'\nSummary:\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   8.000   9.000   8.579  10.000  12.000 \n\nWindow: rectangle = [521564.1, 695791] x [9658137, 9828767] units\n                    (174200 x 170600 units)\nWindow area = 29728200000 square units\n\n\nNext, we will check if there are duplicated point events by using the code chunk below.\n\nany(duplicated(fire_month_ppp))\n\n[1] FALSE\n\n\n\n\nIncluding Owin object\nThe code chunk below is used to combine origin_am_ppp and am_owin objects into one.\n\nfire_month_owin &lt;- fire_month_ppp[kbb_owin]\nsummary(fire_month_owin)\n\nMarked planar point pattern:  741 points\nAverage intensity 6.424519e-08 points per square unit\n\nCoordinates are given to 10 decimal places\n\nmarks are numeric, of type 'double'\nSummary:\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   8.000   9.000   8.579  10.000  12.000 \n\nWindow: polygonal boundary\n2 separate polygons (no holes)\n           vertices        area relative.area\npolygon 1     47493 11533600000      1.00e+00\npolygon 2       256      306427      2.66e-05\nenclosing rectangle: [512066.8, 705559.4] x [9655398, 9834006] units\n                     (193500 x 178600 units)\nWindow area = 11533900000 square units\nFraction of frame area: 0.334\n\n\nAs a good practice, plot() is used to plot ff_owin so that we can examine the correctness of the output object.\n\nplot(fire_month_owin)\n\n\n\n\n\n\n\n\n\n\nComputing Spatio-temporal KDE\nNext, spattemp.density() of sparr package is used to compute the STKDE.\n\nst_kde &lt;- spattemp.density(fire_month_owin)\nsummary(st_kde)\n\nSpatiotemporal Kernel Density Estimate\n\nBandwidths\n  h = 15102.47 (spatial)\n  lambda = 0.0304 (temporal)\n\nNo. of observations\n  741 \n\nSpatial bound\n  Type: polygonal\n  2D enclosure: [512066.8, 705559.4] x [9655398, 9834006]\n\nTemporal bound\n  [1, 12]\n\nEvaluation\n  128 x 128 x 12 trivariate lattice\n  Density range: [1.233458e-27, 8.202976e-10]\n\n\n\n\nPlotting the spatio-temporal KDE object\nIn the code chunk below, plot() of R base is used to the KDE for between July 2023 - December 2023.\n\ntims &lt;- c(7,8,9,10,11,12)\npar(mfcol=c(2,3))\nfor(i in tims){ \n  plot(st_kde, i, \n       override.par=FALSE, \n       fix.range=TRUE, \n       main=paste(\"KDE at month\",i))\n}"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class Ex04-stpp.html#computing-stkde-by-day-of-year",
    "href": "In-class_Ex/In-class_Ex04/In-class Ex04-stpp.html#computing-stkde-by-day-of-year",
    "title": "Spatio-Temporal Point Patterns Analysis",
    "section": "Computing STKDE by Day of Year",
    "text": "Computing STKDE by Day of Year\nIn this section, you will learn how to computer the STKDE of forest fires by day of year.\n\nCreating ppp object\nIn the code chunk below, DayofYear field is included in the output ppp object.\n\nfire_yday_ppp &lt;- fire_sf %&gt;% \n  select(DayofYear) %&gt;%\n  as.ppp()\n\n\n\nIncluding Owin object\nNext, code chunk below is used to combine the ppp object and the owin object.\n\nfire_yday_owin &lt;- fire_yday_ppp[kbb_owin]\nsummary(fire_yday_owin)\n\nMarked planar point pattern:  741 points\nAverage intensity 6.424519e-08 points per square unit\n\nCoordinates are given to 10 decimal places\n\nmarks are numeric, of type 'double'\nSummary:\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   10.0   213.0   258.0   245.9   287.0   352.0 \n\nWindow: polygonal boundary\n2 separate polygons (no holes)\n           vertices        area relative.area\npolygon 1     47493 11533600000      1.00e+00\npolygon 2       256      306427      2.66e-05\nenclosing rectangle: [512066.8, 705559.4] x [9655398, 9834006] units\n                     (193500 x 178600 units)\nWindow area = 11533900000 square units\nFraction of frame area: 0.334\n\n\n\n\n\n\nkde_yday &lt;- spattemp.density(\n  fire_yday_owin)\nsummary(kde_yday)\n\nSpatiotemporal Kernel Density Estimate\n\nBandwidths\n  h = 15102.47 (spatial)\n  lambda = 6.3198 (temporal)\n\nNo. of observations\n  741 \n\nSpatial bound\n  Type: polygonal\n  2D enclosure: [512066.8, 705559.4] x [9655398, 9834006]\n\nTemporal bound\n  [10, 352]\n\nEvaluation\n  128 x 128 x 343 trivariate lattice\n  Density range: [3.959516e-27, 2.751287e-12]\n\n\n\nplot(kde_yday)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class Ex04-stpp.html#computing-stkde-by-day-of-year-improved-method",
    "href": "In-class_Ex/In-class_Ex04/In-class Ex04-stpp.html#computing-stkde-by-day-of-year-improved-method",
    "title": "Spatio-Temporal Point Patterns Analysis",
    "section": "Computing STKDE by Day of Year: Improved method",
    "text": "Computing STKDE by Day of Year: Improved method\nOne of the nice function provides in sparr package is BOOT.spattemp(). It support bandwidth selection for standalone spatiotemporal density/intensity based on bootstrap estimation of the MISE, providing an isotropic scalar spatial bandwidth and a scalar temporal bandwidth.\nCode chunk below uses BOOT.spattemp() to determine both the spatial bandwidth and the scalar temporal bandwidth.\n\nset.seed(1234)\nBOOT.spattemp(fire_yday_owin) \n\nInitialising...Done.\nOptimising...\nh = 15102.47 \b; lambda = 16.84806 \nh = 16612.72 \b; lambda = 16.84806 \nh = 15102.47 \b; lambda = 1527.095 \nh = 15480.03 \b; lambda = 771.9715 \nh = 15668.81 \b; lambda = 394.4098 \nh = 15763.2 \b; lambda = 205.6289 \nh = 15810.4 \b; lambda = 111.2385 \nh = 15833.99 \b; lambda = 64.04328 \nh = 15845.79 \b; lambda = 40.44567 \nh = 15851.69 \b; lambda = 28.64687 \nh = 15863.49 \b; lambda = 5.049258 \nh = 15854.64 \b; lambda = 22.74746 \nh = 15860.54 \b; lambda = 10.94866 \nh = 15859.07 \b; lambda = 13.89836 \nh = 14348.82 \b; lambda = 13.89836 \nh = 13216.87 \b; lambda = 12.42351 \nh = 12460.27 \b; lambda = 15.37321 \nh = 10760.88 \b; lambda = 16.11064 \nh = 8875.282 \b; lambda = 11.68608 \nh = 10432.08 \b; lambda = 12.97658 \nh = 7976.084 \b; lambda = 16.66371 \nh = 9286.281 \b; lambda = 15.60366 \nh = 9615.08 \b; lambda = 18.73771 \nh = 9206.581 \b; lambda = 21.61828 \nh = 8140.483 \b; lambda = 18.23073 \nh = 8795.582 \b; lambda = 17.70071 \nh = 9124.381 \b; lambda = 20.83477 \nh = 9164.856 \b; lambda = 19.52699 \nh = 8345.358 \b; lambda = 18.48998 \nh = 9297.65 \b; lambda = 18.67578 \nh = 8928.375 \b; lambda = 16.8495 \nh = 9105.736 \b; lambda = 18.85762 \nDone.\n\n\n         h     lambda \n9105.73611   18.85762 \n\n\nNow, the STKDE will be derived by using\n\nkde_yday &lt;- spattemp.density(\n  fire_yday_owin,\n  h = 9000,\n  lambda = 19)\nsummary(kde_yday)\n\nSpatiotemporal Kernel Density Estimate\n\nBandwidths\n  h = 9000 (spatial)\n  lambda = 19 (temporal)\n\nNo. of observations\n  741 \n\nSpatial bound\n  Type: polygonal\n  2D enclosure: [512066.8, 705559.4] x [9655398, 9834006]\n\nTemporal bound\n  [10, 352]\n\nEvaluation\n  128 x 128 x 343 trivariate lattice\n  Density range: [2.001642e-19, 2.445724e-12]\n\n\n\nplot(kde_yday)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class Ex04-stpp.html#spatio-temporal-point-patterns-analysis-stpp-methods",
    "href": "In-class_Ex/In-class_Ex04/In-class Ex04-stpp.html#spatio-temporal-point-patterns-analysis-stpp-methods",
    "title": "Spatio-Temporal Point Patterns Analysis",
    "section": "Spatio-temporal Point Patterns Analysis: stpp methods",
    "text": "Spatio-temporal Point Patterns Analysis: stpp methods\nIn this section, you will gain hands-on experience on using functions of stpp package to perform spatio-temporal point patterns analysis.\n\nStudents are encouraged to read stpp: An R Package for Plotting, Simulating and Analyzing Spatio-Temporal Point Patterns to learn more about the package.\n\n\nPreparing spatio-temporal point process object of stpp\nStep 1: Extracting forest fire coordinates from the fire point events\n\ncoords &lt;- st_coordinates(fire_sf)\n\nStep 2: Creating a data frame by combining the x- and y-coordinates and temporal event. Note that the temporal event must be in integer.\n\nfire_df &lt;- data.frame(\n  x = coords[, 1],  \n  y = coords[, 2],\n  t = fire_sf$`DayofYear`)\n\nStep 3: Creating stpp spatio-temporal object\nIn the code chunk below, as.3dpoint() of stpp package is used to create stpp spatio-temporal object class.\n\nfire_stpp &lt;- as.3dpoints(fire_df)\n\nUse the code chunk below to confirm that the output is in stpp spatio-temporal object class.\n\nclass(fire_stpp)\n\n[1] \"stpp\"\n\n\nNext we can visual fire_stpp by using the code chunk below.\n\nplot(fire_stpp)\n\n\n\n\n\n\n\n\n\n\nComputing spatio-temporal k-function\nIn the code chunk below, STIKhat() of stpp package is used to compute space-time inhomogeneous K-function.\n\nkbb_stik &lt;- STIKhat(fire_stpp)\n\nNext, plotK() is used to visualise the output space-time inhomogeneous K-function.\n\nplotK(kbb_stik)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03-SPAA.html#issue-1-installing-maptools",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03-SPAA.html#issue-1-installing-maptools",
    "title": "In-class Exercise 3: Spatial Point Patterns Analysis: spatstat methods",
    "section": "Issue 1: Installing maptools",
    "text": "Issue 1: Installing maptools\nmaptools is retired and binary is removed from CRAN. However, we can download from Posit Public Package Manager snapshots by using the code chunk below.\n\n\ninstall.packages(\"maptools\", \n                 repos = \"https://packagemanager.posit.co/cran/2023-10-13\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03-SPAA.html#issue-1-installing-maptools-1",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03-SPAA.html#issue-1-installing-maptools-1",
    "title": "In-class Exercise 3: Spatial Point Patterns Analysis: spatstat methods",
    "section": "Issue 1: Installing maptools",
    "text": "Issue 1: Installing maptools\n\n\nAfter the installation is completed, it is important to edit the code chunk as shown below in order to avoid maptools being download and install repetitively every time the Quarto document been rendered."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03-SPAA.html#issue-2-creating-coastal-outline",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03-SPAA.html#issue-2-creating-coastal-outline",
    "title": "In-class Exercise 3: Spatial Point Patterns Analysis: spatstat methods",
    "section": "Issue 2: Creating coastal outline",
    "text": "Issue 2: Creating coastal outline\nIn sf package, there are two functions allow us to combine multiple simple features into one simple features. They are st_combine() and st_union().\nst_combine() returns a single, combined geometry, with no resolved boundaries; returned geometries may well be invalid.\nIf y is missing, st_union(x) returns a single geometry with resolved boundaries, else the geometries for all unioned pairs of x[i] and y[j]."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03-SPAA.html#introducing-spatstat-package",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03-SPAA.html#introducing-spatstat-package",
    "title": "In-class Exercise 3: Spatial Point Patterns Analysis: spatstat methods",
    "section": "Introducing spatstat package",
    "text": "Introducing spatstat package\nspatstat R package is a comprehensive open-source toolbox for analysing Spatial Point Patterns. Focused mainly on two-dimensional point patterns, including multitype or marked points, in any spatial region."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03-SPAA.html#spatstat",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03-SPAA.html#spatstat",
    "title": "In-class Exercise 3: Spatial Point Patterns Analysis: spatstat methods",
    "section": "spatstat",
    "text": "spatstat\nspatstat sub-packages\n\n\nThe spatstat package now contains only documentation and introductory material. It provides beginner’s introductions, vignettes, interactive demonstration scripts, and a few help files summarising the package.\nThe spatstat.data package now contains all the datasets for spatstat.\nThe spatstat.utils package contains basic utility functions for spatstat.\nThe spatstat.univar package contains functions for estimating and manipulating probability distributions of one-dimensional random variables.\nThe spatstat.sparse package contains functions for manipulating sparse arrays and performing linear algebra.\nThe spatstat.geom package contains definitions of spatial objects (such as point patterns, windows and pixel images) and code which performs geometrical operations.\nThe spatstat.random package contains functions for random generation of spatial patterns and random simulation of models.\nThe spatstat.explore package contains the code for exploratory data analysis and nonparametric analysis of spatial data.\nThe spatstat.model package contains the code for model-fitting, model diagnostics, and formal inference.\nThe spatstat.linnet package defines spatial data on a linear network, and performs geometrical operations and statistical analysis on such data."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03-SPAA.html#creating-ppp-objects-from-sf-data.frame",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03-SPAA.html#creating-ppp-objects-from-sf-data.frame",
    "title": "In-class Exercise 3: Spatial Point Patterns Analysis: spatstat methods",
    "section": "Creating ppp objects from sf data.frame",
    "text": "Creating ppp objects from sf data.frame\nInstead of using the two steps approaches discussed in Hands-on Exercise 3 to create the ppp objects, in this section you will learn how to work with sf data.frame.\n\n\nIn the code chunk below, as.ppp() of spatstat.geom package is used to derive an ppp object layer directly from a sf tibble data.frame.\n\n\nchildcare_ppp &lt;- as.ppp(childcare_sf)\nplot(childcare_ppp)\n\n\n\n\n\n\n\n\n\n\nNext, summary() can be used to reveal the properties of the newly created ppp objects.\n\n\nsummary(childcare_ppp)\n\nMarked planar point pattern:  1925 points\nAverage intensity 2.417323e-06 points per square unit\n\nCoordinates are given to 11 decimal places\n\nmarks are of type 'character'\nSummary:\n   Length     Class      Mode \n     1925 character character \n\nWindow: rectangle = [11810.03, 45404.24] x [25596.33, 49300.88] units\n                    (33590 x 23700 units)\nWindow area = 796335000 square units"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03-SPAA.html#creating-owin-object-from-sf-data.frame",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03-SPAA.html#creating-owin-object-from-sf-data.frame",
    "title": "In-class Exercise 3: Spatial Point Patterns Analysis: spatstat methods",
    "section": "Creating owin object from sf data.frame",
    "text": "Creating owin object from sf data.frame\n\n\nIn the code chunk as.owin() of spatstat.geom is used to create an owin object class from polygon sf tibble data.frame.\n\n\nsg_owin &lt;- as.owin(sg_sf)\nplot(sg_owin)\n\n\n\n\n\n\n\n\n\n\nNext, summary() function is used to display the summary information of the owin object class.\n\n\nsummary(sg_owin)\n\nWindow: polygonal boundary\n80 separate polygons (35 holes)\n                  vertices         area relative.area\npolygon 1            14650  6.97996e+08      8.93e-01\npolygon 2 (hole)         3 -2.21090e+00     -2.83e-09\npolygon 3              285  1.61128e+06      2.06e-03\npolygon 4 (hole)         3 -2.05920e-03     -2.63e-12\npolygon 5 (hole)         3 -8.83647e-03     -1.13e-11\npolygon 6              668  5.40368e+07      6.91e-02\npolygon 7               44  2.26577e+03      2.90e-06\npolygon 8               27  1.50315e+04      1.92e-05\npolygon 9              711  1.28815e+07      1.65e-02\npolygon 10 (hole)       36 -4.01660e+04     -5.14e-05\npolygon 11 (hole)      317 -5.11280e+04     -6.54e-05\npolygon 12 (hole)        3 -3.41405e-01     -4.37e-10\npolygon 13 (hole)        3 -2.89050e-05     -3.70e-14\npolygon 14              77  3.29939e+05      4.22e-04\npolygon 15              30  2.80002e+04      3.58e-05\npolygon 16 (hole)        3 -2.83151e-01     -3.62e-10\npolygon 17              71  8.18750e+03      1.05e-05\npolygon 18 (hole)        3 -1.68316e-04     -2.15e-13\npolygon 19 (hole)       36 -7.79904e+03     -9.97e-06\npolygon 20 (hole)        4 -2.05611e-02     -2.63e-11\npolygon 21 (hole)        3 -2.18000e-06     -2.79e-15\npolygon 22 (hole)        3 -3.65501e-03     -4.67e-12\npolygon 23 (hole)        3 -4.95057e-02     -6.33e-11\npolygon 24 (hole)        3 -3.99521e-02     -5.11e-11\npolygon 25 (hole)        3 -6.62377e-01     -8.47e-10\npolygon 26 (hole)        3 -2.09065e-03     -2.67e-12\npolygon 27              91  1.49663e+04      1.91e-05\npolygon 28 (hole)       26 -1.25665e+03     -1.61e-06\npolygon 29 (hole)      349 -1.21433e+03     -1.55e-06\npolygon 30 (hole)       20 -4.39069e+00     -5.62e-09\npolygon 31 (hole)       48 -1.38338e+02     -1.77e-07\npolygon 32 (hole)       28 -1.99862e+01     -2.56e-08\npolygon 33              40  1.38607e+04      1.77e-05\npolygon 34 (hole)       40 -6.00381e+03     -7.68e-06\npolygon 35 (hole)        7 -1.40545e-01     -1.80e-10\npolygon 36 (hole)       12 -8.36709e+01     -1.07e-07\npolygon 37              45  2.51218e+03      3.21e-06\npolygon 38             142  3.22293e+03      4.12e-06\npolygon 39             148  3.10395e+03      3.97e-06\npolygon 40              75  1.73526e+04      2.22e-05\npolygon 41              83  5.28920e+03      6.76e-06\npolygon 42             211  4.70521e+05      6.02e-04\npolygon 43             106  3.04104e+03      3.89e-06\npolygon 44             266  1.50631e+06      1.93e-03\npolygon 45              71  5.63061e+03      7.20e-06\npolygon 46              10  1.99717e+02      2.55e-07\npolygon 47             478  2.06120e+06      2.64e-03\npolygon 48             155  2.67502e+05      3.42e-04\npolygon 49            1027  1.27782e+06      1.63e-03\npolygon 50 (hole)        3 -1.16959e-03     -1.50e-12\npolygon 51              65  8.42861e+04      1.08e-04\npolygon 52              47  3.82087e+04      4.89e-05\npolygon 53               6  4.50259e+02      5.76e-07\npolygon 54             132  9.53357e+04      1.22e-04\npolygon 55 (hole)        3 -3.23310e-04     -4.13e-13\npolygon 56               4  2.69313e+02      3.44e-07\npolygon 57 (hole)        3 -1.46474e-03     -1.87e-12\npolygon 58            1045  4.44510e+06      5.68e-03\npolygon 59              22  6.74651e+03      8.63e-06\npolygon 60              64  3.43149e+04      4.39e-05\npolygon 61 (hole)        3 -1.98390e-03     -2.54e-12\npolygon 62 (hole)        4 -1.13774e-02     -1.46e-11\npolygon 63              14  5.86546e+03      7.50e-06\npolygon 64              95  5.96187e+04      7.62e-05\npolygon 65 (hole)        4 -1.86410e-02     -2.38e-11\npolygon 66 (hole)        3 -5.12482e-03     -6.55e-12\npolygon 67 (hole)        3 -1.96410e-03     -2.51e-12\npolygon 68 (hole)        3 -5.55856e-03     -7.11e-12\npolygon 69             234  2.08755e+06      2.67e-03\npolygon 70              10  4.90942e+02      6.28e-07\npolygon 71             234  4.72886e+05      6.05e-04\npolygon 72 (hole)       13 -3.91907e+02     -5.01e-07\npolygon 73              15  4.03300e+04      5.16e-05\npolygon 74             227  1.10308e+06      1.41e-03\npolygon 75              10  6.60195e+03      8.44e-06\npolygon 76              19  3.09221e+04      3.95e-05\npolygon 77             145  9.61782e+05      1.23e-03\npolygon 78              30  4.28933e+03      5.49e-06\npolygon 79              37  1.29481e+04      1.66e-05\npolygon 80               4  9.47108e+01      1.21e-07\nenclosing rectangle: [2667.54, 56396.44] x [15748.72, 50256.33] units\n                     (53730 x 34510 units)\nWindow area = 781945000 square units\nFraction of frame area: 0.422"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03-SPAA.html#combining-point-events-object-and-owin-object",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03-SPAA.html#combining-point-events-object-and-owin-object",
    "title": "In-class Exercise 3: Spatial Point Patterns Analysis: spatstat methods",
    "section": "Combining point events object and owin object",
    "text": "Combining point events object and owin object\n\nThe taskThe codeThe output\n\n\nUsing the step you learned from Hands-on Exercise 3, create an ppp object by combining childcare_ppp and sg_owin.\n\n\n\n\nchildcareSG_ppp = childcare_ppp[sg_owin]\n\n\n\n\nThe output object combined both the point and polygon feature in one ppp object class as shown below.\n\n\nplot(childcareSG_ppp)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03-SPAA.html#kernel-density-estimation-of-spatial-point-event",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03-SPAA.html#kernel-density-estimation-of-spatial-point-event",
    "title": "In-class Exercise 3: Spatial Point Patterns Analysis: spatstat methods",
    "section": "Kernel Density Estimation of Spatial Point Event",
    "text": "Kernel Density Estimation of Spatial Point Event\nThe code chunk below re-scale the unit of measurement from metre to kilometre before performing KDE.\n\n\nchildcareSG_ppp.km &lt;- rescale.ppp(childcareSG_ppp, \n                                  1000, \n                                  \"km\")\n\nkde_childcareSG_adaptive &lt;- adaptive.density(\n  childcareSG_ppp.km, \n  method=\"kernel\")\nplot(kde_childcareSG_adaptive)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03-SPAA.html#kernel-density-estimation",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03-SPAA.html#kernel-density-estimation",
    "title": "In-class Exercise 3: Spatial Point Patterns Analysis: spatstat methods",
    "section": "Kernel Density Estimation",
    "text": "Kernel Density Estimation\nCode chunk shown two different ways to convert KDE output into grid object\n\nmaptools methodspatstat.geom method\n\n\n\n\npar(bg = '#E4D5C9')\n\ngridded_kde_childcareSG_ad &lt;- maptools::as.SpatialGridDataFrame.im(\n  kde_childcareSG_adaptive)\nspplot(gridded_kde_childcareSG_ad)\n\n\n\n\n\n\ngridded_kde_childcareSG_ad &lt;- as(\n  kde_childcareSG_adaptive,\n  \"SpatialGridDataFrame\")\nspplot(gridded_kde_childcareSG_ad)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03-SPAA.html#kernel-density-estimation-1",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03-SPAA.html#kernel-density-estimation-1",
    "title": "In-class Exercise 3: Spatial Point Patterns Analysis: spatstat methods",
    "section": "Kernel Density Estimation",
    "text": "Kernel Density Estimation\nVisualising KDE using tmap\nThe code chunk below is used to plot the output raster by using tmap functions.\n\n\ntm_shape(kde_childcareSG_ad_raster) + \n  tm_raster(palette = \"viridis\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\"), \n            frame = FALSE,\n            bg.color = \"#E4D5C9\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03-SPAA.html#extracting-study-area-using-sf-objects",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03-SPAA.html#extracting-study-area-using-sf-objects",
    "title": "In-class Exercise 3: Spatial Point Patterns Analysis: spatstat methods",
    "section": "Extracting study area using sf objects",
    "text": "Extracting study area using sf objects\n\nThe taskThe code\n\n\nExtract and create an ppp object showing child care services and within Punggol Planning Area\n\n\nOn the other hand, filter() of dplyr package should be used to extract the target planning areas as shown in the code chunk below.\n\n\npg_owin &lt;- mpsz_sf %&gt;%\n  filter(PLN_AREA_N == \"PUNGGOL\") %&gt;%\n  as.owin()\n\nchildcare_pg = childcare_ppp[pg_owin]\n\nplot(childcare_pg)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03-SPAA.html#geospatial-analytics-for-social-good-myanmar-arm-conflict-case-study",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03-SPAA.html#geospatial-analytics-for-social-good-myanmar-arm-conflict-case-study",
    "title": "In-class Exercise 3: Spatial Point Patterns Analysis: spatstat methods",
    "section": "Geospatial Analytics for Social Good: Myanmar Arm Conflict Case Study",
    "text": "Geospatial Analytics for Social Good: Myanmar Arm Conflict Case Study\n\nBackground\nMyanmar’s Troubled History: Coups, Military Rule, and Ethnic Conflict\nMyanmar conflict\nMyanmar civil war (2021–present)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03-SPAA.html#the-data",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03-SPAA.html#the-data",
    "title": "In-class Exercise 3: Spatial Point Patterns Analysis: spatstat methods",
    "section": "The Data",
    "text": "The Data\nArmed Conflict Location & Event Data (ACLED)\n\n\nAn independent, impartial, international non-profit organization collecting data on violent conflict and protest in all countries and territories in the world.\n\nGIS Data\n\nMyanmar Information Management Unit, MIMU"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03-SPAA.html#importing-acled-data",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03-SPAA.html#importing-acled-data",
    "title": "In-class Exercise 3: Spatial Point Patterns Analysis: spatstat methods",
    "section": "Importing ACLED Data",
    "text": "Importing ACLED Data\n\nThe taskThe code\n\n\nUsing the steps you learned in previous lessons, import the ACLED data into R environment as an sf tibble data.frame.\n\n\n\n\nacled_sf &lt;- read_csv(\"data/ACLED_Myanmar.csv\") %&gt;%\n  st_as_sf(coords = c(\n    \"longitude\", \"latitude\"),\n    crs=4326) %&gt;%\n  st_transform(crs = 32647) %&gt;%\n  mutate(event_date = dmy(event_date))"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03-SPAA.html#visualising-acled-data",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03-SPAA.html#visualising-acled-data",
    "title": "In-class Exercise 3: Spatial Point Patterns Analysis: spatstat methods",
    "section": "Visualising ACLED Data",
    "text": "Visualising ACLED Data\n\nThe taskThe code\n\n\nUsing the steps you learned in previous lessons, import the ACLED data into R environment as an sf tibble data.frame.\n\n\n\n\ntmap_mode(\"plot\")\nacled_sf %&gt;%\n  filter(year == 2023 | \n           event_type == \"Political violence\") %&gt;%\n  tm_shape()+\n  tm_dots()\n\n\n\n\n\n\n\ntmap_mode(\"plot\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/data/rawdata/Kepulauan_Bangka_Belitung.html",
    "href": "In-class_Ex/In-class_Ex04/data/rawdata/Kepulauan_Bangka_Belitung.html",
    "title": "IS415 AY2024-25T1",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     \n\n\n       GEOGCRS[“WGS 84”,ENSEMBLE[“World Geodetic System 1984 ensemble”,MEMBER[“World Geodetic System 1984 (Transit)”],MEMBER[“World Geodetic System 1984 (G730)”],MEMBER[“World Geodetic System 1984 (G873)”],MEMBER[“World Geodetic System 1984 (G1150)”],MEMBER[“World Geodetic System 1984 (G1674)”],MEMBER[“World Geodetic System 1984 (G1762)”],MEMBER[“World Geodetic System 1984 (G2139)”],ELLIPSOID[“WGS 84”,6378137,298.257223563,LENGTHUNIT[“metre”,1]],ENSEMBLEACCURACY[2.0]],PRIMEM[“Greenwich”,0,ANGLEUNIT[“degree”,0.0174532925199433]],CS[ellipsoidal,2],AXIS[“geodetic latitude (Lat)”,north,ORDER[1],ANGLEUNIT[“degree”,0.0174532925199433]],AXIS[“geodetic longitude (Lon)”,east,ORDER[2],ANGLEUNIT[“degree”,0.0174532925199433]],USAGE[SCOPE[“Horizontal component of 3D system.”],AREA[“World.”],BBOX[-90,-180,90,180]],ID[“EPSG”,4326]] +proj=longlat +datum=WGS84 +no_defs 3452 4326 EPSG:4326 WGS 84 longlat EPSG:7030 true"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04-stpp.html",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04-stpp.html",
    "title": "Spatio-Temporal Point Patterns Analysis",
    "section": "",
    "text": "A spatio-temporal point process (also called space-time or spatial-temporal point process) is a random collection of points, where each point represents the time and location of an event. Examples of events include incidence of disease, sightings or births of a species, or the occurrences of fires, earthquakes, lightning strikes, tsunamis, or volcanic eruptions.\nThe analysis of spatio-temporal point patterns is becoming increasingly necessary, given the rapid emergence of geographically and temporally indexed data in a wide range of fields. Several spatio-temporal point patterns analysis methods have been introduced and implemented in R in the last ten years. This chapter shows how various R packages can be combined to run a set of spatio-temporal point pattern analyses in a guided and intuitive way. A real world forest fire events in Kepulauan Bangka Belitung, Indonesia from 1st January 2023 to 31st December 2023 is used to illustrate the methods, procedures and interpretations."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04-stpp.html#overview",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04-stpp.html#overview",
    "title": "Spatio-Temporal Point Patterns Analysis",
    "section": "",
    "text": "A spatio-temporal point process (also called space-time or spatial-temporal point process) is a random collection of points, where each point represents the time and location of an event. Examples of events include incidence of disease, sightings or births of a species, or the occurrences of fires, earthquakes, lightning strikes, tsunamis, or volcanic eruptions.\nThe analysis of spatio-temporal point patterns is becoming increasingly necessary, given the rapid emergence of geographically and temporally indexed data in a wide range of fields. Several spatio-temporal point patterns analysis methods have been introduced and implemented in R in the last ten years. This chapter shows how various R packages can be combined to run a set of spatio-temporal point pattern analyses in a guided and intuitive way. A real world forest fire events in Kepulauan Bangka Belitung, Indonesia from 1st January 2023 to 31st December 2023 is used to illustrate the methods, procedures and interpretations."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04-stpp.html#learning-outcome",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04-stpp.html#learning-outcome",
    "title": "Spatio-Temporal Point Patterns Analysis",
    "section": "Learning Outcome",
    "text": "Learning Outcome\n\nThe research questions\nThe specific questions we would like to answer are:\n\nare the locations of forest fire in Kepulauan Bangka Belitung spatial and spatio-temporally independent?\nif the answer is NO, where and when the observed forest fire locations tend to cluster?"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04-stpp.html#the-data",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04-stpp.html#the-data",
    "title": "Spatio-Temporal Point Patterns Analysis",
    "section": "The data",
    "text": "The data\nFor the purpose of this exercise, two data sets will be used, they are:\n\nforestfires, a csv file provides locations of forest fire detected from the Moderate Resolution Imaging Spectroradiometer (MODIS) sensor data. The data are downloaded from Fire Information for Resource Management System. For the purpose of this exercise, only forest fires within Kepulauan Bangka Belitung will be used.\n\nKepulauan_Bangka_Belitung, an ESRI shapefile showing the sub-district (i.e. kelurahan) boundary of Kepulauan Bangka Belitung. The data set was downloaded from Indonesia Geospatial portal. The original data covers the whole Indonesia. For the purpose of this exercise, only sub-districts within Kepulauan Bangka Belitung are extracted."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04-stpp.html#installing-and-loading-the-r-packages",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04-stpp.html#installing-and-loading-the-r-packages",
    "title": "Spatio-Temporal Point Patterns Analysis",
    "section": "Installing and Loading the R packages",
    "text": "Installing and Loading the R packages\nFor the purpose of this study, six R packages will be used. They are:\n\nsf provides functions for importing processing and wrangling geospatial data,,\nraster for handling raster data in R,\nspatstat for performing Spatial Point Patterns Analysis such as kcross, Lcross, etc.,\nsparr provides functions to estimate fixed and adaptive kernel-smoothed spatial relative risk surfaces via the density-ratio method and perform subsequent inference. Fixed-bandwidth spatiotemporal density and relative risk estimation is also supported\ntmap provides functions to produce cartographic quality thematic maps, and\ntidyverse, a family of R packages that provide functions to perform common data science tasks including and not limited to data import, data transformation, data wrangling and data visualisation.\n\n\nDIYThe solution\n\n\nUsing the steps you learned from previous chapter, write a code chunk to load the packages above onto R environment.\n\n\n\npacman::p_load(sf, raster, spatstat, sparr, tmap, stopp, tidyverse, stpp)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04-stpp.html#importing-and-preparing-study-area",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04-stpp.html#importing-and-preparing-study-area",
    "title": "Spatio-Temporal Point Patterns Analysis",
    "section": "Importing and Preparing Study Area",
    "text": "Importing and Preparing Study Area\n\nImporting study area\nCode chunk below is used import study area (i.e. Kepulauan Bangka Belitung) into R environment.\n\nkbb &lt;- st_read(dsn=\"data/rawdata\",\n               layer = \"Kepulauan_Bangka_Belitung\") %&gt;%\n  st_transform(crs = 32748)\n\nThe revised code chunk.\n\nkbb_sf &lt;- st_read(dsn=\"data/rawdata\",\n               layer = \"Kepulauan_Bangka_Belitung\") %&gt;%\n  st_union() %&gt;%\n  st_zm(drop = TRUE, what = \"ZM\") %&gt;%\n  st_transform(crs = 32748)\n\nReading layer `Kepulauan_Bangka_Belitung' from data source \n  `D:\\tskam\\IS415_AY2024-25T1\\In-class_Ex\\In-class_Ex04\\data\\rawdata' \n  using driver `ESRI Shapefile'\nSimple feature collection with 298 features and 27 fields\nGeometry type: POLYGON\nDimension:     XYZ\nBounding box:  xmin: 105.1085 ymin: -3.116593 xmax: 106.8488 ymax: -1.501603\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\n\n\nConverting OWIN\nNext, as.owin() is used to convert kbb into an owin object.\n\nkbb_owin &lt;- as.owin(kbb_sf)\nkbb_owin\n\nwindow: polygonal boundary\nenclosing rectangle: [512066.8, 705559.4] x [9655398, 9834006] units\n\n\nNext, class() is used to confirm if the output is indeed an owin object.\n\nclass(kbb_owin)\n\n[1] \"owin\"\n\n\n\nwrite_rds(kbb_owin, \"data/rds/kbb_owin\")\n\n\nkbb_owin &lt;- read_rds(\"data/rds/kbb_owin\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04-stpp.html#importing-and-preparing-forest-fire-data",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04-stpp.html#importing-and-preparing-forest-fire-data",
    "title": "Spatio-Temporal Point Patterns Analysis",
    "section": "Importing and Preparing Forest Fire data",
    "text": "Importing and Preparing Forest Fire data\nNext, we will import the forest fire data set (i.e. forestfires.csv) into R environment.\n\nfire_sf &lt;- read_csv(\"data/rawdata/forestfires.csv\") %&gt;%\n  st_as_sf(coords = c(\"longitude\", \"latitude\"),\n                       crs = 4326) %&gt;%\n  st_transform(crs = 32748)\n\nBecause ppp object only accept numerical or character as mark. The code chunk below is used to convert data type of acq_date to numeric.\n\nfire_sf &lt;- fire_sf %&gt;% \n  mutate(DayofYear = yday(acq_date)) %&gt;%\n  mutate(Month_num = month(acq_date)) %&gt;%\n  mutate(Month_fac = month(acq_date, \n                           label = TRUE, \n                           abbr = FALSE))"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04-stpp.html#visualising-the-fire-points",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04-stpp.html#visualising-the-fire-points",
    "title": "Spatio-Temporal Point Patterns Analysis",
    "section": "Visualising the Fire Points",
    "text": "Visualising the Fire Points\n\nOverall plot\n\nDIYThe code\n\n\nUsing the steps you learned in Hands-on Exercise 2, prepare a point symbol map showing the distribution of fire points. The map should look similar to the figure below.\n\n\n\n\n\n\n\n\n\n\n\n\ntm_shape(kbb_sf)+\n  tm_polygons() +\ntm_shape(fire_sf) +\n  tm_dots()\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisuaising geographic distribution of forest fires by month\n\nDIYThe code\n\n\nUsing the steps you learned in Hands-on Exercise 2, prepare a point symbol map showing the monthly geographic distribution of forest fires in 2023. The map should look similar to the figure below.\n\n\n\n\n\n\n\n\n\n\n\n\ntm_shape(kbb_sf)+\n  tm_polygons() +\ntm_shape(fire_sf) +\n  tm_dots(size = 0.1) +\ntm_facets(by=\"Month_fac\", \n            free.coords=FALSE, \n            drop.units = TRUE)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04-stpp.html#computing-stkde-by-month",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04-stpp.html#computing-stkde-by-month",
    "title": "Spatio-Temporal Point Patterns Analysis",
    "section": "Computing STKDE by Month",
    "text": "Computing STKDE by Month\nIn this section, you will learn how to compute STKDE by using spattemp.density() of sparr package. Before using the function, it is highly recommended you read the function’s reference guide in detail in order to understand the input data requirements and the output object generated.\n\nExtracting forest fires by month\nThe code chunk below is used to remove the unwanted fields from fire_sf sf data.frame. This is because as.ppp() only need the mark field and geometry field from the input sf data.frame.\n\nfire_month &lt;- fire_sf %&gt;% \n  select(Month_num)\n\n\n\nCreating ppp\nThe code chunk below is used to derive a ppp object called fire_month from fire_month sf data.frame.\n\nfire_month_ppp &lt;- as.ppp(fire_month)\nfire_month_ppp\n\nMarked planar point pattern: 741 points\nmarks are numeric, of storage type  'double'\nwindow: rectangle = [521564.1, 695791] x [9658137, 9828767] units\n\n\nThe code chunk below is used to check the output is in the correct object class.\n\nsummary(fire_month_ppp)\n\nMarked planar point pattern:  741 points\nAverage intensity 2.49258e-08 points per square unit\n\nCoordinates are given to 10 decimal places\n\nmarks are numeric, of type 'double'\nSummary:\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   8.000   9.000   8.579  10.000  12.000 \n\nWindow: rectangle = [521564.1, 695791] x [9658137, 9828767] units\n                    (174200 x 170600 units)\nWindow area = 29728200000 square units\n\n\nNext, we will check if there are duplicated point events by using the code chunk below.\n\nany(duplicated(fire_month_ppp))\n\n[1] FALSE\n\n\n\n\nIncluding Owin object\nThe code chunk below is used to combine origin_am_ppp and am_owin objects into one.\n\nfire_month_owin &lt;- fire_month_ppp[kbb_owin]\nsummary(fire_month_owin)\n\nMarked planar point pattern:  741 points\nAverage intensity 6.424519e-08 points per square unit\n\nCoordinates are given to 10 decimal places\n\nmarks are numeric, of type 'double'\nSummary:\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   8.000   9.000   8.579  10.000  12.000 \n\nWindow: polygonal boundary\n2 separate polygons (no holes)\n           vertices        area relative.area\npolygon 1     47493 11533600000      1.00e+00\npolygon 2       256      306427      2.66e-05\nenclosing rectangle: [512066.8, 705559.4] x [9655398, 9834006] units\n                     (193500 x 178600 units)\nWindow area = 11533900000 square units\nFraction of frame area: 0.334\n\n\nAs a good practice, plot() is used to plot ff_owin so that we can examine the correctness of the output object.\n\nplot(fire_month_owin)\n\n\n\n\n\n\n\n\n\n\nComputing Spatio-temporal KDE\nNext, spattemp.density() of sparr package is used to compute the STKDE.\n\nst_kde &lt;- spattemp.density(fire_month_owin)\nsummary(st_kde)\n\nSpatiotemporal Kernel Density Estimate\n\nBandwidths\n  h = 15102.47 (spatial)\n  lambda = 0.0304 (temporal)\n\nNo. of observations\n  741 \n\nSpatial bound\n  Type: polygonal\n  2D enclosure: [512066.8, 705559.4] x [9655398, 9834006]\n\nTemporal bound\n  [1, 12]\n\nEvaluation\n  128 x 128 x 12 trivariate lattice\n  Density range: [1.233458e-27, 8.202976e-10]\n\n\n\n\nPlotting the spatio-temporal KDE object\nIn the code chunk below, plot() of R base is used to the KDE for between July 2023 - December 2023.\n\ntims &lt;- c(7,8,9,10,11,12)\npar(mfcol=c(2,3))\nfor(i in tims){ \n  plot(st_kde, i, \n       override.par=FALSE, \n       fix.range=TRUE, \n       main=paste(\"KDE at month\",i))\n}"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04-stpp.html#computing-stkde-by-day-of-year",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04-stpp.html#computing-stkde-by-day-of-year",
    "title": "Spatio-Temporal Point Patterns Analysis",
    "section": "Computing STKDE by Day of Year",
    "text": "Computing STKDE by Day of Year\nIn this section, you will learn how to computer the STKDE of forest fires by day of year.\n\nCreating ppp object\nIn the code chunk below, DayofYear field is included in the output ppp object.\n\nfire_yday_ppp &lt;- fire_sf %&gt;% \n  select(DayofYear) %&gt;%\n  as.ppp()\n\n\n\nIncluding Owin object\nNext, code chunk below is used to combine the ppp object and the owin object.\n\nfire_yday_owin &lt;- fire_yday_ppp[kbb_owin]\nsummary(fire_yday_owin)\n\nMarked planar point pattern:  741 points\nAverage intensity 6.424519e-08 points per square unit\n\nCoordinates are given to 10 decimal places\n\nmarks are numeric, of type 'double'\nSummary:\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   10.0   213.0   258.0   245.9   287.0   352.0 \n\nWindow: polygonal boundary\n2 separate polygons (no holes)\n           vertices        area relative.area\npolygon 1     47493 11533600000      1.00e+00\npolygon 2       256      306427      2.66e-05\nenclosing rectangle: [512066.8, 705559.4] x [9655398, 9834006] units\n                     (193500 x 178600 units)\nWindow area = 11533900000 square units\nFraction of frame area: 0.334\n\n\n\n\n\n\nkde_yday &lt;- spattemp.density(\n  fire_yday_owin)\nsummary(kde_yday)\n\nSpatiotemporal Kernel Density Estimate\n\nBandwidths\n  h = 15102.47 (spatial)\n  lambda = 6.3198 (temporal)\n\nNo. of observations\n  741 \n\nSpatial bound\n  Type: polygonal\n  2D enclosure: [512066.8, 705559.4] x [9655398, 9834006]\n\nTemporal bound\n  [10, 352]\n\nEvaluation\n  128 x 128 x 343 trivariate lattice\n  Density range: [3.959516e-27, 2.751287e-12]\n\n\n\nplot(kde_yday)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04-stpp.html#computing-stkde-by-day-of-year-improved-method",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04-stpp.html#computing-stkde-by-day-of-year-improved-method",
    "title": "Spatio-Temporal Point Patterns Analysis",
    "section": "Computing STKDE by Day of Year: Improved method",
    "text": "Computing STKDE by Day of Year: Improved method\nOne of the nice function provides in sparr package is BOOT.spattemp(). It support bandwidth selection for standalone spatiotemporal density/intensity based on bootstrap estimation of the MISE, providing an isotropic scalar spatial bandwidth and a scalar temporal bandwidth.\nCode chunk below uses BOOT.spattemp() to determine both the spatial bandwidth and the scalar temporal bandwidth.\n\nset.seed(1234)\nBOOT.spattemp(fire_yday_owin) \n\nInitialising...Done.\nOptimising...\nh = 15102.47 \b; lambda = 16.84806 \nh = 16612.72 \b; lambda = 16.84806 \nh = 15102.47 \b; lambda = 1527.095 \nh = 15480.03 \b; lambda = 771.9715 \nh = 15668.81 \b; lambda = 394.4098 \nh = 15763.2 \b; lambda = 205.6289 \nh = 15810.4 \b; lambda = 111.2385 \nh = 15833.99 \b; lambda = 64.04328 \nh = 15845.79 \b; lambda = 40.44567 \nh = 15851.69 \b; lambda = 28.64687 \nh = 15863.49 \b; lambda = 5.049258 \nh = 15854.64 \b; lambda = 22.74746 \nh = 15860.54 \b; lambda = 10.94866 \nh = 15859.07 \b; lambda = 13.89836 \nh = 14348.82 \b; lambda = 13.89836 \nh = 13216.87 \b; lambda = 12.42351 \nh = 12460.27 \b; lambda = 15.37321 \nh = 10760.88 \b; lambda = 16.11064 \nh = 8875.282 \b; lambda = 11.68608 \nh = 10432.08 \b; lambda = 12.97658 \nh = 7976.084 \b; lambda = 16.66371 \nh = 9286.281 \b; lambda = 15.60366 \nh = 9615.08 \b; lambda = 18.73771 \nh = 9206.581 \b; lambda = 21.61828 \nh = 8140.483 \b; lambda = 18.23073 \nh = 8795.582 \b; lambda = 17.70071 \nh = 9124.381 \b; lambda = 20.83477 \nh = 9164.856 \b; lambda = 19.52699 \nh = 8345.358 \b; lambda = 18.48998 \nh = 9297.65 \b; lambda = 18.67578 \nh = 8928.375 \b; lambda = 16.8495 \nh = 9105.736 \b; lambda = 18.85762 \nDone.\n\n\n         h     lambda \n9105.73611   18.85762 \n\n\nNow, the STKDE will be derived by using\n\nkde_yday &lt;- spattemp.density(\n  fire_yday_owin,\n  h = 9000,\n  lambda = 19)\nsummary(kde_yday)\n\nSpatiotemporal Kernel Density Estimate\n\nBandwidths\n  h = 9000 (spatial)\n  lambda = 19 (temporal)\n\nNo. of observations\n  741 \n\nSpatial bound\n  Type: polygonal\n  2D enclosure: [512066.8, 705559.4] x [9655398, 9834006]\n\nTemporal bound\n  [10, 352]\n\nEvaluation\n  128 x 128 x 343 trivariate lattice\n  Density range: [2.001642e-19, 2.445724e-12]\n\n\n\nplot(kde_yday)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04-stpp.html#spatio-temporal-point-patterns-analysis-stpp-methods",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04-stpp.html#spatio-temporal-point-patterns-analysis-stpp-methods",
    "title": "Spatio-Temporal Point Patterns Analysis",
    "section": "Spatio-temporal Point Patterns Analysis: stpp methods",
    "text": "Spatio-temporal Point Patterns Analysis: stpp methods\nIn this section, you will gain hands-on experience on using functions of stpp package to perform spatio-temporal point patterns analysis.\n\nStudents are encouraged to read stpp: An R Package for Plotting, Simulating and Analyzing Spatio-Temporal Point Patterns to learn more about the package.\n\n\nPreparing spatio-temporal point process object of stpp\nStep 1: Extracting forest fire coordinates from the fire point events\n\ncoords &lt;- st_coordinates(fire_sf)\n\nStep 2: Creating a data frame by combining the x- and y-coordinates and temporal event. Note that the temporal event must be in integer.\n\nfire_df &lt;- data.frame(\n  x = coords[, 1],  \n  y = coords[, 2],\n  t = fire_sf$`DayofYear`)\n\nStep 3: Creating stpp spatio-temporal object\nIn the code chunk below, as.3dpoint() of stpp package is used to create stpp spatio-temporal object class.\n\nfire_stpp &lt;- as.3dpoints(fire_df)\n\nUse the code chunk below to confirm that the output is in stpp spatio-temporal object class.\n\nclass(fire_stpp)\n\n[1] \"stpp\"\n\n\nNext we can visual fire_stpp by using the code chunk below.\n\nplot(fire_stpp)\n\n\n\n\n\n\n\n\n\n\nComputing spatio-temporal k-function\nIn the code chunk below, STIKhat() of stpp package is used to compute space-time inhomogeneous K-function.\n\nkbb_stik &lt;- STIKhat(fire_stpp)\n\nNext, plotK() is used to visualise the output space-time inhomogeneous K-function.\n\nplotK(kbb_stik)\n\n\n\n\nGuide to interpret the plot\n\nIf the contours show high values for small values of uu and vv, it suggests clustering at short spatial and temporal distances, meaning events occur close to each other in both space and time.\nIf the contours are flat or show low values, it indicates a more random distribution or a lack of significant clustering at those distances.\n\nThe spatial and temporal extent of clustering can be understood by observing how rapidly the contours increase or decrease as we move along the axes. In the plot above, we can see clustering at specific distances by observing the spacing and values of the contour lines."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#loading-the-package",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#loading-the-package",
    "title": "In-class Exercise 5: Geographically Weighted Statistics - gwModel methods",
    "section": "Loading the package",
    "text": "Loading the package\nIn this in-class exercise, sf, spdep, tmap, tidyverse, knitr and GWmodel will be used.\n\nDIYThe code\n\n\nUsing the step you leanred from previous hands-in, install and load the necessary R packages in R environment.\n\n\n\n\npacman::p_load(sf, spdep, tmap, tidyverse, knitr, GWmodel)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#preparing-the-data",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#preparing-the-data",
    "title": "In-class Exercise 5: Geographically Weighted Statistics - gwModel methods",
    "section": "Preparing the Data",
    "text": "Preparing the Data\nFor this in-class exercise, Hunan shapefile and Hunan_2012 data file will be used.\n\nDIYImporting Hunan shapefileImporting Hunan_2012 tableJoining Hunan and Hunan_2012\n\n\nUsing the steps you learned from previous hands-on, complete the following tasks:\n\nimport Hunan shapefile and parse it into a sf polygon feature object.\nimport Hunan_2012.csv file parse it into a tibble data.frame.\njoin Hunan and Hunan_2012 data.frames.\n\n\n\n\n\nhunan_sf &lt;- st_read(dsn = \"data/geospatial\", \n                 layer = \"Hunan\")\n\nReading layer `Hunan' from data source \n  `D:\\tskam\\IS415_AY2024-25T1\\In-class_Ex\\In-class_Ex05\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\n\n\n\n\n\nhunan2012 &lt;- read_csv(\"data/aspatial/Hunan_2012.csv\")\n\n\n\n\n\n\nhunan_sf &lt;- left_join(hunan_sf, hunan2012) %&gt;%\n  select(1:3, 7, 15, 16, 31, 32)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#mapping-gdppc",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#mapping-gdppc",
    "title": "In-class Exercise 5: Geographically Weighted Statistics - gwModel methods",
    "section": "Mapping GDPPC",
    "text": "Mapping GDPPC\n\nDIYThe code\n\n\nUsing the steps you learned from Hands-on Exercise 5, prepare a choropleth map showing the geographic distribution of GDPPC of Hunan Province.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbasemap &lt;- tm_shape(hunan_sf) +\n  tm_polygons() +\n  tm_text(\"NAME_3\", size=0.5)\n\ngdppc &lt;- qtm(hunan, \"GDPPC\")\ntmap_arrange(basemap, gdppc, asp=1, ncol=2)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#converting-to-spatialpolygondataframe",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#converting-to-spatialpolygondataframe",
    "title": "In-class Exercise 5: Geographically Weighted Statistics - gwModel methods",
    "section": "Converting to SpatialPolygonDataFrame",
    "text": "Converting to SpatialPolygonDataFrame\n\n\n\n\n\n\nNote\n\n\nGWmodel presently is built around the older sp and not sf formats for handling spatial data in R.\n\n\n\n\n\nhunan_sp &lt;- hunan_sf %&gt;%\n  as_Spatial()"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#geographically-weighted-summary-statistics-with-adaptive-bandwidth",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#geographically-weighted-summary-statistics-with-adaptive-bandwidth",
    "title": "In-class Exercise 5: Geographically Weighted Statistics - gwModel methods",
    "section": "Geographically Weighted Summary Statistics with adaptive bandwidth",
    "text": "Geographically Weighted Summary Statistics with adaptive bandwidth\nDetermine adaptive bandwidth\n\nCross-validationAIC\n\n\n\n\nbw_CV &lt;- bw.gwr(GDPPC ~ 1, \n             data = hunan_sp,\n             approach = \"CV\",\n             adaptive = TRUE, \n             kernel = \"bisquare\", \n             longlat = T)\n\nAdaptive bandwidth: 62 CV score: 15515442343 \nAdaptive bandwidth: 46 CV score: 14937956887 \nAdaptive bandwidth: 36 CV score: 14408561608 \nAdaptive bandwidth: 29 CV score: 14198527496 \nAdaptive bandwidth: 26 CV score: 13898800611 \nAdaptive bandwidth: 22 CV score: 13662299974 \nAdaptive bandwidth: 22 CV score: 13662299974 \n\nbw_CV\n\n[1] 22\n\n\n\n\n\n\n\nbw_AIC &lt;- bw.gwr(GDPPC ~ 1, \n             data = hunan_sp,\n             approach =\"AIC\",\n             adaptive = TRUE, \n             kernel = \"bisquare\", \n             longlat = T)\n\nAdaptive bandwidth (number of nearest neighbours): 62 AICc value: 1923.156 \nAdaptive bandwidth (number of nearest neighbours): 46 AICc value: 1920.469 \nAdaptive bandwidth (number of nearest neighbours): 36 AICc value: 1917.324 \nAdaptive bandwidth (number of nearest neighbours): 29 AICc value: 1916.661 \nAdaptive bandwidth (number of nearest neighbours): 26 AICc value: 1914.897 \nAdaptive bandwidth (number of nearest neighbours): 22 AICc value: 1914.045 \nAdaptive bandwidth (number of nearest neighbours): 22 AICc value: 1914.045 \n\nbw_AIC\n\n[1] 22"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#geographically-weighted-summary-statistics-with-adaptive-bandwidth-1",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#geographically-weighted-summary-statistics-with-adaptive-bandwidth-1",
    "title": "In-class Exercise 5: Geographically Weighted Statistics - gwModel methods",
    "section": "Geographically Weighted Summary Statistics with adaptive bandwidth",
    "text": "Geographically Weighted Summary Statistics with adaptive bandwidth\nComputing geographically wieghted summary statistics\n\n\ngwstat &lt;- gwss(data = hunan_sp,\n               vars = \"GDPPC\",\n               bw = bw_AIC,\n               kernel = \"bisquare\",\n               adaptive = TRUE,\n               longlat = T)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#geographically-weighted-summary-statistics-with-adaptive-bandwidth-2",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#geographically-weighted-summary-statistics-with-adaptive-bandwidth-2",
    "title": "In-class Exercise 5: Geographically Weighted Statistics - gwModel methods",
    "section": "Geographically Weighted Summary Statistics with adaptive bandwidth",
    "text": "Geographically Weighted Summary Statistics with adaptive bandwidth\nPreparing the output data\nCode chunk below is used to extract SDF data table from gwss object output from gwss(). It will be converted into data.frame by using as.data.frame().\n\n\ngwstat_df &lt;- as.data.frame(gwstat$SDF)\n\n\nNext, cbind() is used to append the newly derived data.frame onto hunan_sf sf data.frame.\n\n\nhunan_gstat &lt;- cbind(hunan_sf, gwstat_df)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#geographically-weighted-summary-statistics-with-adaptive-bandwidth-3",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#geographically-weighted-summary-statistics-with-adaptive-bandwidth-3",
    "title": "In-class Exercise 5: Geographically Weighted Statistics - gwModel methods",
    "section": "Geographically Weighted Summary Statistics with adaptive bandwidth",
    "text": "Geographically Weighted Summary Statistics with adaptive bandwidth\nVisualising geographically weighted summary statistics\n\nThe Geographically Weighted MeanThe code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntm_shape(hunan_gstat) +\n  tm_fill(\"GDPPC_LM\",\n          n = 5,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Distribution of geographically wieghted mean\",\n            main.title.position = \"center\",\n            main.title.size = 2.0,\n            legend.text.size = 1.2,\n            legend.height = 1.50, \n            legend.width = 1.50,\n            frame = TRUE)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#geographically-weighted-summary-statistics-with-fixed",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#geographically-weighted-summary-statistics-with-fixed",
    "title": "In-class Exercise 5: Geographically Weighted Statistics - gwModel methods",
    "section": "Geographically Weighted Summary Statistics with fixed",
    "text": "Geographically Weighted Summary Statistics with fixed\nDetermine fixed bandwidth\n\nCross-validationAIC\n\n\n\n\nbw_CV &lt;- bw.gwr(GDPPC ~ 1, \n             data = hunan_sp,\n             approach = \"CV\",\n             adaptive = FALSE, \n             kernel = \"bisquare\", \n             longlat = T)\n\nFixed bandwidth: 357.4897 CV score: 16265191728 \nFixed bandwidth: 220.985 CV score: 14954930931 \nFixed bandwidth: 136.6204 CV score: 14134185837 \nFixed bandwidth: 84.48025 CV score: 13693362460 \nFixed bandwidth: 52.25585 CV score: Inf \nFixed bandwidth: 104.396 CV score: 13891052305 \nFixed bandwidth: 72.17162 CV score: 13577893677 \nFixed bandwidth: 64.56447 CV score: 14681160609 \nFixed bandwidth: 76.8731 CV score: 13444716890 \nFixed bandwidth: 79.77877 CV score: 13503296834 \nFixed bandwidth: 75.07729 CV score: 13452450771 \nFixed bandwidth: 77.98296 CV score: 13457916138 \nFixed bandwidth: 76.18716 CV score: 13442911302 \nFixed bandwidth: 75.76323 CV score: 13444600639 \nFixed bandwidth: 76.44916 CV score: 13442994078 \nFixed bandwidth: 76.02523 CV score: 13443285248 \nFixed bandwidth: 76.28724 CV score: 13442844774 \nFixed bandwidth: 76.34909 CV score: 13442864995 \nFixed bandwidth: 76.24901 CV score: 13442855596 \nFixed bandwidth: 76.31086 CV score: 13442847019 \nFixed bandwidth: 76.27264 CV score: 13442846793 \nFixed bandwidth: 76.29626 CV score: 13442844829 \nFixed bandwidth: 76.28166 CV score: 13442845238 \nFixed bandwidth: 76.29068 CV score: 13442844678 \nFixed bandwidth: 76.29281 CV score: 13442844691 \nFixed bandwidth: 76.28937 CV score: 13442844698 \nFixed bandwidth: 76.2915 CV score: 13442844676 \nFixed bandwidth: 76.292 CV score: 13442844679 \nFixed bandwidth: 76.29119 CV score: 13442844676 \nFixed bandwidth: 76.29099 CV score: 13442844676 \nFixed bandwidth: 76.29131 CV score: 13442844676 \nFixed bandwidth: 76.29138 CV score: 13442844676 \nFixed bandwidth: 76.29126 CV score: 13442844676 \nFixed bandwidth: 76.29123 CV score: 13442844676 \n\nbw_CV\n\n[1] 76.29126\n\n\n\n\n\n\n\nbw_AIC &lt;- bw.gwr(GDPPC ~ 1, \n             data = hunan_sp,\n             approach =\"AIC\",\n             adaptive = FALSE, \n             kernel = \"bisquare\", \n             longlat = T)\n\nFixed bandwidth: 357.4897 AICc value: 1927.631 \nFixed bandwidth: 220.985 AICc value: 1921.547 \nFixed bandwidth: 136.6204 AICc value: 1919.993 \nFixed bandwidth: 84.48025 AICc value: 1940.603 \nFixed bandwidth: 168.8448 AICc value: 1919.457 \nFixed bandwidth: 188.7606 AICc value: 1920.007 \nFixed bandwidth: 156.5362 AICc value: 1919.41 \nFixed bandwidth: 148.929 AICc value: 1919.527 \nFixed bandwidth: 161.2377 AICc value: 1919.392 \nFixed bandwidth: 164.1433 AICc value: 1919.403 \nFixed bandwidth: 159.4419 AICc value: 1919.393 \nFixed bandwidth: 162.3475 AICc value: 1919.394 \nFixed bandwidth: 160.5517 AICc value: 1919.391 \n\nbw_AIC\n\n[1] 160.5517"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#geographically-weighted-summary-statistics-with-fixed-1",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#geographically-weighted-summary-statistics-with-fixed-1",
    "title": "In-class Exercise 5: Geographically Weighted Statistics - gwModel methods",
    "section": "Geographically Weighted Summary Statistics with fixed",
    "text": "Geographically Weighted Summary Statistics with fixed\nComputing adaptive bandwidth\n\n\ngwstat &lt;- gwss(data = hunan_sp,\n               vars = \"GDPPC\",\n               bw = bw_AIC,\n               kernel = \"bisquare\",\n               adaptive = FALSE,\n               longlat = T)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#geographically-weighted-summary-statistics-with-fixed-bandwidth",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#geographically-weighted-summary-statistics-with-fixed-bandwidth",
    "title": "In-class Exercise 5: Geographically Weighted Statistics - gwModel methods",
    "section": "Geographically Weighted Summary Statistics with fixed bandwidth",
    "text": "Geographically Weighted Summary Statistics with fixed bandwidth\nPreparing the output data\nCode chunk below is used to extract SDF data table from gwss object output from gwss(). It will be converted into data.frame by using as.data.frame().\n\n\ngwstat_df &lt;- as.data.frame(gwstat$SDF)\n\n\nNext, cbind() is used to append the newly derived data.frame onto hunan_sf sf data.frame.\n\n\nhunan_gstat &lt;- cbind(hunan_sf, gwstat_df)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#geographically-weighted-summary-statistics-with-fixed-2",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#geographically-weighted-summary-statistics-with-fixed-2",
    "title": "In-class Exercise 5: Geographically Weighted Statistics - gwModel methods",
    "section": "Geographically Weighted Summary Statistics with fixed",
    "text": "Geographically Weighted Summary Statistics with fixed\nVisualising geographically weighted summary statistics\n\nThe Geographically Weighted MeanThe code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntm_shape(hunan_gstat) +\n  tm_fill(\"GDPPC_LM\",\n          n = 5,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Distribution of geographically wieghted mean\",\n            main.title.position = \"center\",\n            main.title.size = 2.0,\n            legend.text.size = 1.2,\n            legend.height = 1.50, \n            legend.width = 1.50,\n            frame = TRUE)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex06/In-class_Ex06-sfdep.html#content",
    "href": "In-class_Ex/In-class_Ex06/In-class_Ex06-sfdep.html#content",
    "title": "In-class Exercise 6: Global and Local Measures of Spatial Autocorrelation: sfdep methods",
    "section": "Content",
    "text": "Content\n\nIntroducing sfdep.\n\nsfdep creates an sf and tidyverse friendly interface to the package as well as introduces new functionality that is not present in spdep.\nsfdep utilizes list columns extensively to make this interface possible.”"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex06/In-class_Ex06-sfdep.html#getting-started",
    "href": "In-class_Ex/In-class_Ex06/In-class_Ex06-sfdep.html#getting-started",
    "title": "In-class Exercise 6: Global and Local Measures of Spatial Autocorrelation: sfdep methods",
    "section": "Getting started",
    "text": "Getting started\nInstalling and Loading the R Packages\nFour R packages will be used for this in-class exercise, they are: sf, sfdep, tmap and tidyverse.\n\nDo It Yourself!The code\n\n\nUsing the steps you learned in previous lesson, install and load sf, tmap, sfdep and tidyverse packages into R environment.\n\n\n\n\npacman::p_load(sf, sfdep, tmap, tidyverse)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex06/In-class_Ex06-sfdep.html#the-data",
    "href": "In-class_Ex/In-class_Ex06/In-class_Ex06-sfdep.html#the-data",
    "title": "In-class Exercise 6: Global and Local Measures of Spatial Autocorrelation: sfdep methods",
    "section": "The Data",
    "text": "The Data\nFor the purpose of this in-class exercise, the Hunan data sets will be used. There are two data sets in this use case, they are:\n\nHunan, a geospatial data set in ESRI shapefile format, and\nHunan_2012, an attribute data set in csv format.\n\n\nDo It Yourself!The code\n\n\nUsing the steps you learned in previous lesson, import Hunan shapefile into R environment as an sf data frame.\n\n\n\n\nhunan &lt;- st_read(dsn = \"data/geospatial\", \n                 layer = \"Hunan\")\n\nReading layer `Hunan' from data source \n  `D:\\tskam\\IS415_AY2024-25T1\\In-class_Ex\\In-class_Ex06\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex06/In-class_Ex06-sfdep.html#importing-attribute-table",
    "href": "In-class_Ex/In-class_Ex06/In-class_Ex06-sfdep.html#importing-attribute-table",
    "title": "In-class Exercise 6: Global and Local Measures of Spatial Autocorrelation: sfdep methods",
    "section": "Importing Attribute Table",
    "text": "Importing Attribute Table\n\nDo It Yourself!The code\n\n\nUsing the steps you learned in previous lesson, import Hunan_2012.csv into R environment as an tibble data frame.\n\n\n\n\nhunan2012 &lt;- read_csv(\"data/aspatial/Hunan_2012.csv\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex06/In-class_Ex06-sfdep.html#combining-both-data-frame-by-using-left-join",
    "href": "In-class_Ex/In-class_Ex06/In-class_Ex06-sfdep.html#combining-both-data-frame-by-using-left-join",
    "title": "In-class Exercise 6: Global and Local Measures of Spatial Autocorrelation: sfdep methods",
    "section": "Combining both data frame by using left join",
    "text": "Combining both data frame by using left join\n\nDo It Yourself!The code\n\n\nUsing the steps you learned in previous lesson, combine the Hunan sf data frame and Hunan_2012 data frame. Ensure that the output is an sf data frame.\n\n\n\n\nhunan_GDPPC &lt;- left_join(hunan, hunan2012) %&gt;%\n  select(1:4, 7, 15)\n\n\n\n\n\n\n\n\nNote\n\n\nFor the purpose of this exercise, we only retain column 1 to 4, column 7 and column 15. You should examine the output sf data.frame to learn know what are these fields.\n\n\n\n\n\n\n\n\n\nImportant\n\n\nIn order to retain the geospatial properties, the left data frame must the sf data.frame (i.e. hunan)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex06/In-class_Ex06-sfdep.html#plotting-a-choropleth-map",
    "href": "In-class_Ex/In-class_Ex06/In-class_Ex06-sfdep.html#plotting-a-choropleth-map",
    "title": "In-class Exercise 6: Global and Local Measures of Spatial Autocorrelation: sfdep methods",
    "section": "Plotting a choropleth map",
    "text": "Plotting a choropleth map\n\nDo It Yourself!The plotThe code\n\n\nUsing the steps you learned in previous lesson, plot a choropleth map showing the distribution of GDPPC of Hunan Province.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntmap_mode(\"plot\")\ntm_shape(hunan_GDPPC) +\n  tm_fill(\"GDPPC\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"GDPPC\") +\n  tm_layout(main.title = \"Distribution of GDP per capita by county, Hunan Province\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex06/In-class_Ex06-sfdep.html#global-measures-of-spatial-association",
    "href": "In-class_Ex/In-class_Ex06/In-class_Ex06-sfdep.html#global-measures-of-spatial-association",
    "title": "In-class Exercise 6: Global and Local Measures of Spatial Autocorrelation: sfdep methods",
    "section": "Global Measures of Spatial Association",
    "text": "Global Measures of Spatial Association\nStep 1: Deriving Queen’s contiguity weights: sfdep methods\n\n\n\n\nwm_q &lt;- hunan_GDPPC %&gt;%\n  mutate(nb = st_contiguity(geometry),\n         wt = st_weights(nb,\n                         style = \"W\"),\n         .before = 1) \n\n\n\n\nNotice that st_weights() provides tree arguments, they are:\n\nnb: A neighbor list object as created by st_neighbors().\nstyle: Default “W” for row standardized weights. This value can also be “B”, “C”, “U”, “minmax”, and “S”. B is the basic binary coding, W is row standardised (sums over all links to n), C is globally standardised (sums over all links to n), U is equal to C divided by the number of neighbours (sums over all links to unity), while S is the variance-stabilizing coding scheme proposed by Tiefelsdorf et al. 1999, p. 167-168 (sums over all links to n).\nallow_zero: If TRUE, assigns zero as lagged value to zone without neighbors."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex06/In-class_Ex06-sfdep.html#lisa-map",
    "href": "In-class_Ex/In-class_Ex06/In-class_Ex06-sfdep.html#lisa-map",
    "title": "In-class Exercise 6: Global and Local Measures of Spatial Autocorrelation: sfdep methods",
    "section": "LISA map",
    "text": "LISA map\n\n\nLISA map is a categorical map showing outliers and clusters. There are two types of outliers namely: High-Low and Low-High outliers. Likewise, there are two type of clusters namely: High-High and Low-Low cluaters. In fact, LISA map is an interpreted map by combining local Moran’s I of geographical areas and their respective p-values."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex06/In-class_Ex06-sfdep.html#computing-local-morans-i",
    "href": "In-class_Ex/In-class_Ex06/In-class_Ex06-sfdep.html#computing-local-morans-i",
    "title": "In-class Exercise 6: Global and Local Measures of Spatial Autocorrelation: sfdep methods",
    "section": "Computing local Moran’s I",
    "text": "Computing local Moran’s I\nIn this section, you will learn how to compute Local Moran’s I of GDPPC at county level by using local_moran() of sfdep package.\n\nThe codeThe output\n\n\n\n\nlisa &lt;- wm_q %&gt;% \n  mutate(local_moran = local_moran(\n    GDPPC, nb, wt, nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_moran)\n\n\n\n\n\nThe output of local_moran() is a sf data.frame containing the columns ii, eii, var_ii, z_ii, p_ii, p_ii_sim, and p_folded_sim.\n\nii: local moran statistic\neii: expectation of local moran statistic; for localmoran_permthe permutation sample means\nvar_ii: variance of local moran statistic; for localmoran_permthe permutation sample standard deviations\nz_ii: standard deviate of local moran statistic; for localmoran_perm based on permutation sample means and standard deviations p_ii: p-value of local moran statistic using pnorm(); for localmoran_perm using standard deviatse based on permutation sample means and standard deviations p_ii_sim: For localmoran_perm(), rank() and punif() of observed statistic rank for [0, 1] p-values using alternative= -p_folded_sim: the simulation folded [0, 0.5] range ranked p-value (based on https://github.com/pysal/esda/blob/4a63e0b5df1e754b17b5f1205b cadcbecc5e061/esda/crand.py#L211-L213)\nskewness: For localmoran_perm, the output of e1071::skewness() for the permutation samples underlying the standard deviates\nkurtosis: For localmoran_perm, the output of e1071::kurtosis() for the permutation samples underlying the standard deviates."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex06/In-class_Ex06-sfdep.html#hot-spot-and-cold-spot-area-analysis-hcsa",
    "href": "In-class_Ex/In-class_Ex06/In-class_Ex06-sfdep.html#hot-spot-and-cold-spot-area-analysis-hcsa",
    "title": "In-class Exercise 6: Global and Local Measures of Spatial Autocorrelation: sfdep methods",
    "section": "Hot Spot and Cold Spot Area Analysis (HCSA)",
    "text": "Hot Spot and Cold Spot Area Analysis (HCSA)\n\n\nHCSA uses spatial weights to identify locations of statistically significant hot spots and cold spots in an spatially weighted attribute that are in proximity to one another based on a calculated distance. The analysis groups features when similar high (hot) or low (cold) values are found in a cluster. The polygon features usually represent administration boundaries or a custom grid structure."
  },
  {
    "objectID": "outline/Lesson11_outline.html#references",
    "href": "outline/Lesson11_outline.html#references",
    "title": "Lesson 11: Geographically Weighted Regression",
    "section": "References",
    "text": "References\n\nMennis, Jeremy (2006) “Mapping the Results of Geographically Weighted Regression”, The Cartographic Journal, Vol.43 (2), p.171-179.\nStephen A. Matthews ; Tse-Chuan Yang (2012) “Mapping the results of local statistics: Using geographically weighted regression”, Demographic Research, Vol.26, p.151-166."
  },
  {
    "objectID": "outline/Lesson10_outline.html#self-reading-before-meet-up",
    "href": "outline/Lesson10_outline.html#self-reading-before-meet-up",
    "title": "Lesson 10: Regression Modelling of Geographically Reference Data",
    "section": "Self-reading Before Meet-up",
    "text": "Self-reading Before Meet-up\nTo read before class:\n\nBrunsdon, C., Fotheringham, A.S., and Charlton, M. (2002) “Geographically weighted regression: A method for exploring spatial nonstationarity”. Geographical Analysis, 28: 281-289.\nBrunsdon, C., Fotheringham, A.S. and Charlton, M., (1999) “Some Notes on Parametric Significance Tests for Geographically Weighted Regression”. Journal of Regional Science, 39(3), 497-524."
  },
  {
    "objectID": "outline/Lesson10_outline.html#references",
    "href": "outline/Lesson10_outline.html#references",
    "title": "Lesson 10: Regression Modelling of Geographically Reference Data",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "outline/Lesson10_outline.html#all-about-r",
    "href": "outline/Lesson10_outline.html#all-about-r",
    "title": "Lesson 10: Regression Modelling of Geographically Reference Data",
    "section": "All About R",
    "text": "All About R"
  },
  {
    "objectID": "outline/Lesson09_outline.html#lesson-slides",
    "href": "outline/Lesson09_outline.html#lesson-slides",
    "title": "Lesson 9: Spatially Constrained Cluster Analysis",
    "section": "Lesson slides",
    "text": "Lesson slides\n\nLesson 9: Geographic Segmentation with Spatially Constrained Cluster Analysis slides."
  },
  {
    "objectID": "outline/Lesson09_outline.html#hands-on-exercise",
    "href": "outline/Lesson09_outline.html#hands-on-exercise",
    "title": "Lesson 9: Spatially Constrained Cluster Analysis",
    "section": "Hands-on Exercise",
    "text": "Hands-on Exercise\n\n12 Geographical Segmentation with Spatially Constrained Clustering Techniques, from Section 12.8 onward.\n\n\nSelf-reading Before Meet-up\nTo read before class:\n\nAssuncao, R. M., Neves, M.C., Camara, G. and Costa Freitas, C.D. 2006. “Efficient Regionalization Techniques for Socio-Economic Geographical Units Using Minimum Spanning Trees”. International Journal of Geographical Information Science 20: 797-811. (Available in SMU digital library)\nChavent, M., Kuentz-Simonet, V., Labenne,A. and Saracco, J. 2018. “ClustGeo: an R package for hierarchical clustering with spatial constraints” Computational Statistics, 33: 1799-1822. (Available in SMU digital library)\n\n\n\nReferences\n\nJaya, I.G.M., Ruchjana, B.N., Andriyana, Y. & Agata, R (2019) “Clustering with spatial constraints: The case of diarrhea in Bandung city, Indonesia”\nDistefano, V., Mameli, V., & Poli, I. (2020) “Identifying spatial patterns with the Bootstrap ClustGeo technique”, Spatial statistics, Vol.38. (Available in SMU digital library)\nde Souza, D. C. & Taconeli, C. A. (2022) “Spatial and non-spatial clustering algorithm in the analysis of Brazilian educational data”, Communications in Statistics: Case Studies, Data Analysis, and Applications. Vol. 8, No. 4, 588-606. (Available in SMU digital library)\n\n\n\nAll About R\n\nHierarchical Cluster Analysis.\nskater: A function from spdep package that implements a SKATER procedure for spatial clustering analysis.\nClustGeo: Hierarchical Clustering with Spatial Constraints\n\nIntroduction to Clustgeo"
  },
  {
    "objectID": "outline/Lesson08_outline.html#references",
    "href": "outline/Lesson08_outline.html#references",
    "title": "Lesson 8: Hierarchical Clustering for Multivariate Geographically Referenced Data",
    "section": "References",
    "text": "References\n\nRovan, J. and Sambt, J. (2003) “Socio-economic Differences Among Slovenian Municipalities: A Cluster Analysis Approach”, Developments in Applied Statistics, pp. 265-278.\n\nDemeter, T. and Bratucu, G. (2013) “Statistical Analysis Of The EU Countries from A Touristic Point of View”, Bulletin of the Transilvania University of Braşov, 6(55): 121-130.\nBrown, N.S. & Watson, P. (2012) “What can a comprehensive plan really tell us about a region?: A cluster analysis of county comprehensive plans in Idaho”, Western Economics Forum. Pp.22-37.\n\n\nAll About R\n\nHierarchical Cluster Analysis."
  },
  {
    "objectID": "th_ex3.html",
    "href": "th_ex3.html",
    "title": "Take-home Exercise 3: Prototyping Modules for Geospatial Analytics Shiny Application",
    "section": "",
    "text": "Prototyping is crucial\n\n\n\nPrototyping first may help keep you from investing far too much time for marginal gains.\nThe Art of UNIX Programming (Raymond 2003)"
  },
  {
    "objectID": "th_ex3.html#the-task",
    "href": "th_ex3.html#the-task",
    "title": "Take-home Exercise 3: Prototyping Modules for Geospatial Analytics Shiny Application",
    "section": "The Task",
    "text": "The Task\nIn this take-home exercise, you are required to select one of the module of your proposed Geospatial Analytics Shiny Application and complete the following tasks:\n\nTo evaluate and determine the necessary R packages needed for your Shiny application are supported in R CRAN,\nTo prepare and test the specific R codes can be run and returned the correct output as expected,\nTo determine the parameters and outputs that will be exposed on the Shiny applications, and\nTo select the appropriate Shiny UI components for exposing the parameters determine above.\n\nAll teams must consult the SimplyGeo page of SimplyGeo. Then, examine their respective Take-home Exercise 3 page:\n\nTake-Home Exercise 03: Prototyping Modules for Geospatial Analytics Shiny Application,\nTake Home Exercise 3: Prototyping Modules for Geospatial Analytics Shiny Application\nTakehome-Ex 3: Exploratory Spatial Data Analysis for Potential Push & Pull Factors of Locations in Singapore using Public Bus Data\n\nEach of them were prepared by one of the member of the project team. After that they combined them into the Prototype page of their project website."
  },
  {
    "objectID": "th_ex3.html#submission-instructions",
    "href": "th_ex3.html#submission-instructions",
    "title": "Take-home Exercise 3: Prototyping Modules for Geospatial Analytics Shiny Application",
    "section": "Submission Instructions",
    "text": "Submission Instructions\nThis is an individual assignment. You are required to work on the take-home exercises and prepare submission individually.\nThe specific submission instructions are as follows:\n\nYou are required to prepare the prototype module report as Take-home Exercise 3 submission. This mean, it has to be published on your own coursework page.\nYou are required to include a section called UI design for the different components of the UIs for the proposed design. For storyboarding the UI Design, please consult Storyboard link.\nThe write-up of the take-home exercise must be in Quarto html document format. You are required to publish the write-up on Netlify.\nZip the take-home exercise folder and upload it onto eLearn. If the size of the zip file is beyond the capacity of eLearn, you can upload it on SMU OneDrive and provide the download link on eLearn."
  },
  {
    "objectID": "th_ex3.html#submission-date",
    "href": "th_ex3.html#submission-date",
    "title": "Take-home Exercise 3: Prototyping Modules for Geospatial Analytics Shiny Application",
    "section": "Submission date",
    "text": "Submission date\nYour completed take-home exercise is due on 3rd November 2024, by 11:59pm evening.\n\n\n\n\n\n\nWarning\n\n\n\nStudents are strongly recommended to complete Take-home Exercise 3 ahead of the submission deadline. This will allow you to have more time to focus on the building of the Shiny application and to iterate the usability of the application."
  },
  {
    "objectID": "th_ex3.html#peer-learning",
    "href": "th_ex3.html#peer-learning",
    "title": "Take-home Exercise 3: Prototyping Modules for Geospatial Analytics Shiny Application",
    "section": "Peer Learning",
    "text": "Peer Learning"
  },
  {
    "objectID": "lesson/Lesson09/Lesson09.html#content",
    "href": "lesson/Lesson09/Lesson09.html#content",
    "title": "Lesson 9: Geographic Segmentation with Spatially Constrained Clustering",
    "section": "Content",
    "text": "Content\n\nIntroduction to Geographic Segmentation\nSpatialising classic clustering methods\nSpatially Constrained Clustering - Hierarchical methods\n\nskater\nREDCAP\nclustGeo"
  },
  {
    "objectID": "lesson/Lesson09/Lesson09.html#regionalisation-and-clustering",
    "href": "lesson/Lesson09/Lesson09.html#regionalisation-and-clustering",
    "title": "Lesson 9: Geographic Segmentation with Spatially Constrained Clustering",
    "section": "Regionalisation and Clustering",
    "text": "Regionalisation and Clustering\n\n\n\nRegionalisation is a process of to group a large number of geographical units such as provinces, districts or counties spatial objects into a smaller number of subsets of objects also known as regions, which are internally homogeneous and occupy contiguous regions in space.\nThe process taking into consideration multivariates. Figure on the right shows regions delineated by using six ICT measures, namely: Radio, Television, Land line phone, Mobile phone, Computer, and Internet at home of Shan State, Myanmar."
  },
  {
    "objectID": "lesson/Lesson09/Lesson09.html#spatially-constrained-clustering-versus-non-spatially-constrained-clustering",
    "href": "lesson/Lesson09/Lesson09.html#spatially-constrained-clustering-versus-non-spatially-constrained-clustering",
    "title": "Lesson 9: Geographic Segmentation with Spatially Constrained Clustering",
    "section": "Spatially Constrained Clustering versus Non-spatially Constrained Clustering",
    "text": "Spatially Constrained Clustering versus Non-spatially Constrained Clustering\n\n\nA comparison of conventional non-spatially constrained clustering and spatially constrained clustering. Stars represent the centroids of sampled grid cells and polygons are Thiessen polygons that contain the centroids. Grey shading contrasts between polygons stand for the Simpson dissimilarity index (βsim) between them. Non-spatially constrained clustering produces two clusters, one of which contains polygons (C, D, and E) that are spatially disjoint. In contrast, the two clusters produced by the spatially constrained clustering form two spatially contiguous regions."
  },
  {
    "objectID": "lesson/Lesson09/Lesson09.html#spatially-constrained-clustering",
    "href": "lesson/Lesson09/Lesson09.html#spatially-constrained-clustering",
    "title": "Lesson 9: Geographic Segmentation with Spatially Constrained Clustering",
    "section": "Spatially Constrained Clustering",
    "text": "Spatially Constrained Clustering\n\nSKATER (Spatial ’K’luster Analysis by Tree Edge Removal) algorithm\nREDCAP (Regionalization with dynamically constrained agglomerative clustering and partitioning algorithm\nClustGeo algorithm"
  },
  {
    "objectID": "lesson/Lesson09/Lesson09.html#skater-spatial-kluster-analysis-by-tree-edge-removal-algorithm",
    "href": "lesson/Lesson09/Lesson09.html#skater-spatial-kluster-analysis-by-tree-edge-removal-algorithm",
    "title": "Lesson 9: Geographic Segmentation with Spatially Constrained Clustering",
    "section": "SKATER (Spatial ’K’luster Analysis by Tree Edge Removal) algorithm",
    "text": "SKATER (Spatial ’K’luster Analysis by Tree Edge Removal) algorithm\n\n\nThe SKATER (Spatial ’K’luster Analysis by Tree Edge Removal) builds off of a connectivity graph to represent spatial relationships between neighbouring areas, where each area is represented by a node and edges represent connections between areas. Edge costs are calculated by evaluating the dissimilarity between neighbouring areas. The connectivity graph is reduced by pruning edges with higher dissimilarity until we are left with n nodes and n−1 edges. At this point any further pruning would create subgraphs and these subgraphs become cluster candidates."
  },
  {
    "objectID": "lesson/Lesson09/Lesson09.html#redcap-algorithm",
    "href": "lesson/Lesson09/Lesson09.html#redcap-algorithm",
    "title": "Lesson 9: Geographic Segmentation with Spatially Constrained Clustering",
    "section": "REDCAP algorithm",
    "text": "REDCAP algorithm\n\nRegionalization with dynamically constrained agglomerative clustering and partitioning, in short REDCAP is specially developed by D. Guo (2008) to the limitation of SKATER discussed in previous slide.\nLike SKATER, REDCAP starts from building a spanning tree with 4 different ways (single-linkage, average-linkage, ward-linkage and the complete-linkage). The single-linkage way leads to build a minimum spanning tree. Then,REDCAP provides 2 different ways (first-order and full-order constraining) to prune the tree to find clusters. The first-order approach with a minimum spanning tree is exactly the same with SKATER."
  },
  {
    "objectID": "lesson/Lesson09/Lesson09.html#clustgeo-package",
    "href": "lesson/Lesson09/Lesson09.html#clustgeo-package",
    "title": "Lesson 9: Geographic Segmentation with Spatially Constrained Clustering",
    "section": "ClustGeo Package",
    "text": "ClustGeo Package\nThe R package ClustGeo implements a Ward-like hierarchical clustering algorithm including spatial/geographical constraints.\n\nTwo dissimilarity matrices D0 and D1 are inputted, along with a mixing parameter alpha in [0,1]. The dissimilarities can be non-Euclidean and the weights of the observations can be non-uniform.\nThe first matrix gives the dissimilarities in the “feature space”” and the second matrix gives the dissimilarities in the “constraint space”.\nThe criterion minimized at each stage is a convex combination of the homogeneity criterion calculated with D0 and the homogeneity criterion calculated with D1. The idea is to determine a value of alpha which increases the spatial contiguity without deteriorating too much the quality of the solution based on the variables of interest i.e. those of the feature space."
  },
  {
    "objectID": "lesson/Lesson09/Lesson09.html",
    "href": "lesson/Lesson09/Lesson09.html",
    "title": "Lesson 9: Geographic Segmentation with Spatial Clustering",
    "section": "",
    "text": "Introduction to Geographic Segmentation\nSpatialising classic clustering methods\nSpatially Constrained Clustering - Hierarchical methods\n\nskater\nREDCAP\nclustGeo"
  },
  {
    "objectID": "lesson/Lesson11/Lesson11-gwr.html#content",
    "href": "lesson/Lesson11/Lesson11-gwr.html#content",
    "title": "Lesson 11: Geographically Weighted Regression",
    "section": "Content",
    "text": "Content\n\nWhat is Spatial Non-stationary\nIntroducing Geographically Weighted Regression\n\nWeighting functions (kernel)\nWeighting schemes\nBandwidth\n\nInterpreting and Visualising"
  },
  {
    "objectID": "lesson/Lesson11/Lesson11-gwr.html#the-why-questions",
    "href": "lesson/Lesson11/Lesson11-gwr.html#the-why-questions",
    "title": "Lesson 11: Geographically Weighted Regression",
    "section": "The WHY Questions",
    "text": "The WHY Questions\nWhy condominium units located at the central part of Singapore were transacted at relatively higher prices than others?"
  },
  {
    "objectID": "lesson/Lesson11/Lesson11-gwr.html#the-why-questions-1",
    "href": "lesson/Lesson11/Lesson11-gwr.html#the-why-questions-1",
    "title": "Lesson 11: Geographically Weighted Regression",
    "section": "The WHY Questions",
    "text": "The WHY Questions\nWhy condominium units located at the central part of Singapore were transacted at relatively higher prices than others?"
  },
  {
    "objectID": "lesson/Lesson11/Lesson11-gwr.html#what-is-regression-analysis",
    "href": "lesson/Lesson11/Lesson11-gwr.html#what-is-regression-analysis",
    "title": "Lesson 11: Geographically Weighted Regression",
    "section": "What is regression analysis?",
    "text": "What is regression analysis?\n\nA set of statistical processes for explaining the relationships among variables.\nThe focus is on the relationship between a dependent variable (y) and one or more independent variables (x)\n\nDoes X affect Y? If so, how?\nWhat is the change in Y given a one unit change in X?\n\nEstimate outcomes based on the relationships modelled."
  },
  {
    "objectID": "lesson/Lesson11/Lesson11-gwr.html#assumptions-of-linear-regression-models",
    "href": "lesson/Lesson11/Lesson11-gwr.html#assumptions-of-linear-regression-models",
    "title": "Lesson 11: Geographically Weighted Regression",
    "section": "Assumptions of linear regression models",
    "text": "Assumptions of linear regression models\n\nLinearity assumption. The relationship between the dependent variable and independent variables is (approximately) linear.\nNormality assumption. The residual errors are assumed to be normally distributed.\nHomogeneity of residuals variance. The residuals are assumed to have a constant variance (homoscedasticity).\nThe residuals are uncorrelated with each other.\n\nserial correlation, as with time series\n\n(Optional) The errors (residuals) are normally distributed and have a 0 population mean.]"
  },
  {
    "objectID": "lesson/Lesson11/Lesson11-gwr.html#spatial-non-stationary",
    "href": "lesson/Lesson11/Lesson11-gwr.html#spatial-non-stationary",
    "title": "Lesson 11: Geographically Weighted Regression",
    "section": "Spatial Non-stationary",
    "text": "Spatial Non-stationary\n\nWhen applied to spatial data, as can be seen, it assumes a stationary spatial process.\n\nThe same stimulus provokes the same response in all parts of the study region.\nHighly untenable for spatial process."
  },
  {
    "objectID": "lesson/Lesson11/Lesson11-gwr.html#geographically-weighted-regression-gwr",
    "href": "lesson/Lesson11/Lesson11-gwr.html#geographically-weighted-regression-gwr",
    "title": "Lesson 11: Geographically Weighted Regression",
    "section": "Geographically Weighted Regression (GWR)",
    "text": "Geographically Weighted Regression (GWR)\n\nLocal statistical technique to analyze spatial variations in relationships.\nSpatial non-stationarity is assumed and will be tested.\nBased on the “First Law of Geography”: everything is related with everything else, but closer things are more related."
  },
  {
    "objectID": "lesson/Lesson11/Lesson11-gwr.html#references",
    "href": "lesson/Lesson11/Lesson11-gwr.html#references",
    "title": "Lesson 11: Geographically Weighted Regression",
    "section": "References",
    "text": "References\nBrunsdon, C., Fotheringham, A.S., and Charlton, M. (2002) “Geographically weighted regression: A method for exploring spatial nonstationarity”. Geographical Analysis, 28: 281-289.\nBrunsdon, C., Fotheringham, A.S. and Charlton, M., (1999) “Some Notes on Parametric Significance Tests for Geographically Weighted Regression”. Journal of Regional Science, 39(3), 497-524.\nMennis, Jeremy (2006) “Mapping the Results of Geographically Weighted Regression”, The Cartographic Journal, Vol.43 (2), p.171-179.\nStephen A. Matthews ; Tse-Chuan Yang (2012) “Mapping the results of local statistics: Using geographically weighted regression”, Demographic Research, Vol.26, p.151-166."
  },
  {
    "objectID": "lesson/Lesson12/Lesson12-GWRF.html#content",
    "href": "lesson/Lesson12/Lesson12-GWRF.html#content",
    "title": "Lesson 12: Geographically Weighted Predictive Modelling",
    "section": "Content",
    "text": "Content\n\nWhat is Predictive Modelling?\nWhat is Geospatial Predictive Modelling\nIntroducing Recursive Partitioning\nAdvanced Recursive Partitioning: Random Forest\nIntroducing Geographically Weighted Random Forest"
  },
  {
    "objectID": "lesson/Lesson12/Lesson12-GWRF.html#what-is-predictive-modelling",
    "href": "lesson/Lesson12/Lesson12-GWRF.html#what-is-predictive-modelling",
    "title": "Lesson 12: Geographically Weighted Predictive Modelling",
    "section": "What is Predictive Modelling?",
    "text": "What is Predictive Modelling?\n\n\n\nPredictive modelling uses statistical learning or machine learning techniques to predict outcomes.\n\nBy and large, the event one wants to predict is in the future. However, a set of known outcome and predictors (also known as variables) will be used to calibrate the predictive models."
  },
  {
    "objectID": "lesson/Lesson12/Lesson12-GWRF.html#what-is-geospatial-predictive-modelling",
    "href": "lesson/Lesson12/Lesson12-GWRF.html#what-is-geospatial-predictive-modelling",
    "title": "Lesson 12: Geographically Weighted Predictive Modelling",
    "section": "What is Geospatial Predictive Modelling",
    "text": "What is Geospatial Predictive Modelling\n\nGeospatial predictive modelling is conceptually rooted in the principle that the occurrences of events being modeled are limited in distribution.\n\nWhen geographically referenced data are used, occurrences of events are neither uniform nor random in distribution over space. There are geospatial factors (infrastructure, sociocultural, topographic, etc.) that constrain and influence where the locations of events occur.\nGeospatial predictive modeling attempts to describe those constraints and influences by spatially correlating occurrences of historical geospatial locations with environmental factors that represent those constraints and influences."
  },
  {
    "objectID": "lesson/Lesson12/Lesson12-GWRF.html#predictive-modelling-process",
    "href": "lesson/Lesson12/Lesson12-GWRF.html#predictive-modelling-process",
    "title": "Lesson 12: Geographically Weighted Predictive Modelling",
    "section": "Predictive Modelling Process",
    "text": "Predictive Modelling Process"
  },
  {
    "objectID": "lesson/Lesson12/Lesson12-GWRF.html#introducing-recursive-partitioning",
    "href": "lesson/Lesson12/Lesson12-GWRF.html#introducing-recursive-partitioning",
    "title": "Lesson 12: Geographically Weighted Predictive Modelling",
    "section": "Introducing recursive partitioning",
    "text": "Introducing recursive partitioning\n\nA predictive methodology involving a dependent variable y and one and more predictors.\nThe dependent variable can be either a continuous or categorical scales.\nRules partition data into mutually exclusive groups.\nNo need to worry about transformations such as logs.\nNo prior distribution requirement."
  },
  {
    "objectID": "lesson/Lesson12/Lesson12-GWRF.html#advanced-recursive-partitioning-random-forest",
    "href": "lesson/Lesson12/Lesson12-GWRF.html#advanced-recursive-partitioning-random-forest",
    "title": "Lesson 12: Geographically Weighted Predictive Modelling",
    "section": "Advanced Recursive Partitioning: Random Forest",
    "text": "Advanced Recursive Partitioning: Random Forest\nRandom forest, like its name implies, consists of a large number of individual decision trees that operate as an ensemble. - Each individual tree in the random forest spits out a class prediction and the class with the most votes becomes our model’s prediction as shown the figure."
  },
  {
    "objectID": "lesson/Lesson12/Lesson12-GWRF.html#introducing-geographically-weighted-random-forest-gwrf",
    "href": "lesson/Lesson12/Lesson12-GWRF.html#introducing-geographically-weighted-random-forest-gwrf",
    "title": "Lesson 12: Geographically Weighted Predictive Modelling",
    "section": "Introducing Geographically Weighted Random Forest (gwRF)",
    "text": "Introducing Geographically Weighted Random Forest (gwRF)\n\nGeographically Weighted Random Forest (GRF) is a spatial analysis method using a local version of the famous Machine Learning algorithm.\n\nThis technique adopts the idea of the Geographically Weighted Regression.\nThe main difference between a tradition (linear) GWR and GRF is that we can model non-stationarity coupled with a flexible non-linear model which is very hard to overfit due to its bootstrapping nature, thus relaxing the assumptions of traditional Gaussian statistics."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04-stpp.html#reference",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04-stpp.html#reference",
    "title": "Spatio-Temporal Point Patterns Analysis",
    "section": "Reference",
    "text": "Reference\nPeter Hall (1990) “Using the Bootstrap to Estimate Mean Squared Error and Select Smoothing Parameter in Nonparametric Problems”, JOURNAL OF MULTIVARIATE ANALYSIS, Vol. 32, pp. 177-203."
  },
  {
    "objectID": "lesson/Lesson10/Lesson10-mlr.html",
    "href": "lesson/Lesson10/Lesson10-mlr.html",
    "title": "Lesson 11: Geographically Weighted Regression",
    "section": "",
    "text": "Introducing Regression Modelling\n\nSimple Linear Regression\nMultiple Linear Regression\n\nWhat is Spatial Non-stationary\nIntroducing Geographically Weighted Regression\n\nWeighting functions (kernel)\nWeighting schemes\nBandwidth\n\nInterpreting and Visualising"
  },
  {
    "objectID": "lesson/Lesson10/Lesson10-mlr.html#content",
    "href": "lesson/Lesson10/Lesson10-mlr.html#content",
    "title": "Lesson 11: Geographically Weighted Regression",
    "section": "Content",
    "text": "Content\n\nIntroducing Regression Modelling\n\nSimple Linear Regression\nMultiple Linear Regression\n\nWhat is Spatial Non-stationary\nIntroducing Geographically Weighted Regression\n\nWeighting functions (kernel)\nWeighting schemes\nBandwidth\n\nInterpreting and Visualising"
  },
  {
    "objectID": "lesson/Lesson10/Lesson10-mlr.html#the-why-questions",
    "href": "lesson/Lesson10/Lesson10-mlr.html#the-why-questions",
    "title": "Lesson 11: Geographically Weighted Regression",
    "section": "The WHY Questions",
    "text": "The WHY Questions\n\nWhy some condominium units were transacted at relatively higher prices than others?"
  },
  {
    "objectID": "lesson/Lesson10/Lesson10-mlr.html#the-why-questions-1",
    "href": "lesson/Lesson10/Lesson10-mlr.html#the-why-questions-1",
    "title": "Lesson 11: Geographically Weighted Regression",
    "section": "The WHY Questions",
    "text": "The WHY Questions\nWhy condominium units located at the central part of Singapore were transacted at relatively higher prices than others?"
  },
  {
    "objectID": "lesson/Lesson10/Lesson10-mlr.html#what-is-regression-analysis",
    "href": "lesson/Lesson10/Lesson10-mlr.html#what-is-regression-analysis",
    "title": "Lesson 11: Geographically Weighted Regression",
    "section": "What is regression analysis?",
    "text": "What is regression analysis?\n\nA set of statistical processes for explaining the relationships among variables.\nThe focus is on the relationship between a dependent variable (y) and one or more independent variables (x)\n\nDoes X affect Y? If so, how?\nWhat is the change in Y given a one unit change in X?\n\nEstimate outcomes based on the relationships modelled."
  },
  {
    "objectID": "lesson/Lesson10/Lesson10-mlr.html#assumptions-of-linear-regression-models",
    "href": "lesson/Lesson10/Lesson10-mlr.html#assumptions-of-linear-regression-models",
    "title": "Lesson 11: Geographically Weighted Regression",
    "section": "Assumptions of linear regression models",
    "text": "Assumptions of linear regression models\n\nLinearity assumption. The relationship between the dependent variable and independent variables is (approximately) linear.\nNormality assumption. The residual errors are assumed to be normally distributed.\nHomogeneity of residuals variance. The residuals are assumed to have a constant variance (homoscedasticity).\nThe residuals are uncorrelated with each other.\n\nserial correlation, as with time series\n\n(Optional) The errors (residuals) are normally distributed and have a 0 population mean.]"
  },
  {
    "objectID": "lesson/Lesson10/Lesson10-mlr.html#spatial-non-stationary",
    "href": "lesson/Lesson10/Lesson10-mlr.html#spatial-non-stationary",
    "title": "Lesson 11: Geographically Weighted Regression",
    "section": "Spatial Non-stationary",
    "text": "Spatial Non-stationary\n\nWhen applied to spatial data, as can be seen, it assumes a stationary spatial process.\n\nThe same stimulus provokes the same response in all parts of the study region.\nHighly untenable for spatial process."
  },
  {
    "objectID": "lesson/Lesson10/Lesson10-mlr.html#geographically-weighted-regression-gwr",
    "href": "lesson/Lesson10/Lesson10-mlr.html#geographically-weighted-regression-gwr",
    "title": "Lesson 11: Geographically Weighted Regression",
    "section": "Geographically Weighted Regression (GWR)",
    "text": "Geographically Weighted Regression (GWR)\n\nLocal statistical technique to analyze spatial variations in relationships.\nSpatial non-stationarity is assumed and will be tested.\nBased on the “First Law of Geography”: everything is related with everything else, but closer things are more related."
  },
  {
    "objectID": "lesson/Lesson10/Lesson10-mlr.html#references",
    "href": "lesson/Lesson10/Lesson10-mlr.html#references",
    "title": "Lesson 11: Geographically Weighted Regression",
    "section": "References",
    "text": "References\nBrunsdon, C., Fotheringham, A.S., and Charlton, M. (2002) “Geographically weighted regression: A method for exploring spatial nonstationarity”. Geographical Analysis, 28: 281-289.\nBrunsdon, C., Fotheringham, A.S. and Charlton, M., (1999) [“Some Notes on Parametric Significance Tests for Geographically Weighted Regression”](https://onlinelibrary-wiley-com.libproxy.smu.edu.sg/doi/abs/10.1111/0022-4146.00146. Journal of Regional Science, 39(3), 497-524.\nMennis, Jeremy (2006) “Mapping the Results of Geographically Weighted Regression”, The Cartographic Journal, Vol.43 (2), p.171-179.\nStephen A. Matthews ; Tse-Chuan Yang (2012) “Mapping the results of local statistics: Using geographically weighted regression”, Demographic Research, Vol.26, p.151-166."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex08/In-class_Ex07-HCA.html",
    "href": "In-class_Ex/In-class_Ex08/In-class_Ex07-HCA.html",
    "title": "Hierarchical Clustering for Multivariate Geographically Referenced Data with R",
    "section": "",
    "text": "pacman::p_load(spdep, sp, tmap, sf, ClustGeo, \n               ggpubr, cluster, factoextra, NbClust,\n               heatmaply, corrplot, psych, tidyverse, GGally)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex08/In-class_Ex07-HCA.html#loading-r-packages",
    "href": "In-class_Ex/In-class_Ex08/In-class_Ex07-HCA.html#loading-r-packages",
    "title": "Hierarchical Clustering for Multivariate Geographically Referenced Data with R",
    "section": "",
    "text": "pacman::p_load(spdep, sp, tmap, sf, ClustGeo, \n               ggpubr, cluster, factoextra, NbClust,\n               heatmaply, corrplot, psych, tidyverse, GGally)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex08/In-class_Ex07-HCA.html#data-import-and-prepatation",
    "href": "In-class_Ex/In-class_Ex08/In-class_Ex07-HCA.html#data-import-and-prepatation",
    "title": "Hierarchical Clustering for Multivariate Geographically Referenced Data with R",
    "section": "Data Import and Prepatation",
    "text": "Data Import and Prepatation\n\nImporting geospatial data into R environment\n\nshan_sf &lt;- st_read(dsn = \"data/geospatial\", \n                   layer = \"myanmar_township_boundaries\") %&gt;%\n  filter(ST %in% c(\"Shan (East)\", \"Shan (North)\", \"Shan (South)\")) %&gt;%\n  select(c(2:7))\n\nReading layer `myanmar_township_boundaries' from data source \n  `D:\\tskam\\IS415_AY2024-25T1\\In-class_Ex\\In-class_Ex08\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 330 features and 14 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 92.17275 ymin: 9.671252 xmax: 101.1699 ymax: 28.54554\nGeodetic CRS:  WGS 84\n\n\n\nshan_sf\n\nSimple feature collection with 55 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 96.15107 ymin: 19.29932 xmax: 101.1699 ymax: 24.15907\nGeodetic CRS:  WGS 84\nFirst 10 features:\n             ST ST_PCODE       DT   DT_PCODE        TS  TS_PCODE\n1  Shan (North)   MMR015  Mongmit MMR015D008   Mongmit MMR015017\n2  Shan (South)   MMR014 Taunggyi MMR014D001   Pindaya MMR014006\n3  Shan (South)   MMR014 Taunggyi MMR014D001   Ywangan MMR014007\n4  Shan (South)   MMR014 Taunggyi MMR014D001  Pinlaung MMR014009\n5  Shan (North)   MMR015  Mongmit MMR015D008    Mabein MMR015018\n6  Shan (South)   MMR014 Taunggyi MMR014D001     Kalaw MMR014005\n7  Shan (South)   MMR014 Taunggyi MMR014D001     Pekon MMR014010\n8  Shan (South)   MMR014 Taunggyi MMR014D001  Lawksawk MMR014008\n9  Shan (North)   MMR015  Kyaukme MMR015D003 Nawnghkio MMR015013\n10 Shan (North)   MMR015  Kyaukme MMR015D003   Kyaukme MMR015012\n                         geometry\n1  MULTIPOLYGON (((96.96001 23...\n2  MULTIPOLYGON (((96.7731 21....\n3  MULTIPOLYGON (((96.78483 21...\n4  MULTIPOLYGON (((96.49518 20...\n5  MULTIPOLYGON (((96.66306 24...\n6  MULTIPOLYGON (((96.49518 20...\n7  MULTIPOLYGON (((97.14738 19...\n8  MULTIPOLYGON (((96.94981 22...\n9  MULTIPOLYGON (((96.75648 22...\n10 MULTIPOLYGON (((96.95498 22...\n\n\n\n\nImporting aspatial data into R environment\nThe csv file will be import using read_csv function of readr package.\nThe code chunks used are shown below:\n\nict &lt;- read_csv (\"data/aspatial/Shan-ICT.csv\")\n\nThe imported InfoComm variables are extracted from The 2014 Myanmar Population and Housing Census Myanmar. The attribute data set is called ict. It is saved in R’s * tibble data.frame* format.\nThe code chunk below reveal the summary statistics of ict data.frame.\n\nsummary(ict)\n\n District Pcode     District Name      Township Pcode     Township Name     \n Length:55          Length:55          Length:55          Length:55         \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n Total households     Radio         Television    Land line phone \n Min.   : 3318    Min.   :  115   Min.   :  728   Min.   :  20.0  \n 1st Qu.: 8711    1st Qu.: 1260   1st Qu.: 3744   1st Qu.: 266.5  \n Median :13685    Median : 2497   Median : 6117   Median : 695.0  \n Mean   :18369    Mean   : 4487   Mean   :10183   Mean   : 929.9  \n 3rd Qu.:23471    3rd Qu.: 6192   3rd Qu.:13906   3rd Qu.:1082.5  \n Max.   :82604    Max.   :30176   Max.   :62388   Max.   :6736.0  \n  Mobile phone      Computer      Internet at home\n Min.   :  150   Min.   :  20.0   Min.   :   8.0  \n 1st Qu.: 2037   1st Qu.: 121.0   1st Qu.:  88.0  \n Median : 3559   Median : 244.0   Median : 316.0  \n Mean   : 6470   Mean   : 575.5   Mean   : 760.2  \n 3rd Qu.: 7177   3rd Qu.: 507.0   3rd Qu.: 630.5  \n Max.   :48461   Max.   :6705.0   Max.   :9746.0  \n\n\nThere are a total of eleven fields and 55 observation in the tibble data.frame.\n\n\nDerive new variables using dplyr package\nThe unit of measurement of the values are number of household. Using these values directly will be bias by the underlying total number of households. In general, the townships with relatively higher total number of households will also have higher number of households owning radio, TV, etc.\nIn order to overcome this problem, we will derive the penetration rate of each ICT variable by using the code chunk below.\n\nict_derived &lt;- ict %&gt;%\n  mutate(`RADIO_PR` = `Radio`/`Total households`*1000) %&gt;%\n  mutate(`TV_PR` = `Television`/`Total households`*1000) %&gt;%\n  mutate(`LLPHONE_PR` = `Land line phone`/`Total households`*1000) %&gt;%\n  mutate(`MPHONE_PR` = `Mobile phone`/`Total households`*1000) %&gt;%\n  mutate(`COMPUTER_PR` = `Computer`/`Total households`*1000) %&gt;%\n  mutate(`INTERNET_PR` = `Internet at home`/`Total households`*1000) %&gt;%\n  rename(`DT_PCODE` =`District Pcode`,`DT`=`District Name`,\n         `TS_PCODE`=`Township Pcode`, `TS`=`Township Name`,\n         `TT_HOUSEHOLDS`=`Total households`,\n         `RADIO`=`Radio`, `TV`=`Television`, \n         `LLPHONE`=`Land line phone`, `MPHONE`=`Mobile phone`,\n         `COMPUTER`=`Computer`, `INTERNET`=`Internet at home`) \n\nLet us review the summary statistics of the newly derived penetration rates using the code chunk below.\n\nsummary(ict_derived)\n\n   DT_PCODE              DT              TS_PCODE              TS           \n Length:55          Length:55          Length:55          Length:55         \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n TT_HOUSEHOLDS       RADIO             TV           LLPHONE      \n Min.   : 3318   Min.   :  115   Min.   :  728   Min.   :  20.0  \n 1st Qu.: 8711   1st Qu.: 1260   1st Qu.: 3744   1st Qu.: 266.5  \n Median :13685   Median : 2497   Median : 6117   Median : 695.0  \n Mean   :18369   Mean   : 4487   Mean   :10183   Mean   : 929.9  \n 3rd Qu.:23471   3rd Qu.: 6192   3rd Qu.:13906   3rd Qu.:1082.5  \n Max.   :82604   Max.   :30176   Max.   :62388   Max.   :6736.0  \n     MPHONE         COMPUTER         INTERNET         RADIO_PR     \n Min.   :  150   Min.   :  20.0   Min.   :   8.0   Min.   : 21.05  \n 1st Qu.: 2037   1st Qu.: 121.0   1st Qu.:  88.0   1st Qu.:138.95  \n Median : 3559   Median : 244.0   Median : 316.0   Median :210.95  \n Mean   : 6470   Mean   : 575.5   Mean   : 760.2   Mean   :215.68  \n 3rd Qu.: 7177   3rd Qu.: 507.0   3rd Qu.: 630.5   3rd Qu.:268.07  \n Max.   :48461   Max.   :6705.0   Max.   :9746.0   Max.   :484.52  \n     TV_PR         LLPHONE_PR       MPHONE_PR       COMPUTER_PR    \n Min.   :116.0   Min.   :  2.78   Min.   : 36.42   Min.   : 3.278  \n 1st Qu.:450.2   1st Qu.: 22.84   1st Qu.:190.14   1st Qu.:11.832  \n Median :517.2   Median : 37.59   Median :305.27   Median :18.970  \n Mean   :509.5   Mean   : 51.09   Mean   :314.05   Mean   :24.393  \n 3rd Qu.:606.4   3rd Qu.: 69.72   3rd Qu.:428.43   3rd Qu.:29.897  \n Max.   :842.5   Max.   :181.49   Max.   :735.43   Max.   :92.402  \n  INTERNET_PR     \n Min.   :  1.041  \n 1st Qu.:  8.617  \n Median : 22.829  \n Mean   : 30.644  \n 3rd Qu.: 41.281  \n Max.   :117.985  \n\n\nNotice that six new fields have been added into the data.frame. They are RADIO_PR, TV_PR, LLPHONE_PR, MPHONE_PR, COMPUTER_PR, and INTERNET_PR."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex08/In-class_Ex07-HCA.html#exploratory-data-analysis-eda",
    "href": "In-class_Ex/In-class_Ex08/In-class_Ex07-HCA.html#exploratory-data-analysis-eda",
    "title": "Hierarchical Clustering for Multivariate Geographically Referenced Data with R",
    "section": "Exploratory Data Analysis (EDA)",
    "text": "Exploratory Data Analysis (EDA)\n\nEDA using statistical graphics\nWe can plot the distribution of the variables (i.e. Number of households with radio) by using appropriate Exploratory Data Analysis (EDA) as shown in the code chunk below.\nHistogram is useful to identify the overall distribution of the data values (i.e. left skew, right skew or normal distribution)\n\nggplot(data=ict_derived, \n       aes(x=`RADIO`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\")\n\n\n\n\n\n\n\n\nBoxplot is useful to detect if there are outliers.\n\nggplot(data=ict_derived, \n       aes(x=`RADIO`)) +\n  geom_boxplot(color=\"black\", \n               fill=\"light blue\")\n\n\n\n\n\n\n\n\nNext, we will also plotting the distribution of the newly derived variables (i.e. Radio penetration rate) by using the code chunk below.\n\nggplot(data=ict_derived, \n       aes(x=`RADIO_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\")\n\n\n\n\n\n\n\n\n\nggplot(data=ict_derived, \n       aes(x=`RADIO_PR`)) +\n  geom_boxplot(color=\"black\", \n               fill=\"light blue\")\n\n\n\n\n\n\n\n\nWhat can you observed from the distributions reveal in the histogram and boxplot.\nIn the figure below, multiple histograms are plotted to reveal the distribution of the selected variables in the ict_derived data.frame.\n\n\n\n\n\n\n\n\n\nThe code chunks below are used to create the data visualisation. They consist of two main parts. First, we will create the individual histograms using the code chunk below.\n\nradio &lt;- ggplot(data=ict_derived, \n             aes(x= `RADIO_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\")\n\ntv &lt;- ggplot(data=ict_derived, \n             aes(x= `TV_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\")\n\nllphone &lt;- ggplot(data=ict_derived, \n             aes(x= `LLPHONE_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\")\n\nmphone &lt;- ggplot(data=ict_derived, \n             aes(x= `MPHONE_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\")\n\ncomputer &lt;- ggplot(data=ict_derived, \n             aes(x= `COMPUTER_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\")\n\ninternet &lt;- ggplot(data=ict_derived, \n             aes(x= `INTERNET_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\")\n\nNext, the ggarrange() function of ggpubr package is used to group these histograms together.\n\nggarrange(radio, tv, llphone, mphone, computer, internet, \n          ncol = 3, \n          nrow = 2)\n\n\n\nEDA using choropleth map\n\nJoining geospatial data with aspatial data\nBefore we can prepare the choropleth map, we need to combine both the geospatial data object (i.e. shan_sf) and aspatial data.frame object (i.e. ict_derived) into one. This will be performed by using the left_join function of dplyr package. The shan_sf simple feature data.frame will be used as the base data object and the ict_derived data.frame will be used as the join table.\nThe code chunks below is used to perform the task. The unique identifier used to join both data objects is TS_PCODE.\n\nshan_sf &lt;- left_join(shan_sf, \n                     ict_derived, \n                     by=c(\"TS_PCODE\"=\"TS_PCODE\"))\n  \nwrite_rds(shan_sf, \"data/rds/shan_sf.rds\")\n\nThe message above shows that TS_CODE field is the common field used to perform the left-join.\nIt is important to note that there is no new output data been created. Instead, the data fields from ict_derived data frame are now updated into the data frame of shan_sf.\n\nshan_sf &lt;- read_rds(\"data/rds/shan_sf.rds\")\n\n\n\nPreparing a choropleth map\nTo have a quick look at the distribution of Radio penetration rate of Shan State at township level, a choropleth map will be prepared.\nThe code chunks below are used to prepare the choroplethby using the qtm() function of tmap package.\n\nqtm(shan_sf, \"RADIO_PR\")\n\n\n\n\n\n\n\n\nIn order to reveal the distribution shown in the choropleth map above are bias to the underlying total number of households at the townships, we will create two choropleth maps, one for the total number of households (i.e. TT_HOUSEHOLDS.map) and one for the total number of household with Radio (RADIO.map) by using the code chunk below.\n\nTT_HOUSEHOLDS.map &lt;- tm_shape(shan_sf) + \n  tm_fill(col = \"TT_HOUSEHOLDS\",\n          n = 5,\n          style = \"jenks\", \n          title = \"Total households\") + \n  tm_borders(alpha = 0.5) \n\nRADIO.map &lt;- tm_shape(shan_sf) + \n  tm_fill(col = \"RADIO\",\n          n = 5,\n          style = \"jenks\",\n          title = \"Number Radio \") + \n  tm_borders(alpha = 0.5) \n\ntmap_arrange(TT_HOUSEHOLDS.map, RADIO.map,\n             asp=NA, ncol=2)\n\n\n\n\n\n\n\n\nNotice that the choropleth maps above clearly show that townships with relatively larger number ot households are also showing relatively higher number of radio ownership.\nNow let us plot the choropleth maps showing the dsitribution of total number of households and Radio penetration rate by using the code chunk below.\n\ntm_shape(shan_sf) +\n    tm_polygons(c(\"TT_HOUSEHOLDS\", \"RADIO_PR\"),\n                style=\"jenks\") +\n    tm_facets(sync = TRUE, ncol = 2) +\n  tm_legend(legend.position = c(\"right\", \"bottom\"))+\n  tm_layout(outer.margins=0, asp=0)\n\n\n\n\n\n\n\n\nCan you identify the differences?"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex08/In-class_Ex07-HCA.html#correlation-analysis",
    "href": "In-class_Ex/In-class_Ex08/In-class_Ex07-HCA.html#correlation-analysis",
    "title": "Hierarchical Clustering for Multivariate Geographically Referenced Data with R",
    "section": "Correlation Analysis",
    "text": "Correlation Analysis\nBefore we perform cluster analysis, it is important for us to ensure that the cluster variables are not highly correlated.\nIn this section, you will learn how to use corrplot.mixed() function of corrplot package to visualise and analyse the correlation of the input variables.\n\ncluster_vars.cor = cor(ict_derived[,12:17])\ncorrplot.mixed(cluster_vars.cor,\n         lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               tl.col = \"black\")\n\n\n\n\n\n\n\n\nThe correlation plot above shows that COMPUTER_PR and INTERNET_PR are highly correlated. This suggest that only one of them should be used in the cluster analysis instead of both."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex08/In-class_Ex07-HCA.html#hierarchy-cluster-analysis",
    "href": "In-class_Ex/In-class_Ex08/In-class_Ex07-HCA.html#hierarchy-cluster-analysis",
    "title": "Hierarchical Clustering for Multivariate Geographically Referenced Data with R",
    "section": "Hierarchy Cluster Analysis",
    "text": "Hierarchy Cluster Analysis\nIn this section, you will learn how to perform hierarchical cluster analysis. The analysis consists of four major steps:\n\nExtracting clustering variables\nThe code chunk below will be used to extract the clustering variables from the shan_sf simple feature object into data.frame.\n\ncluster_vars &lt;- shan_sf %&gt;%\n  st_set_geometry(NULL) %&gt;%\n  select(\"TS.x\", \"RADIO_PR\", \"TV_PR\", \"LLPHONE_PR\", \"MPHONE_PR\", \"COMPUTER_PR\")\nhead(cluster_vars,10)\n\n        TS.x RADIO_PR    TV_PR LLPHONE_PR MPHONE_PR COMPUTER_PR\n1    Mongmit 286.1852 554.1313   35.30618  260.6944    12.15939\n2    Pindaya 417.4647 505.1300   19.83584  162.3917    12.88190\n3    Ywangan 484.5215 260.5734   11.93591  120.2856     4.41465\n4   Pinlaung 231.6499 541.7189   28.54454  249.4903    13.76255\n5     Mabein 449.4903 708.6423   72.75255  392.6089    16.45042\n6      Kalaw 280.7624 611.6204   42.06478  408.7951    29.63160\n7      Pekon 318.6118 535.8494   39.83270  214.8476    18.97032\n8   Lawksawk 387.1017 630.0035   31.51366  320.5686    21.76677\n9  Nawnghkio 349.3359 547.9456   38.44960  323.0201    15.76465\n10   Kyaukme 210.9548 601.1773   39.58267  372.4930    30.94709\n\n\nNotice that the final clustering variables list does not include variable INTERNET_PR because it is highly correlated with variable COMPUTER_PR.\nNext, we need to change the rows by township name instead of row number by using the code chunk below\n\nrow.names(cluster_vars) &lt;- cluster_vars$\"TS.x\"\nhead(cluster_vars,10)\n\n               TS.x RADIO_PR    TV_PR LLPHONE_PR MPHONE_PR COMPUTER_PR\nMongmit     Mongmit 286.1852 554.1313   35.30618  260.6944    12.15939\nPindaya     Pindaya 417.4647 505.1300   19.83584  162.3917    12.88190\nYwangan     Ywangan 484.5215 260.5734   11.93591  120.2856     4.41465\nPinlaung   Pinlaung 231.6499 541.7189   28.54454  249.4903    13.76255\nMabein       Mabein 449.4903 708.6423   72.75255  392.6089    16.45042\nKalaw         Kalaw 280.7624 611.6204   42.06478  408.7951    29.63160\nPekon         Pekon 318.6118 535.8494   39.83270  214.8476    18.97032\nLawksawk   Lawksawk 387.1017 630.0035   31.51366  320.5686    21.76677\nNawnghkio Nawnghkio 349.3359 547.9456   38.44960  323.0201    15.76465\nKyaukme     Kyaukme 210.9548 601.1773   39.58267  372.4930    30.94709\n\n\nNotice that the row number has been replaced into the township name.\nNow, we will delete the TS.x field by using the code chunk below.\n\nshan_ict &lt;- select(cluster_vars, c(2:6))\nhead(shan_ict, 10)\n\n          RADIO_PR    TV_PR LLPHONE_PR MPHONE_PR COMPUTER_PR\nMongmit   286.1852 554.1313   35.30618  260.6944    12.15939\nPindaya   417.4647 505.1300   19.83584  162.3917    12.88190\nYwangan   484.5215 260.5734   11.93591  120.2856     4.41465\nPinlaung  231.6499 541.7189   28.54454  249.4903    13.76255\nMabein    449.4903 708.6423   72.75255  392.6089    16.45042\nKalaw     280.7624 611.6204   42.06478  408.7951    29.63160\nPekon     318.6118 535.8494   39.83270  214.8476    18.97032\nLawksawk  387.1017 630.0035   31.51366  320.5686    21.76677\nNawnghkio 349.3359 547.9456   38.44960  323.0201    15.76465\nKyaukme   210.9548 601.1773   39.58267  372.4930    30.94709\n\n\n\n\nData Standardisation\nIn general, multiple variables will be used in cluster analysis. It is not unusual their values range are different. In order to avoid the cluster analysis result is baised to clustering variables with large values, it is useful to standardise the input variables before performing cluster analysis.\n\n\nMin-Max standardisation\nIn the code chunk below, normalize() of heatmaply package is used to stadardisation the clustering variables by using Min-Max method. The summary() is then used to display the summary statistics of the standardised clustering variables.\n\nshan_ict.std &lt;- normalize(shan_ict)\nsummary(shan_ict.std)\n\n    RADIO_PR          TV_PR          LLPHONE_PR       MPHONE_PR     \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.2544   1st Qu.:0.4600   1st Qu.:0.1123   1st Qu.:0.2199  \n Median :0.4097   Median :0.5523   Median :0.1948   Median :0.3846  \n Mean   :0.4199   Mean   :0.5416   Mean   :0.2703   Mean   :0.3972  \n 3rd Qu.:0.5330   3rd Qu.:0.6750   3rd Qu.:0.3746   3rd Qu.:0.5608  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n  COMPUTER_PR     \n Min.   :0.00000  \n 1st Qu.:0.09598  \n Median :0.17607  \n Mean   :0.23692  \n 3rd Qu.:0.29868  \n Max.   :1.00000  \n\n\nNotice that the values range of the Min-max standardised clustering variables are 0-1 now.\n\n\nZ-score standardisation\nZ-score standardisation can be performed easily by using scale() of Base R. The code chunk below will be used to stadardisation the clustering variables by using Z-score method.\n\nshan_ict.z &lt;- scale(shan_ict)\ndescribe(shan_ict.z)\n\n            vars  n mean sd median trimmed  mad   min  max range  skew kurtosis\nRADIO_PR       1 55    0  1  -0.04   -0.06 0.94 -1.85 2.55  4.40  0.48    -0.27\nTV_PR          2 55    0  1   0.05    0.04 0.78 -2.47 2.09  4.56 -0.38    -0.23\nLLPHONE_PR     3 55    0  1  -0.33   -0.15 0.68 -1.19 3.20  4.39  1.37     1.49\nMPHONE_PR      4 55    0  1  -0.05   -0.06 1.01 -1.58 2.40  3.98  0.48    -0.34\nCOMPUTER_PR    5 55    0  1  -0.26   -0.18 0.64 -1.03 3.31  4.34  1.80     2.96\n              se\nRADIO_PR    0.13\nTV_PR       0.13\nLLPHONE_PR  0.13\nMPHONE_PR   0.13\nCOMPUTER_PR 0.13\n\n\nNotice the mean and standard deviation of the Z-score standardised clustering variables are 0 and 1 respectively.\nNote: describe() of psych package is used here instead of summary() of Base R because the earlier provides standard deviation.\nWarning: Z-score standardisation method should only be used if we would assume all variables come from some normal distribution.\n\n\nVisualising the standardised clustering variables\nBeside reviewing the summary statistics of the standardised clustering variables, it is also a good practice to visualise their distribution graphical.\nThe code chunk below plot the scaled Radio_PR field.\n\nr &lt;- ggplot(data=ict_derived, \n             aes(x= `RADIO_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\") +\n  ggtitle(\"Raw values without standardisation\")\n\nshan_ict_s_df &lt;- as.data.frame(shan_ict.std)\ns &lt;- ggplot(data=shan_ict_s_df, \n       aes(x=`RADIO_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\") +\n  ggtitle(\"Min-Max Standardisation\")\n\nshan_ict_z_df &lt;- as.data.frame(shan_ict.z)\nz &lt;- ggplot(data=shan_ict_z_df, \n       aes(x=`RADIO_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\") +\n  ggtitle(\"Z-score Standardisation\")\n\nggarrange(r, s, z,\n          ncol = 3,\n          nrow = 1)\n\n\n\n\n\n\n\n\n\nWhat statistical conclusion can you draw from the histograms above?\n\n\nr &lt;- ggplot(data=ict_derived, \n             aes(x= `RADIO_PR`)) +\n  geom_density(color=\"black\",\n               fill=\"light blue\") +\n  ggtitle(\"Raw values without standardisation\")\n\nshan_ict_s_df &lt;- as.data.frame(shan_ict.std)\ns &lt;- ggplot(data=shan_ict_s_df, \n       aes(x=`RADIO_PR`)) +\n  geom_density(color=\"black\",\n               fill=\"light blue\") +\n  ggtitle(\"Min-Max Standardisation\")\n\nshan_ict_z_df &lt;- as.data.frame(shan_ict.z)\nz &lt;- ggplot(data=shan_ict_z_df, \n       aes(x=`RADIO_PR`)) +\n  geom_density(color=\"black\",\n               fill=\"light blue\") +\n  ggtitle(\"Z-score Standardisation\")\n\nggarrange(r, s, z,\n          ncol = 3,\n          nrow = 1)\n\n\n\n\n\n\n\n\n\n\nComputing proximity matrix\nIn R, many packages provide functions to calculate distance matrix. We will compute the proximity matrix by using dist() of R.\ndist() supports six distance proximity calculations, they are: euclidean, maximum, manhattan, canberra, binary and minkowski. The default is euclidean proximity matrix.\nThe code chunk below is used to compute the proximity matrix using euclidean method.\n\nproxmat &lt;- dist(shan_ict, method = 'euclidean')\n\nThe code chunk below can then be used to list the content of proxmat for visual inspection.\n\nproxmat\n\n\n\nComputing hierarchical clustering\nIn R, there are several packages provide hierarchical clustering function. In this hands-on exercise, hclust() of R stats will be used.\nhclust() employed agglomeration method to compute the cluster. Eight clustering algorithms are supported, they are: ward.D, ward.D2, single, complete, average(UPGMA), mcquitty(WPGMA), median(WPGMC) and centroid(UPGMC).\nThe code chunk below performs hierarchical cluster analysis using ward.D method. The hierarchical clustering output is stored in an object of class hclust which describes the tree produced by the clustering process.\n\nhclust_ward &lt;- hclust(proxmat, method = 'ward.D')\n\nWe can then plot the tree by using plot() of R Graphics as shown in the code chunk below.\n\nplot(hclust_ward, cex = 0.6)\n\n\n\n\n\n\n\n\n\n\nSelecting the optimal clustering algorithm\nOne of the challenge in performing hierarchical clustering is to identify stronger clustering structures. The issue can be solved by using use agnes() function of cluster package. It functions like hclus(), however, with the agnes() function you can also get the agglomerative coefficient, which measures the amount of clustering structure found (values closer to 1 suggest strong clustering structure).\nThe code chunk below will be used to compute the agglomerative coefficients of all hierarchical clustering algorithms.\n\nm &lt;- c( \"average\", \"single\", \"complete\", \"ward\")\nnames(m) &lt;- c( \"average\", \"single\", \"complete\", \"ward\")\n\nac &lt;- function(x) {\n  agnes(shan_ict, method = x)$ac\n}\n\nmap_dbl(m, ac)\n\n  average    single  complete      ward \n0.8131144 0.6628705 0.8950702 0.9427730 \n\n\nWith reference to the output above, we can see that Ward’s method provides the strongest clustering structure among the four methods assessed. Hence, in the subsequent analysis, only Ward’s method will be used.\n\n\nDetermining Optimal Clusters\nAnother technical challenge face by data analyst in performing clustering analysis is to determine the optimal clusters to retain.\nThere are three commonly used methods to determine the optimal clusters, they are:\n\nElbow Method\nAverage Silhouette Method\nGap Statistic Method\n\n\nGap Statistic Method\nThe gap statistic compares the total within intra-cluster variation for different values of k with their expected values under null reference distribution of the data. The estimate of the optimal clusters will be value that maximize the gap statistic (i.e., that yields the largest gap statistic). This means that the clustering structure is far away from the random uniform distribution of points.\nTo compute the gap statistic, clusGap() of cluster package will be used.\n\nset.seed(12345)\ngap_stat &lt;- clusGap(shan_ict, \n                    FUN = hcut, \n                    nstart = 25, \n                    K.max = 10, \n                    B = 50)\n# Print the result\nprint(gap_stat, method = \"firstmax\")\n\nClustering Gap statistic [\"clusGap\"] from call:\nclusGap(x = shan_ict, FUNcluster = hcut, K.max = 10, B = 50, nstart = 25)\nB=50 simulated reference sets, k = 1..10; spaceH0=\"scaledPCA\"\n --&gt; Number of clusters (method 'firstmax'): 1\n          logW   E.logW       gap     SE.sim\n [1,] 8.407129 8.680794 0.2736651 0.04460994\n [2,] 8.130029 8.350712 0.2206824 0.03880130\n [3,] 7.992265 8.202550 0.2102844 0.03362652\n [4,] 7.862224 8.080655 0.2184311 0.03784781\n [5,] 7.756461 7.978022 0.2215615 0.03897071\n [6,] 7.665594 7.887777 0.2221833 0.03973087\n [7,] 7.590919 7.806333 0.2154145 0.04054939\n [8,] 7.526680 7.731619 0.2049390 0.04198644\n [9,] 7.458024 7.660795 0.2027705 0.04421874\n[10,] 7.377412 7.593858 0.2164465 0.04540947\n\n\nAlso note that the hcut function used is from factoextra package.\nNext, we can visualise the plot by using fviz_gap_stat() of factoextra package.\n\nfviz_gap_stat(gap_stat)\n\n\n\n\n\n\n\n\nWith reference to the gap statistic graph above, the recommended number of cluster to retain is 1. However, it is not logical to retain only one cluster. By examine the gap statistic graph, the 6-cluster gives the largest gap statistic and should be the next best cluster to pick.\nNote: In addition to these commonly used approaches, the NbClust package, published by Charrad et al., 2014, provides 30 indices for determining the relevant number of clusters and proposes to users the best clustering scheme from the different results obtained by varying all combinations of number of clusters, distance measures, and clustering methods.\n\n\n\nInterpreting the dendrograms\nIn the dendrogram displayed above, each leaf corresponds to one observation. As we move up the tree, observations that are similar to each other are combined into branches, which are themselves fused at a higher height.\nThe height of the fusion, provided on the vertical axis, indicates the (dis)similarity between two observations. The higher the height of the fusion, the less similar the observations are. Note that, conclusions about the proximity of two observations can be drawn only based on the height where branches containing those two observations first are fused. We cannot use the proximity of two observations along the horizontal axis as a criteria of their similarity.\nIt’s also possible to draw the dendrogram with a border around the selected clusters by using rect.hclust() of R stats. The argument border is used to specify the border colors for the rectangles.\n\nplot(hclust_ward, cex = 0.6)\nrect.hclust(hclust_ward, \n            k = 6, \n            border = 2:5)\n\n\n\n\n\n\n\n\n\n\nVisually-driven hierarchical clustering analysis\nIn this section, we will learn how to perform visually-driven hiearchical clustering analysis by using heatmaply package.\nWith heatmaply, we are able to build both highly interactive cluster heatmap or static cluster heatmap.\n\nTransforming the data frame into a matrix\nThe data was loaded into a data frame, but it has to be a data matrix to make your heatmap.\nThe code chunk below will be used to transform shan_ict data frame into a data matrix.\n\nshan_ict_mat &lt;- data.matrix(shan_ict)\n\n\n\nPlotting interactive cluster heatmap using heatmaply()\nIn the code chunk below, the heatmaply() of heatmaply package is used to build an interactive cluster heatmap.\n\nheatmaply(normalize(shan_ict_mat),\n          Colv=NA,\n          dist_method = \"euclidean\",\n          hclust_method = \"ward.D\",\n          seriate = \"OLO\",\n          colors = Blues,\n          k_row = 6,\n          margins = c(NA,200,60,NA),\n          fontsize_row = 4,\n          fontsize_col = 5,\n          main=\"Geographic Segmentation of Shan State by ICT indicators\",\n          xlab = \"ICT Indicators\",\n          ylab = \"Townships of Shan State\"\n          )\n\n\n\n\n\n\n\n\nMapping the clusters formed\nWith closed examination of the dendragram above, we have decided to retain six clusters.\ncutree() of R Base will be used in the code chunk below to derive a 6-cluster model.\n\ngroups &lt;- as.factor(cutree(hclust_ward, k=6))\n\nThe output is called groups. It is a list object.\nIn order to visualise the clusters, the groups object need to be appended onto shan_sf simple feature object.\nThe code chunk below form the join in three steps:\n\nthe groups list object will be converted into a matrix;\ncbind() is used to append groups matrix onto shan_sf to produce an output simple feature object called shan_sf_cluster; and\nrename of dplyr package is used to rename as.matrix.groups field as CLUSTER.\n\n\nshan_sf_cluster &lt;- cbind(shan_sf, as.matrix(groups)) %&gt;%\n  rename(`CLUSTER`=`as.matrix.groups.`)\n\nNext, qtm() of tmap package is used to plot the choropleth map showing the cluster formed.\n\nqtm(shan_sf_cluster, \"CLUSTER\")\n\n\n\n\n\n\n\n\nThe choropleth map above reveals the clusters are very fragmented. The is one of the major limitation when non-spatial clustering algorithm such as hierarchical cluster analysis method is used."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex08/In-class_Ex07-HCA.html#spatially-constrained-clustering-skater-approach",
    "href": "In-class_Ex/In-class_Ex08/In-class_Ex07-HCA.html#spatially-constrained-clustering-skater-approach",
    "title": "Hierarchical Clustering for Multivariate Geographically Referenced Data with R",
    "section": "Spatially Constrained Clustering: SKATER approach",
    "text": "Spatially Constrained Clustering: SKATER approach\nIn this section, you will learn how to derive spatially constrained cluster by using skater() method of spdep package.\n\nConverting into SpatialPolygonsDataFrame\nFirst, we need to convert shan_sf into SpatialPolygonsDataFrame. This is because SKATER function only support sp objects such as SpatialPolygonDataFrame.\nThe code chunk below uses as_Spatial() of sf package to convert shan_sf into a SpatialPolygonDataFrame called shan_sp.\n\nshan_sp &lt;- as_Spatial(shan_sf)\n\n\n\nComputing Neighbour List\nNext, poly2nd() of spdep package will be used to compute the neighbours list from polygon list.\n\nshan.nb &lt;- poly2nb(shan_sf)\nsummary(shan.nb)\n\nNeighbour list object:\nNumber of regions: 55 \nNumber of nonzero links: 264 \nPercentage nonzero weights: 8.727273 \nAverage number of links: 4.8 \nLink number distribution:\n\n 2  3  4  5  6  7  8  9 \n 5  9  7 21  4  3  5  1 \n5 least connected regions:\n3 5 7 9 47 with 2 links\n1 most connected region:\n8 with 9 links\n\n\nWe can plot the neighbours list on shan_sp by using the code chunk below. Since we now can plot the community area boundaries as well, we plot this graph on top of the map. The first plot command gives the boundaries. This is followed by the plot of the neighbor list object, with coordinates applied to the original SpatialPolygonDataFrame (Shan state township boundaries) to extract the centroids of the polygons. These are used as the nodes for the graph representation. We also set the color to blue and specify add=TRUE to plot the network on top of the boundaries.\n\nplot(st_geometry(shan_sf), \n     border=grey(.5))\npts &lt;- st_coordinates(st_centroid(shan_sf))\nplot(shan.nb, \n     pts, \n     col=\"blue\", \n     add=TRUE)\n\n\n\n\n\n\n\n\nNote that if you plot the network first and then the boundaries, some of the areas will be clipped. This is because the plotting area is determined by the characteristics of the first plot. In this example, because the boundary map extends further than the graph, we plot it first.\n\n\nComputing minimum spanning tree\n\nCalculating edge costs\nNext, nbcosts() of spdep package is used to compute the cost of each edge. It is the distance between it nodes. This function compute this distance using a data.frame with observations vector in each node.\nThe code chunk below is used to compute the cost of each edge.\n\nlcosts &lt;- nbcosts(shan.nb, shan_ict)\n\nFor each observation, this gives the pairwise dissimilarity between its values on the five variables and the values for the neighbouring observation (from the neighbour list). Basically, this is the notion of a generalised weight for a spatial weights matrix.\nNext, We will incorporate these costs into a weights object in the same way as we did in the calculation of inverse of distance weights. In other words, we convert the neighbour list to a list weights object by specifying the just computed lcosts as the weights.\nIn order to achieve this, nb2listw() of spdep package is used as shown in the code chunk below.\nNote that we specify the style as B to make sure the cost values are not row-standardised.\n\nshan.w &lt;- nb2listw(shan.nb, \n                   lcosts, \n                   style=\"B\")\nsummary(shan.w)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 55 \nNumber of nonzero links: 264 \nPercentage nonzero weights: 8.727273 \nAverage number of links: 4.8 \nLink number distribution:\n\n 2  3  4  5  6  7  8  9 \n 5  9  7 21  4  3  5  1 \n5 least connected regions:\n3 5 7 9 47 with 2 links\n1 most connected region:\n8 with 9 links\n\nWeights style: B \nWeights constants summary:\n   n   nn       S0       S1        S2\nB 55 3025 76267.65 58260785 522016004\n\n\n\n\n\nComputing minimum spanning tree\nThe minimum spanning tree is computed by mean of the mstree() of spdep package as shown in the code chunk below.\n\nshan.mst &lt;- mstree(shan.w)\n\nAfter computing the MST, we can check its class and dimension by using the code chunk below.\n\nclass(shan.mst)\n\n[1] \"mst\"    \"matrix\"\n\ndim(shan.mst)\n\n[1] 54  3\n\n\nNote that the dimension is 54 and not 55. This is because the minimum spanning tree consists on n-1 edges (links) in order to traverse all the nodes.\nWe can display the content of shan.mst by using head() as shown in the code chunk below.\n\nhead(shan.mst)\n\n     [,1] [,2]      [,3]\n[1,]   54   48  47.79331\n[2,]   54   17 109.08506\n[3,]   54   45 127.42203\n[4,]   45   52 146.78891\n[5,]   52   13 110.55197\n[6,]   13   28  92.79567\n\n\nThe plot method for the MST include a way to show the observation numbers of the nodes in addition to the edge. As before, we plot this together with the township boundaries. We can see how the initial neighbour list is simplified to just one edge connecting each of the nodes, while passing through all the nodes.\n\nplot(st_geometry(shan_sf), \n     border=gray(.5))\nplot.mst(shan.mst, \n         pts, \n         col=\"blue\", \n         cex.lab=0.7, \n         cex.circles=0.005, \n         add=TRUE)\n\n\n\n\n\n\n\n\n\n\nComputing spatially constrained clusters using SKATER method\nThe code chunk below compute the spatially constrained cluster using skater() of spdep package.\n\nclust6 &lt;- spdep::skater(edges = shan.mst[,1:2], \n                 data = shan_ict, \n                 method = \"euclidean\", \n                 ncuts = 5)\n\nThe skater() takes three mandatory arguments: - the first two columns of the MST matrix (i.e. not the cost), - the data matrix (to update the costs as units are being grouped), and - the number of cuts. Note: It is set to one less than the number of clusters. So, the value specified is not the number of clusters, but the number of cuts in the graph, one less than the number of clusters.\nThe result of the skater() is an object of class skater. We can examine its contents by using the code chunk below.\n\nstr(clust6)\n\nList of 8\n $ groups      : num [1:55] 3 3 6 3 3 3 3 3 3 3 ...\n $ edges.groups:List of 6\n  ..$ :List of 3\n  .. ..$ node: num [1:22] 13 48 54 55 45 37 34 16 25 52 ...\n  .. ..$ edge: num [1:21, 1:3] 48 55 54 37 34 16 45 25 13 13 ...\n  .. ..$ ssw : num 3423\n  ..$ :List of 3\n  .. ..$ node: num [1:18] 47 27 53 38 42 15 41 51 43 32 ...\n  .. ..$ edge: num [1:17, 1:3] 53 15 42 38 41 51 15 27 15 43 ...\n  .. ..$ ssw : num 3759\n  ..$ :List of 3\n  .. ..$ node: num [1:11] 2 6 8 1 36 4 10 9 46 5 ...\n  .. ..$ edge: num [1:10, 1:3] 6 1 8 36 4 6 8 10 10 9 ...\n  .. ..$ ssw : num 1458\n  ..$ :List of 3\n  .. ..$ node: num [1:2] 44 20\n  .. ..$ edge: num [1, 1:3] 44 20 95\n  .. ..$ ssw : num 95\n  ..$ :List of 3\n  .. ..$ node: num 23\n  .. ..$ edge: num[0 , 1:3] \n  .. ..$ ssw : num 0\n  ..$ :List of 3\n  .. ..$ node: num 3\n  .. ..$ edge: num[0 , 1:3] \n  .. ..$ ssw : num 0\n $ not.prune   : NULL\n $ candidates  : int [1:6] 1 2 3 4 5 6\n $ ssto        : num 12613\n $ ssw         : num [1:6] 12613 10977 9962 9540 9123 ...\n $ crit        : num [1:2] 1 Inf\n $ vec.crit    : num [1:55] 1 1 1 1 1 1 1 1 1 1 ...\n - attr(*, \"class\")= chr \"skater\"\n\n\nThe most interesting component of this list structure is the groups vector containing the labels of the cluster to which each observation belongs (as before, the label itself is arbitary). This is followed by a detailed summary for each of the clusters in the edges.groups list. Sum of squares measures are given as ssto for the total and ssw to show the effect of each of the cuts on the overall criterion.\nWe can check the cluster assignment by using the conde chunk below.\n\nccs6 &lt;- clust6$groups\nccs6\n\n [1] 3 3 6 3 3 3 3 3 3 3 2 1 1 1 2 1 1 1 2 4 1 2 5 1 1 1 2 1 2 2 1 2 2 1 1 3 1 2\n[39] 2 2 2 2 2 4 1 3 2 1 1 1 2 1 2 1 1\n\n\nWe can find out how many observations are in each cluster by means of the table command. Parenthetially, we can also find this as the dimension of each vector in the lists contained in edges.groups. For example, the first list has node with dimension 12, which is also the number of observations in the first cluster.\n\ntable(ccs6)\n\nccs6\n 1  2  3  4  5  6 \n22 18 11  2  1  1 \n\n\nLastly, we can also plot the pruned tree that shows the five clusters on top of the townshop area.\n\nplot(st_geometry(shan_sf), \n     border=gray(.5))\nplot(clust6, \n     pts, \n     cex.lab=.7,\n     groups.colors=c(\"red\",\"green\",\"blue\", \"brown\", \"pink\"),\n     cex.circles=0.005, \n     add=TRUE)\n\n\n\n\n\n\n\n\n\n\nVisualising the clusters in choropleth map\nThe code chunk below is used to plot the newly derived clusters by using SKATER method.\n\ngroups_mat &lt;- as.matrix(clust6$groups)\nshan_sf_spatialcluster &lt;- cbind(shan_sf_cluster, as.factor(groups_mat)) %&gt;%\n  rename(`SP_CLUSTER`=`as.factor.groups_mat.`)\nqtm(shan_sf_spatialcluster, \"SP_CLUSTER\")\n\n\n\n\n\n\n\n\nFor easy comparison, it will be better to place both the hierarchical clustering and spatially constrained hierarchical clustering maps next to each other.\n\nhclust.map &lt;- qtm(shan_sf_cluster,\n                  \"CLUSTER\") + \n  tm_borders(alpha = 0.5) \n\nshclust.map &lt;- qtm(shan_sf_spatialcluster,\n                   \"SP_CLUSTER\") + \n  tm_borders(alpha = 0.5) \n\ntmap_arrange(hclust.map, shclust.map,\n             asp=NA, ncol=2)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex08/In-class_Ex07-HCA.html#spatially-constrained-clustering-clustgeo-method",
    "href": "In-class_Ex/In-class_Ex08/In-class_Ex07-HCA.html#spatially-constrained-clustering-clustgeo-method",
    "title": "Hierarchical Clustering for Multivariate Geographically Referenced Data with R",
    "section": "Spatially Constrained Clustering: ClustGeo Method",
    "text": "Spatially Constrained Clustering: ClustGeo Method\nIn this section, you will gain hands-on experience on using functions provided by ClustGeo package to perform non-spatially constrained hierarchical cluster analysis and spatially constrained cluster analysis.\n\nA short note about ClustGeo package\nClustGeo package is an R package specially designed to support the need of performing spatially constrained cluster analysis. More specifically, it provides a Ward-like hierarchical clustering algorithm called hclustgeo() including spatial/geographical constraints.\nIn the nutshell, the algorithm uses two dissimilarity matrices D0 and D1 along with a mixing parameter alpha, whereby the value of alpha must be a real number between [0, 1]. D0 can be non-Euclidean and the weights of the observations can be non-uniform. It gives the dissimilarities in the attribute/clustering variable space. D1, on the other hand, gives the dissimilarities in the constraint space. The criterion minimised at each stage is a convex combination of the homogeneity criterion calculated with D0 and the homogeneity criterion calculated with D1.\nThe idea is then to determine a value of alpha which increases the spatial contiguity without deteriorating too much the quality of the solution based on the variables of interest. This need is supported by a function called choicealpha().\n\n\nWard-like hierarchical clustering: ClustGeo\nClustGeo package provides function called hclustgeo() to perform a typical Ward-like hierarchical clustering just like hclust() you learned in previous section.\nTo perform non-spatially constrained hierarchical clustering, we only need to provide the function a dissimilarity matrix as shown in the code chunk below.\n\nnongeo_cluster &lt;- hclustgeo(proxmat)\nplot(nongeo_cluster, cex = 0.5)\nrect.hclust(nongeo_cluster, \n            k = 6, \n            border = 2:5)\n\n\n\n\n\n\n\n\nNote that the dissimilarity matrix must be an object of class dist, i.e. an object obtained with the function dist(). For sample code chunk, please refer to 5.7.6 Computing proximity matrix\n\nMapping the clusters formed\nSimilarly, we can plot the clusters on a categorical area shaded map by using the steps we learned in 5.7.12 Mapping the clusters formed.\n\ngroups &lt;- as.factor(cutree(nongeo_cluster, k=6))\n\n\nshan_sf_ngeo_cluster &lt;- cbind(shan_sf, as.matrix(groups)) %&gt;%\n  rename(`CLUSTER` = `as.matrix.groups.`)\n\n\nqtm(shan_sf_ngeo_cluster, \"CLUSTER\")\n\n\n\n\n\n\n\n\n\n\n\nSpatially Constrained Hierarchical Clustering\nBefore we can performed spatially constrained hierarchical clustering, a spatial distance matrix will be derived by using st_distance() of sf package.\n\ndist &lt;- st_distance(shan_sf, shan_sf)\ndistmat &lt;- as.dist(dist)\n\nNotice that as.dist() is used to convert the data frame into matrix.\nNext, choicealpha() will be used to determine a suitable value for the mixing parameter alpha as shown in the code chunk below.\n\ncr &lt;- choicealpha(proxmat, distmat, range.alpha = seq(0, 1, 0.1), K=6, graph = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWith reference to the graphs above, alpha = 0.3 will be used as shown in the code chunk below.\n\nclustG &lt;- hclustgeo(proxmat, distmat, alpha = 0.3)\n\nNext, cutree() is used to derive the cluster objecct.\n\ngroups &lt;- as.factor(cutree(clustG, k=6))\n\nWe will then join back the group list with shan_sf polygon feature data frame by using the code chunk below.\n\nshan_sf_Gcluster &lt;- cbind(shan_sf, as.matrix(groups)) %&gt;%\n  rename(`CLUSTER` = `as.matrix.groups.`)\n\nWe can now plot the map of the newly delineated spatially constrained clusters.\n\nqtm(shan_sf_Gcluster, \"CLUSTER\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex08/In-class_Ex07-HCA.html#visual-interpretation-of-clusters",
    "href": "In-class_Ex/In-class_Ex08/In-class_Ex07-HCA.html#visual-interpretation-of-clusters",
    "title": "Hierarchical Clustering for Multivariate Geographically Referenced Data with R",
    "section": "Visual Interpretation of Clusters",
    "text": "Visual Interpretation of Clusters\n\nVisualising individual clustering variable\nCode chunk below is used to reveal the distribution of a clustering variable (i.e RADIO_PR) by cluster.\n\nggplot(data = shan_sf_ngeo_cluster,\n       aes(x = CLUSTER, y = RADIO_PR)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nThe boxplot reveals Cluster 3 displays the highest mean Radio Ownership Per Thousand Household. This is followed by Cluster 2, 1, 4, 6 and 5.\n\n\nMultivariate Visualisation\nPast studies shown that parallel coordinate plot can be used to reveal clustering variables by cluster very effectively. In the code chunk below, ggparcoord() of GGally package\n\nggparcoord(data = shan_sf_ngeo_cluster, \n           columns = c(17:21), \n           scale = \"globalminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of ICT Variables by Cluster\") +\n  facet_grid(~ CLUSTER) + \n  theme(axis.text.x = element_text(angle = 30))\n\n\n\n\n\n\n\n\nThe parallel coordinate plot above reveals that households in Cluster 4 townships tend to own the highest number of TV and mobile-phone. On the other hand, households in Cluster 5 tends to own the lowest of all the five ICT.\nNote that the scale argument of ggparcoor() provide several methods to scale the clustering variables. They are:\n\nstd: univariately, subtract mean and divide by standard deviation.\nrobust: univariately, subtract median and divide by median absolute deviation.\nuniminmax: univariately, scale so the minimum of the variable is zero, and the maximum is one.\nglobalminmax: no scaling is done; the range of the graphs is defined by the global minimum and the global maximum.\ncenter: use uniminmax to standardize vertical height, then center each variable at a value specified by the scaleSummary param.\ncenterObs: use uniminmax to standardize vertical height, then center each variable at the value of the observation specified by the centerObsID param\n\nThere is no one best scaling method to use. You should explore them and select the one that best meet your analysis need.\nLast but not least, we can also compute the summary statistics such as mean, median, sd, etc to complement the visual interpretation.\nIn the code chunk below, group_by() and summarise() of dplyr are used to derive mean values of the clustering variables.\n\nshan_sf_ngeo_cluster %&gt;% \n  st_set_geometry(NULL) %&gt;%\n  group_by(CLUSTER) %&gt;%\n  summarise(mean_RADIO_PR = mean(RADIO_PR),\n            mean_TV_PR = mean(TV_PR),\n            mean_LLPHONE_PR = mean(LLPHONE_PR),\n            mean_MPHONE_PR = mean(MPHONE_PR),\n            mean_COMPUTER_PR = mean(COMPUTER_PR))\n\n# A tibble: 6 × 6\n  CLUSTER mean_RADIO_PR mean_TV_PR mean_LLPHONE_PR mean_MPHONE_PR\n  &lt;chr&gt;           &lt;dbl&gt;      &lt;dbl&gt;           &lt;dbl&gt;          &lt;dbl&gt;\n1 1               221.        521.            44.2           246.\n2 2               237.        402.            23.9           134.\n3 3               300.        611.            52.2           392.\n4 4               196.        744.            99.0           651.\n5 5               124.        224.            38.0           132.\n6 6                98.6       499.            74.5           468.\n# ℹ 1 more variable: mean_COMPUTER_PR &lt;dbl&gt;"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex08/In-class_Ex08.html#loading-r-packages",
    "href": "In-class_Ex/In-class_Ex08/In-class_Ex08.html#loading-r-packages",
    "title": "Hierarchical Clustering for Multivariate Geographically Referenced Data",
    "section": "Loading R packages",
    "text": "Loading R packages"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex08/In-class_Ex08.html#data-import-and-prepatation",
    "href": "In-class_Ex/In-class_Ex08/In-class_Ex08.html#data-import-and-prepatation",
    "title": "Hierarchical Clustering for Multivariate Geographically Referenced Data",
    "section": "Data Import and Prepatation",
    "text": "Data Import and Prepatation\nImporting geospatial data into R environment\n\n\nReading layer `myanmar_township_boundaries' from data source \n  `D:\\tskam\\IS415_AY2024-25T1\\In-class_Ex\\In-class_Ex08\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 330 features and 14 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 92.17275 ymin: 9.671252 xmax: 101.1699 ymax: 28.54554\nGeodetic CRS:  WGS 84\n\n\n\n\nSimple feature collection with 55 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 96.15107 ymin: 19.29932 xmax: 101.1699 ymax: 24.15907\nGeodetic CRS:  WGS 84\nFirst 10 features:\n             ST ST_PCODE       DT   DT_PCODE        TS  TS_PCODE\n1  Shan (North)   MMR015  Mongmit MMR015D008   Mongmit MMR015017\n2  Shan (South)   MMR014 Taunggyi MMR014D001   Pindaya MMR014006\n3  Shan (South)   MMR014 Taunggyi MMR014D001   Ywangan MMR014007\n4  Shan (South)   MMR014 Taunggyi MMR014D001  Pinlaung MMR014009\n5  Shan (North)   MMR015  Mongmit MMR015D008    Mabein MMR015018\n6  Shan (South)   MMR014 Taunggyi MMR014D001     Kalaw MMR014005\n7  Shan (South)   MMR014 Taunggyi MMR014D001     Pekon MMR014010\n8  Shan (South)   MMR014 Taunggyi MMR014D001  Lawksawk MMR014008\n9  Shan (North)   MMR015  Kyaukme MMR015D003 Nawnghkio MMR015013\n10 Shan (North)   MMR015  Kyaukme MMR015D003   Kyaukme MMR015012\n                         geometry\n1  MULTIPOLYGON (((96.96001 23...\n2  MULTIPOLYGON (((96.7731 21....\n3  MULTIPOLYGON (((96.78483 21...\n4  MULTIPOLYGON (((96.49518 20...\n5  MULTIPOLYGON (((96.66306 24...\n6  MULTIPOLYGON (((96.49518 20...\n7  MULTIPOLYGON (((97.14738 19...\n8  MULTIPOLYGON (((96.94981 22...\n9  MULTIPOLYGON (((96.75648 22...\n10 MULTIPOLYGON (((96.95498 22...\n\n\nImporting aspatial data into R environment\nThe csv file will be import using read_csv function of readr package.\nThe code chunks used are shown below:\nThe imported InfoComm variables are extracted from The 2014 Myanmar Population and Housing Census Myanmar. The attribute data set is called ict. It is saved in R’s * tibble data.frame* format.\nThe code chunk below reveal the summary statistics of ict data.frame.\n\n\n District Pcode     District Name      Township Pcode     Township Name     \n Length:55          Length:55          Length:55          Length:55         \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n Total households     Radio         Television    Land line phone \n Min.   : 3318    Min.   :  115   Min.   :  728   Min.   :  20.0  \n 1st Qu.: 8711    1st Qu.: 1260   1st Qu.: 3744   1st Qu.: 266.5  \n Median :13685    Median : 2497   Median : 6117   Median : 695.0  \n Mean   :18369    Mean   : 4487   Mean   :10183   Mean   : 929.9  \n 3rd Qu.:23471    3rd Qu.: 6192   3rd Qu.:13906   3rd Qu.:1082.5  \n Max.   :82604    Max.   :30176   Max.   :62388   Max.   :6736.0  \n  Mobile phone      Computer      Internet at home\n Min.   :  150   Min.   :  20.0   Min.   :   8.0  \n 1st Qu.: 2037   1st Qu.: 121.0   1st Qu.:  88.0  \n Median : 3559   Median : 244.0   Median : 316.0  \n Mean   : 6470   Mean   : 575.5   Mean   : 760.2  \n 3rd Qu.: 7177   3rd Qu.: 507.0   3rd Qu.: 630.5  \n Max.   :48461   Max.   :6705.0   Max.   :9746.0  \n\n\nThere are a total of eleven fields and 55 observation in the tibble data.frame.\nDerive new variables using dplyr package\nThe unit of measurement of the values are number of household. Using these values directly will be bias by the underlying total number of households. In general, the townships with relatively higher total number of households will also have higher number of households owning radio, TV, etc.\nIn order to overcome this problem, we will derive the penetration rate of each ICT variable by using the code chunk below.\nLet us review the summary statistics of the newly derived penetration rates using the code chunk below.\n\n\n   DT_PCODE              DT              TS_PCODE              TS           \n Length:55          Length:55          Length:55          Length:55         \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n TT_HOUSEHOLDS       RADIO             TV           LLPHONE      \n Min.   : 3318   Min.   :  115   Min.   :  728   Min.   :  20.0  \n 1st Qu.: 8711   1st Qu.: 1260   1st Qu.: 3744   1st Qu.: 266.5  \n Median :13685   Median : 2497   Median : 6117   Median : 695.0  \n Mean   :18369   Mean   : 4487   Mean   :10183   Mean   : 929.9  \n 3rd Qu.:23471   3rd Qu.: 6192   3rd Qu.:13906   3rd Qu.:1082.5  \n Max.   :82604   Max.   :30176   Max.   :62388   Max.   :6736.0  \n     MPHONE         COMPUTER         INTERNET         RADIO_PR     \n Min.   :  150   Min.   :  20.0   Min.   :   8.0   Min.   : 21.05  \n 1st Qu.: 2037   1st Qu.: 121.0   1st Qu.:  88.0   1st Qu.:138.95  \n Median : 3559   Median : 244.0   Median : 316.0   Median :210.95  \n Mean   : 6470   Mean   : 575.5   Mean   : 760.2   Mean   :215.68  \n 3rd Qu.: 7177   3rd Qu.: 507.0   3rd Qu.: 630.5   3rd Qu.:268.07  \n Max.   :48461   Max.   :6705.0   Max.   :9746.0   Max.   :484.52  \n     TV_PR         LLPHONE_PR       MPHONE_PR       COMPUTER_PR    \n Min.   :116.0   Min.   :  2.78   Min.   : 36.42   Min.   : 3.278  \n 1st Qu.:450.2   1st Qu.: 22.84   1st Qu.:190.14   1st Qu.:11.832  \n Median :517.2   Median : 37.59   Median :305.27   Median :18.970  \n Mean   :509.5   Mean   : 51.09   Mean   :314.05   Mean   :24.393  \n 3rd Qu.:606.4   3rd Qu.: 69.72   3rd Qu.:428.43   3rd Qu.:29.897  \n Max.   :842.5   Max.   :181.49   Max.   :735.43   Max.   :92.402  \n  INTERNET_PR     \n Min.   :  1.041  \n 1st Qu.:  8.617  \n Median : 22.829  \n Mean   : 30.644  \n 3rd Qu.: 41.281  \n Max.   :117.985  \n\n\nNotice that six new fields have been added into the data.frame. They are RADIO_PR, TV_PR, LLPHONE_PR, MPHONE_PR, COMPUTER_PR, and INTERNET_PR."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex08/In-class_Ex08.html#exploratory-data-analysis-eda",
    "href": "In-class_Ex/In-class_Ex08/In-class_Ex08.html#exploratory-data-analysis-eda",
    "title": "Hierarchical Clustering for Multivariate Geographically Referenced Data",
    "section": "Exploratory Data Analysis (EDA)",
    "text": "Exploratory Data Analysis (EDA)\nEDA using statistical graphics\nWe can plot the distribution of the variables (i.e. Number of households with radio) by using appropriate Exploratory Data Analysis (EDA) as shown in the code chunk below.\nHistogram is useful to identify the overall distribution of the data values (i.e. left skew, right skew or normal distribution)\n\n\n\n\n\n\n\n\n\nBoxplot is useful to detect if there are outliers.\n\n\n\n\n\n\n\n\n\nNext, we will also plotting the distribution of the newly derived variables (i.e. Radio penetration rate) by using the code chunk below.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat can you observed from the distributions reveal in the histogram and boxplot.\nIn the figure below, multiple histograms are plotted to reveal the distribution of the selected variables in the ict_derived data.frame.\n\n\n\n\n\n\n\n\n\nThe code chunks below are used to create the data visualisation. They consist of two main parts. First, we will create the individual histograms using the code chunk below.\n\nradio &lt;- ggplot(data=ict_derived, \n             aes(x= `RADIO_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\")\n\ntv &lt;- ggplot(data=ict_derived, \n             aes(x= `TV_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\")\n\nllphone &lt;- ggplot(data=ict_derived, \n             aes(x= `LLPHONE_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\")\n\nmphone &lt;- ggplot(data=ict_derived, \n             aes(x= `MPHONE_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\")\n\ncomputer &lt;- ggplot(data=ict_derived, \n             aes(x= `COMPUTER_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\")\n\ninternet &lt;- ggplot(data=ict_derived, \n             aes(x= `INTERNET_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\")\n\nNext, the ggarrange() function of ggpubr package is used to group these histograms together.\n\nggarrange(radio, tv, llphone, mphone, computer, internet, \n          ncol = 3, \n          nrow = 2)\n\nEDA using choropleth map\nJoining geospatial data with aspatial data\nBefore we can prepare the choropleth map, we need to combine both the geospatial data object (i.e. shan_sf) and aspatial data.frame object (i.e. ict_derived) into one. This will be performed by using the left_join function of dplyr package. The shan_sf simple feature data.frame will be used as the base data object and the ict_derived data.frame will be used as the join table.\nThe code chunks below is used to perform the task. The unique identifier used to join both data objects is TS_PCODE.\nThe message above shows that TS_CODE field is the common field used to perform the left-join.\nIt is important to note that there is no new output data been created. Instead, the data fields from ict_derived data frame are now updated into the data frame of shan_sf.\nPreparing a choropleth map\nTo have a quick look at the distribution of Radio penetration rate of Shan State at township level, a choropleth map will be prepared.\nThe code chunks below are used to prepare the choroplethby using the qtm() function of tmap package.\n\n\n\n\n\n\n\n\n\nIn order to reveal the distribution shown in the choropleth map above are bias to the underlying total number of households at the townships, we will create two choropleth maps, one for the total number of households (i.e. TT_HOUSEHOLDS.map) and one for the total number of household with Radio (RADIO.map) by using the code chunk below.\n\n\n\n\n\n\n\n\n\nNotice that the choropleth maps above clearly show that townships with relatively larger number ot households are also showing relatively higher number of radio ownership.\nNow let us plot the choropleth maps showing the dsitribution of total number of households and Radio penetration rate by using the code chunk below.\n\n\n\n\n\n\n\n\n\nCan you identify the differences?"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex08/In-class_Ex08.html#correlation-analysis",
    "href": "In-class_Ex/In-class_Ex08/In-class_Ex08.html#correlation-analysis",
    "title": "Hierarchical Clustering for Multivariate Geographically Referenced Data",
    "section": "Correlation Analysis",
    "text": "Correlation Analysis\nBefore we perform cluster analysis, it is important for us to ensure that the cluster variables are not highly correlated.\nIn this section, you will learn how to use corrplot.mixed() function of corrplot package to visualise and analyse the correlation of the input variables.\n\nThe correlation plot above shows that COMPUTER_PR and INTERNET_PR are highly correlated. This suggest that only one of them should be used in the cluster analysis instead of both."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex08/In-class_Ex08.html#hierarchy-cluster-analysis",
    "href": "In-class_Ex/In-class_Ex08/In-class_Ex08.html#hierarchy-cluster-analysis",
    "title": "Hierarchical Clustering for Multivariate Geographically Referenced Data",
    "section": "Hierarchy Cluster Analysis",
    "text": "Hierarchy Cluster Analysis\nIn this section, you will learn how to perform hierarchical cluster analysis. The analysis consists of four major steps:\nExtracting clustering variables\nThe code chunk below will be used to extract the clustering variables from the shan_sf simple feature object into data.frame.\n\n\n        TS.x RADIO_PR    TV_PR LLPHONE_PR MPHONE_PR COMPUTER_PR\n1    Mongmit 286.1852 554.1313   35.30618  260.6944    12.15939\n2    Pindaya 417.4647 505.1300   19.83584  162.3917    12.88190\n3    Ywangan 484.5215 260.5734   11.93591  120.2856     4.41465\n4   Pinlaung 231.6499 541.7189   28.54454  249.4903    13.76255\n5     Mabein 449.4903 708.6423   72.75255  392.6089    16.45042\n6      Kalaw 280.7624 611.6204   42.06478  408.7951    29.63160\n7      Pekon 318.6118 535.8494   39.83270  214.8476    18.97032\n8   Lawksawk 387.1017 630.0035   31.51366  320.5686    21.76677\n9  Nawnghkio 349.3359 547.9456   38.44960  323.0201    15.76465\n10   Kyaukme 210.9548 601.1773   39.58267  372.4930    30.94709\n\n\nNotice that the final clustering variables list does not include variable INTERNET_PR because it is highly correlated with variable COMPUTER_PR.\nNext, we need to change the rows by township name instead of row number by using the code chunk below\n\n\n               TS.x RADIO_PR    TV_PR LLPHONE_PR MPHONE_PR COMPUTER_PR\nMongmit     Mongmit 286.1852 554.1313   35.30618  260.6944    12.15939\nPindaya     Pindaya 417.4647 505.1300   19.83584  162.3917    12.88190\nYwangan     Ywangan 484.5215 260.5734   11.93591  120.2856     4.41465\nPinlaung   Pinlaung 231.6499 541.7189   28.54454  249.4903    13.76255\nMabein       Mabein 449.4903 708.6423   72.75255  392.6089    16.45042\nKalaw         Kalaw 280.7624 611.6204   42.06478  408.7951    29.63160\nPekon         Pekon 318.6118 535.8494   39.83270  214.8476    18.97032\nLawksawk   Lawksawk 387.1017 630.0035   31.51366  320.5686    21.76677\nNawnghkio Nawnghkio 349.3359 547.9456   38.44960  323.0201    15.76465\nKyaukme     Kyaukme 210.9548 601.1773   39.58267  372.4930    30.94709\n\n\nNotice that the row number has been replaced into the township name.\nNow, we will delete the TS.x field by using the code chunk below.\n\n\n          RADIO_PR    TV_PR LLPHONE_PR MPHONE_PR COMPUTER_PR\nMongmit   286.1852 554.1313   35.30618  260.6944    12.15939\nPindaya   417.4647 505.1300   19.83584  162.3917    12.88190\nYwangan   484.5215 260.5734   11.93591  120.2856     4.41465\nPinlaung  231.6499 541.7189   28.54454  249.4903    13.76255\nMabein    449.4903 708.6423   72.75255  392.6089    16.45042\nKalaw     280.7624 611.6204   42.06478  408.7951    29.63160\nPekon     318.6118 535.8494   39.83270  214.8476    18.97032\nLawksawk  387.1017 630.0035   31.51366  320.5686    21.76677\nNawnghkio 349.3359 547.9456   38.44960  323.0201    15.76465\nKyaukme   210.9548 601.1773   39.58267  372.4930    30.94709\n\n\nData Standardisation\nIn general, multiple variables will be used in cluster analysis. It is not unusual their values range are different. In order to avoid the cluster analysis result is baised to clustering variables with large values, it is useful to standardise the input variables before performing cluster analysis.\nMin-Max standardisation\nIn the code chunk below, normalize() of heatmaply package is used to stadardisation the clustering variables by using Min-Max method. The summary() is then used to display the summary statistics of the standardised clustering variables.\n\n\n    RADIO_PR          TV_PR          LLPHONE_PR       MPHONE_PR     \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.2544   1st Qu.:0.4600   1st Qu.:0.1123   1st Qu.:0.2199  \n Median :0.4097   Median :0.5523   Median :0.1948   Median :0.3846  \n Mean   :0.4199   Mean   :0.5416   Mean   :0.2703   Mean   :0.3972  \n 3rd Qu.:0.5330   3rd Qu.:0.6750   3rd Qu.:0.3746   3rd Qu.:0.5608  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n  COMPUTER_PR     \n Min.   :0.00000  \n 1st Qu.:0.09598  \n Median :0.17607  \n Mean   :0.23692  \n 3rd Qu.:0.29868  \n Max.   :1.00000  \n\n\nNotice that the values range of the Min-max standardised clustering variables are 0-1 now.\nZ-score standardisation\nZ-score standardisation can be performed easily by using scale() of Base R. The code chunk below will be used to stadardisation the clustering variables by using Z-score method.\n\n\n            vars  n mean sd median trimmed  mad   min  max range  skew kurtosis\nRADIO_PR       1 55    0  1  -0.04   -0.06 0.94 -1.85 2.55  4.40  0.48    -0.27\nTV_PR          2 55    0  1   0.05    0.04 0.78 -2.47 2.09  4.56 -0.38    -0.23\nLLPHONE_PR     3 55    0  1  -0.33   -0.15 0.68 -1.19 3.20  4.39  1.37     1.49\nMPHONE_PR      4 55    0  1  -0.05   -0.06 1.01 -1.58 2.40  3.98  0.48    -0.34\nCOMPUTER_PR    5 55    0  1  -0.26   -0.18 0.64 -1.03 3.31  4.34  1.80     2.96\n              se\nRADIO_PR    0.13\nTV_PR       0.13\nLLPHONE_PR  0.13\nMPHONE_PR   0.13\nCOMPUTER_PR 0.13\n\n\nNotice the mean and standard deviation of the Z-score standardised clustering variables are 0 and 1 respectively.\nNote: describe() of psych package is used here instead of summary() of Base R because the earlier provides standard deviation.\nWarning: Z-score standardisation method should only be used if we would assume all variables come from some normal distribution.\nVisualising the standardised clustering variables\nBeside reviewing the summary statistics of the standardised clustering variables, it is also a good practice to visualise their distribution graphical.\nThe code chunk below plot the scaled Radio_PR field.\n\n\n\n\n\n\n\n\n\n\nWhat statistical conclusion can you draw from the histograms above?\n\n\n\n\n\n\n\n\n\n\nComputing proximity matrix\nIn R, many packages provide functions to calculate distance matrix. We will compute the proximity matrix by using dist() of R.\ndist() supports six distance proximity calculations, they are: euclidean, maximum, manhattan, canberra, binary and minkowski. The default is euclidean proximity matrix.\nThe code chunk below is used to compute the proximity matrix using euclidean method.\nThe code chunk below can then be used to list the content of proxmat for visual inspection.\n\nproxmat\n\nComputing hierarchical clustering\nIn R, there are several packages provide hierarchical clustering function. In this hands-on exercise, hclust() of R stats will be used.\nhclust() employed agglomeration method to compute the cluster. Eight clustering algorithms are supported, they are: ward.D, ward.D2, single, complete, average(UPGMA), mcquitty(WPGMA), median(WPGMC) and centroid(UPGMC).\nThe code chunk below performs hierarchical cluster analysis using ward.D method. The hierarchical clustering output is stored in an object of class hclust which describes the tree produced by the clustering process.\nWe can then plot the tree by using plot() of R Graphics as shown in the code chunk below.\n\n\n\n\n\n\n\n\n\nSelecting the optimal clustering algorithm\nOne of the challenge in performing hierarchical clustering is to identify stronger clustering structures. The issue can be solved by using use agnes() function of cluster package. It functions like hclus(), however, with the agnes() function you can also get the agglomerative coefficient, which measures the amount of clustering structure found (values closer to 1 suggest strong clustering structure).\nThe code chunk below will be used to compute the agglomerative coefficients of all hierarchical clustering algorithms.\n\n\n  average    single  complete      ward \n0.8131144 0.6628705 0.8950702 0.9427730 \n\n\nWith reference to the output above, we can see that Ward’s method provides the strongest clustering structure among the four methods assessed. Hence, in the subsequent analysis, only Ward’s method will be used.\nDetermining Optimal Clusters\nAnother technical challenge face by data analyst in performing clustering analysis is to determine the optimal clusters to retain.\nThere are three commonly used methods to determine the optimal clusters, they are:\n\nElbow Method\nAverage Silhouette Method\nGap Statistic Method\n\nGap Statistic Method\nThe gap statistic compares the total within intra-cluster variation for different values of k with their expected values under null reference distribution of the data. The estimate of the optimal clusters will be value that maximize the gap statistic (i.e., that yields the largest gap statistic). This means that the clustering structure is far away from the random uniform distribution of points.\nTo compute the gap statistic, clusGap() of cluster package will be used.\n\n\nClustering Gap statistic [\"clusGap\"] from call:\nclusGap(x = shan_ict, FUNcluster = hcut, K.max = 10, B = 50, nstart = 25)\nB=50 simulated reference sets, k = 1..10; spaceH0=\"scaledPCA\"\n --&gt; Number of clusters (method 'firstmax'): 1\n          logW   E.logW       gap     SE.sim\n [1,] 8.407129 8.680794 0.2736651 0.04460994\n [2,] 8.130029 8.350712 0.2206824 0.03880130\n [3,] 7.992265 8.202550 0.2102844 0.03362652\n [4,] 7.862224 8.080655 0.2184311 0.03784781\n [5,] 7.756461 7.978022 0.2215615 0.03897071\n [6,] 7.665594 7.887777 0.2221833 0.03973087\n [7,] 7.590919 7.806333 0.2154145 0.04054939\n [8,] 7.526680 7.731619 0.2049390 0.04198644\n [9,] 7.458024 7.660795 0.2027705 0.04421874\n[10,] 7.377412 7.593858 0.2164465 0.04540947\n\n\nAlso note that the hcut function used is from factoextra package.\nNext, we can visualise the plot by using fviz_gap_stat() of factoextra package.\n\n\n\n\n\n\n\n\n\nWith reference to the gap statistic graph above, the recommended number of cluster to retain is 1. However, it is not logical to retain only one cluster. By examine the gap statistic graph, the 6-cluster gives the largest gap statistic and should be the next best cluster to pick.\nNote: In addition to these commonly used approaches, the NbClust package, published by Charrad et al., 2014, provides 30 indices for determining the relevant number of clusters and proposes to users the best clustering scheme from the different results obtained by varying all combinations of number of clusters, distance measures, and clustering methods.\nInterpreting the dendrograms\nIn the dendrogram displayed above, each leaf corresponds to one observation. As we move up the tree, observations that are similar to each other are combined into branches, which are themselves fused at a higher height.\nThe height of the fusion, provided on the vertical axis, indicates the (dis)similarity between two observations. The higher the height of the fusion, the less similar the observations are. Note that, conclusions about the proximity of two observations can be drawn only based on the height where branches containing those two observations first are fused. We cannot use the proximity of two observations along the horizontal axis as a criteria of their similarity.\nIt’s also possible to draw the dendrogram with a border around the selected clusters by using rect.hclust() of R stats. The argument border is used to specify the border colors for the rectangles.\n\n\n\n\n\n\n\n\n\nVisually-driven hierarchical clustering analysis\nIn this section, we will learn how to perform visually-driven hiearchical clustering analysis by using heatmaply package.\nWith heatmaply, we are able to build both highly interactive cluster heatmap or static cluster heatmap.\nTransforming the data frame into a matrix\nThe data was loaded into a data frame, but it has to be a data matrix to make your heatmap.\nThe code chunk below will be used to transform shan_ict data frame into a data matrix.\nPlotting interactive cluster heatmap using heatmaply()\nIn the code chunk below, the heatmaply() of heatmaply package is used to build an interactive cluster heatmap.\n\n\n\n\n\n\nMapping the clusters formed\nWith closed examination of the dendragram above, we have decided to retain six clusters.\ncutree() of R Base will be used in the code chunk below to derive a 6-cluster model.\nThe output is called groups. It is a list object.\nIn order to visualise the clusters, the groups object need to be appended onto shan_sf simple feature object.\nThe code chunk below form the join in three steps:\n\nthe groups list object will be converted into a matrix;\ncbind() is used to append groups matrix onto shan_sf to produce an output simple feature object called shan_sf_cluster; and\nrename of dplyr package is used to rename as.matrix.groups field as CLUSTER.\n\nNext, qtm() of tmap package is used to plot the choropleth map showing the cluster formed.\n\n\n\n\n\n\n\n\n\nThe choropleth map above reveals the clusters are very fragmented. The is one of the major limitation when non-spatial clustering algorithm such as hierarchical cluster analysis method is used."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex09/In-class_Ex09-SCCA.html#loading-r-packages",
    "href": "In-class_Ex/In-class_Ex09/In-class_Ex09-SCCA.html#loading-r-packages",
    "title": "In-class Exercise 9: Geographic Segmentation with Spatial Constrained Cluster Analysis",
    "section": "Loading R packages",
    "text": "Loading R packages\n\n\n\n\npacman::p_load(spdep, sp, tmap, sf, ClustGeo, \n               cluster, factoextra, NbClust,\n               tidyverse, GGally)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex09/In-class_Ex09-SCCA.html#importing-the-data",
    "href": "In-class_Ex/In-class_Ex09/In-class_Ex09-SCCA.html#importing-the-data",
    "title": "In-class Exercise 9: Geographic Segmentation with Spatial Constrained Cluster Analysis",
    "section": "Importing the data",
    "text": "Importing the data\n\n\nshan_sf &lt;- read_rds(\"data/rds/shan_sf.rds\")\nshan_ict &lt;- read_rds(\"data/rds/shan_ict.rds\")\nshan_sf_cluster &lt;- read_rds(\"data/rds/shan_sf_cluster.rds\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex09/In-class_Ex09-SCCA.html#visualising-the-cluster-delineated",
    "href": "In-class_Ex/In-class_Ex09/In-class_Ex09-SCCA.html#visualising-the-cluster-delineated",
    "title": "In-class Exercise 9: Geographic Segmentation with Spatial Constrained Cluster Analysis",
    "section": "Visualising the cluster delineated",
    "text": "Visualising the cluster delineated\nNext, qtm() of tmap package is used to plot the choropleth map showing the cluster formed.\n\n\nqtm(shan_sf_cluster, \"CLUSTER\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex09/In-class_Ex09-SCCA.html#spatially-constrained-clustering-skater-approach",
    "href": "In-class_Ex/In-class_Ex09/In-class_Ex09-SCCA.html#spatially-constrained-clustering-skater-approach",
    "title": "In-class Exercise 9: Geographic Segmentation with Spatial Constrained Cluster Analysis",
    "section": "Spatially Constrained Clustering: SKATER approach",
    "text": "Spatially Constrained Clustering: SKATER approach\nStep 1\n\nshan.nb &lt;- poly2nb(shan_sf)\nsummary(shan.nb)\n\nNeighbour list object:\nNumber of regions: 55 \nNumber of nonzero links: 264 \nPercentage nonzero weights: 8.727273 \nAverage number of links: 4.8 \nLink number distribution:\n\n 2  3  4  5  6  7  8  9 \n 5  9  7 21  4  3  5  1 \n5 least connected regions:\n3 5 7 9 47 with 2 links\n1 most connected region:\n8 with 9 links"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex09/In-class_Ex09-SCCA.html#visualising-the-neighbours",
    "href": "In-class_Ex/In-class_Ex09/In-class_Ex09-SCCA.html#visualising-the-neighbours",
    "title": "In-class Exercise 9: Geographic Segmentation with Spatial Constrained Cluster Analysis",
    "section": "Visualising the neighbours",
    "text": "Visualising the neighbours\n\n\n\nplot(st_geometry(shan_sf), \n     border=grey(.5))\npts &lt;- st_coordinates(st_centroid(shan_sf))\nplot(shan.nb, \n     pts, \n     col=\"blue\", \n     add=TRUE)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex09/In-class_Ex09-SCCA.html#computing-minimum-spanning-tree",
    "href": "In-class_Ex/In-class_Ex09/In-class_Ex09-SCCA.html#computing-minimum-spanning-tree",
    "title": "In-class Exercise 9: Geographic Segmentation with Spatial Constrained Cluster Analysis",
    "section": "Computing minimum spanning tree",
    "text": "Computing minimum spanning tree\n\nlcosts &lt;- nbcosts(shan.nb, shan_ict)\n\n\nshan.w &lt;- nb2listw(shan.nb, \n                   lcosts, \n                   style=\"B\")\nsummary(shan.w)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 55 \nNumber of nonzero links: 264 \nPercentage nonzero weights: 8.727273 \nAverage number of links: 4.8 \nLink number distribution:\n\n 2  3  4  5  6  7  8  9 \n 5  9  7 21  4  3  5  1 \n5 least connected regions:\n3 5 7 9 47 with 2 links\n1 most connected region:\n8 with 9 links\n\nWeights style: B \nWeights constants summary:\n   n   nn       S0       S1        S2\nB 55 3025 76267.65 58260785 522016004\n\n\n\nshan.mst &lt;- mstree(shan.w)\n\n\nclass(shan.mst)\n\n[1] \"mst\"    \"matrix\"\n\n\n\ndim(shan.mst)\n\n[1] 54  3\n\n\n\nplot(st_geometry(shan_sf), \n     border=gray(.5))\nplot.mst(shan.mst, \n         pts, \n         col=\"blue\", \n         cex.lab=0.7, \n         cex.circles=0.005, \n         add=TRUE)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex09/In-class_Ex09-SCCA.html#computing-spatially-constrained-clusters-using-skater-method",
    "href": "In-class_Ex/In-class_Ex09/In-class_Ex09-SCCA.html#computing-spatially-constrained-clusters-using-skater-method",
    "title": "In-class Exercise 9: Geographic Segmentation with Spatial Constrained Cluster Analysis",
    "section": "Computing spatially constrained clusters using SKATER method",
    "text": "Computing spatially constrained clusters using SKATER method\n\nThe codeThe skater treeThe code to plot skater tree\n\n\n\n\nskater.clust6 &lt;- skater(edges = shan.mst[,1:2], \n                 data = shan_ict, \n                 method = \"euclidean\", \n                 ncuts = 5)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplot(st_geometry(shan_sf), \n     border=gray(.5))\nplot(skater.clust6, \n     pts, \n     cex.lab=.7,\n     groups.colors=c(\"red\",\"green\",\"blue\", \"brown\", \"pink\"),\n     cex.circles=0.005, \n     add=TRUE)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex09/In-class_Ex09-SCCA.html#visualising-the-clusters-in-choropleth-map",
    "href": "In-class_Ex/In-class_Ex09/In-class_Ex09-SCCA.html#visualising-the-clusters-in-choropleth-map",
    "title": "In-class Exercise 9: Geographic Segmentation with Spatial Constrained Cluster Analysis",
    "section": "Visualising the clusters in choropleth map",
    "text": "Visualising the clusters in choropleth map\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngroups_mat &lt;- as.matrix(skater.clust6$groups)\nshan_sf_spatialcluster &lt;- cbind(shan_sf_cluster, as.factor(groups_mat)) %&gt;%\n  rename(`skater_CLUSTER`=`as.factor.groups_mat.`)\nqtm(shan_sf_spatialcluster, \"skater_CLUSTER\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex09/In-class_Ex09-SCCA.html#section",
    "href": "In-class_Ex/In-class_Ex09/In-class_Ex09-SCCA.html#section",
    "title": "In-class Exercise 9: Geographic Segmentation with Spatial Constrained Cluster Analysis",
    "section": ":::",
    "text": ":::\nPlotting the cluster maps\n\nThe codeThe Plot\n\n\n\n\nhclust.map &lt;- qtm(shan_sf_cluster,\n                  \"CLUSTER\") + \n  tm_borders(alpha = 0.5) +\n  tm_layout(legend.position = c(0.8, 0.6))\n\nshclust.map &lt;- qtm(shan_sf_spatialcluster,\n                   \"skater_CLUSTER\") + \n  tm_borders(alpha = 0.5) +\n  tm_layout(legend.position = c(0.7, 0.6))\n\ntmap_arrange(hclust.map, shclust.map,\n             asp=NA, ncol=2)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex09/In-class_Ex09-SCCA.html#spatially-constrained-clustering-clustgeo-method",
    "href": "In-class_Ex/In-class_Ex09/In-class_Ex09-SCCA.html#spatially-constrained-clustering-clustgeo-method",
    "title": "In-class Exercise 9: Geographic Segmentation with Spatial Constrained Cluster Analysis",
    "section": "Spatially Constrained Clustering: ClustGeo Method",
    "text": "Spatially Constrained Clustering: ClustGeo Method\n\nComputing spatial distance matrixThe cluster graphsThe codeSaving clustGeo output\n\n\nIn the code chunk below, st_distance() of sf package is used to compute the distance matrix.\n\n\ndist &lt;- st_distance(shan_sf, shan_sf)\ndistmat &lt;- as.dist(dist)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncr &lt;- choicealpha(proxmat, distmat, \n                  range.alpha = seq(0, 1, 0.1), \n                  K=6, graph = TRUE)\n\n\n\n\n\n\nclustG &lt;- hclustgeo(proxmat, distmat, alpha = 0.2)\ngroups &lt;- as.factor(cutree(clustG, k=6))\nshan_sf_clustGeo &lt;- cbind(shan_sf, \n                          as.matrix(groups)) %&gt;%\n  rename(`clustGeo` = `as.matrix.groups.`)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex09/In-class_Ex09-SCCA.html#visualising-the-cluster",
    "href": "In-class_Ex/In-class_Ex09/In-class_Ex09-SCCA.html#visualising-the-cluster",
    "title": "In-class Exercise 9: Geographic Segmentation with Spatial Constrained Cluster Analysis",
    "section": "Visualising the cluster",
    "text": "Visualising the cluster\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggparcoord(data = shan_sf_clustGeo, \n           columns = c(17:21), \n           scale = \"globalminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of ICT Variables by Cluster\") +\n  facet_grid(~ clustGeo) + \n  theme(axis.text.x = element_text(angle = 30))"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex09/In-class_Ex09-SCCA.html#spatially-constrained-clustering-skater-method",
    "href": "In-class_Ex/In-class_Ex09/In-class_Ex09-SCCA.html#spatially-constrained-clustering-skater-method",
    "title": "In-class Exercise 9: Geographic Segmentation with Spatial Constrained Cluster Analysis",
    "section": "Spatially Constrained Clustering: SKATER method",
    "text": "Spatially Constrained Clustering: SKATER method\nStep 1: Computing nearest neighbours\n\n\n\n\nshan.nb &lt;- poly2nb(shan_sf)\nsummary(shan.nb)\n\nNeighbour list object:\nNumber of regions: 55 \nNumber of nonzero links: 264 \nPercentage nonzero weights: 8.727273 \nAverage number of links: 4.8 \nLink number distribution:\n\n 2  3  4  5  6  7  8  9 \n 5  9  7 21  4  3  5  1 \n5 least connected regions:\n3 5 7 9 47 with 2 links\n1 most connected region:\n8 with 9 links"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex09/In-class_Ex09-SCCA.html#characterising-the-clusters",
    "href": "In-class_Ex/In-class_Ex09/In-class_Ex09-SCCA.html#characterising-the-clusters",
    "title": "In-class Exercise 9: Geographic Segmentation with Spatial Constrained Cluster Analysis",
    "section": "Characterising the clusters",
    "text": "Characterising the clusters\n\nThe plotThe code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggparcoord(data = shan_sf_clustGeo, \n           columns = c(17:21), \n           scale = \"globalminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of ICT Variables by Cluster\") +\n  facet_grid(~ clustGeo) + \n  theme(axis.text.x = element_text(angle = 30))"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex09/In-class_Ex09-SCCA.html#conventional-hierarchical-clustering",
    "href": "In-class_Ex/In-class_Ex09/In-class_Ex09-SCCA.html#conventional-hierarchical-clustering",
    "title": "In-class Exercise 9: Geographic Segmentation with Spatial Constrained Cluster Analysis",
    "section": "Conventional Hierarchical Clustering",
    "text": "Conventional Hierarchical Clustering\n\nHierachical clusteringAppend to the geospatial dataThe dendrogramCluster map\n\n\n\n\nproxmat &lt;- dist(shan_ict, method = 'euclidean')\nhclust_ward &lt;- hclust(proxmat, method = 'ward.D')\ngroups &lt;- as.factor(cutree(hclust_ward, k=6))\n\n\n\n\n\n\nshan_sf_cluster &lt;- cbind(shan_sf, \n                         as.matrix(groups)) %&gt;%\n  rename(`CLUSTER`=`as.matrix.groups.`) %&gt;%\n  select(-c(3:4, 7:9)) %&gt;%\n  rename(TS = TS.x)\n\n\n\n\n\n\nplot(hclust_ward, cex = 0.6)\nrect.hclust(hclust_ward, k = 6, border = 2:5)\n\n\n\n\n\n\n\n\n\n\n\n\n\nqtm(shan_sf_cluster, \"CLUSTER\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex10/In-class_Ex10.html#getting-started",
    "href": "In-class_Ex/In-class_Ex10/In-class_Ex10.html#getting-started",
    "title": "In-class Exercise 10: Calibrating Hedonic Pricing Model for Private Highrise Property with MLR methods",
    "section": "Getting Started",
    "text": "Getting Started\n\n\npacman::p_load(olsrr, ggstatsplot, sf, \n               tmap, tidyverse, gtsummary,\n               performance, see, sfdep)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex10/In-class_Ex10.html#importing-the-data",
    "href": "In-class_Ex/In-class_Ex10/In-class_Ex10.html#importing-the-data",
    "title": "In-class Exercise 10: Calibrating Hedonic Pricing Model for Private Highrise Property with MLR methods",
    "section": "Importing the data",
    "text": "Importing the data\n\nThe taskThe code chunk\n\n\nUse appropriate tidyverse and sf functions to import Condo_resale_2015.csv, mpsz.rds and condo_resale_sf.rds into RStudio environment.\n\n\n\n\ncondo_resale &lt;- read_csv(\"data/aspatial/Condo_resale_2015.csv\")\n\nmpsz &lt;- read_rds(\"data/rds/mpsz.rds\")\n\ncondo_resale_sf &lt;- read_rds(\n  \"data/rds/condo_resale_sf.rds\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex10/In-class_Ex10.html#correlation-analysis---ggstatsplot-methods",
    "href": "In-class_Ex/In-class_Ex10/In-class_Ex10.html#correlation-analysis---ggstatsplot-methods",
    "title": "In-class Exercise 10: Calibrating Hedonic Pricing Model for Private Highrise Property with MLR methods",
    "section": "Correlation Analysis - ggstatsplot methods",
    "text": "Correlation Analysis - ggstatsplot methods\n\nThe code chunkThe plot\n\n\nCorrelation matrix is an effective graphical method for checking if there are pair independent variables with high correlation. In the code chunk below, ggcorrmat() of ggstatsplot is used to plot the correlation matrix.\n\n\nggcorrmat(condo_resale[, 5:23])"
  },
  {
    "objectID": "lesson/Lesson11/Lesson11-gwr.html",
    "href": "lesson/Lesson11/Lesson11-gwr.html",
    "title": "Lesson 11: Geographically Weighted Regression",
    "section": "",
    "text": "What is Spatial Non-stationary\nIntroducing Geographically Weighted Regression\n\nWeighting functions (kernel)\nWeighting schemes\nBandwidth\n\nInterpreting and Visualising"
  },
  {
    "objectID": "lesson/Lesson12/Lesson12-GWRF.html",
    "href": "lesson/Lesson12/Lesson12-GWRF.html",
    "title": "Lesson 12: Geographically Weighted Predictive Modelling",
    "section": "",
    "text": "What is Predictive Modelling?\nWhat is Geospatial Predictive Modelling\nIntroducing Recursive Partitioning\nAdvanced Recursive Partitioning: Random Forest\nIntroducing Geographically Weighted Random Forest"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex10/In-class_Ex10-gwr.html",
    "href": "In-class_Ex/In-class_Ex10/In-class_Ex10-gwr.html",
    "title": "Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method",
    "section": "",
    "text": "pacman::p_load(olsrr, ggstatsplot, sf,\n               spdep, GWmodel, tmap,\n               tidyverse, performance,\n               see, sfdep)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex10/In-class_Ex10-gwr.html#getting-started",
    "href": "In-class_Ex/In-class_Ex10/In-class_Ex10-gwr.html#getting-started",
    "title": "Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method",
    "section": "",
    "text": "pacman::p_load(olsrr, ggstatsplot, sf,\n               spdep, GWmodel, tmap,\n               tidyverse, performance,\n               see, sfdep)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex10/In-class_Ex10-gwr.html#importing-the-data",
    "href": "In-class_Ex/In-class_Ex10/In-class_Ex10-gwr.html#importing-the-data",
    "title": "Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method",
    "section": "Importing the data",
    "text": "Importing the data\n\nURA Master Plan 2014 planning subzone boundary\n\ncondo_resale &lt;- read_csv(\"data/aspatial/Condo_resale_2015.csv\")\n\n\nmpsz &lt;- read_rds(\"data/rds/mpsz.rds\")\n\n\ncondo_resale_sf &lt;- read_rds(\n  \"data/rds/condo_resale_sf.rds\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex10/In-class_Ex10-gwr.html#section",
    "href": "In-class_Ex/In-class_Ex10/In-class_Ex10-gwr.html#section",
    "title": "Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method",
    "section": "",
    "text": "ggplot(data = condo_resale_sf,\n       aes(x = SELLING_PRICE)) +\n  geom_histogram()"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex10/In-class_Ex10.html#building-a-hedonic-pricing-model-by-using-multiple-linear-regression-method",
    "href": "In-class_Ex/In-class_Ex10/In-class_Ex10.html#building-a-hedonic-pricing-model-by-using-multiple-linear-regression-method",
    "title": "In-class Exercise 10: Calibrating Hedonic Pricing Model for Private Highrise Property with MLR methods",
    "section": "Building a Hedonic Pricing Model by using Multiple Linear Regression Method",
    "text": "Building a Hedonic Pricing Model by using Multiple Linear Regression Method\n\nThe code chunkThe output\n\n\n\n\ncondo_mlr &lt;- lm(formula = SELLING_PRICE ~ AREA_SQM + \n                  AGE   + PROX_CBD + PROX_CHILDCARE + \n                  PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + \n                  PROX_HAWKER_MARKET    + PROX_KINDERGARTEN + \n                  PROX_MRT  + PROX_PARK + PROX_PRIMARY_SCH + \n                  PROX_TOP_PRIMARY_SCH + PROX_SHOPPING_MALL + \n                  PROX_SUPERMARKET + PROX_BUS_STOP + \n                  NO_Of_UNITS + FAMILY_FRIENDLY + \n                  FREEHOLD + LEASEHOLD_99YR, \n                data=condo_resale_sf)\nsummary(condo_mlr)\n\n\n\n\n\n\n\n\nCall:\nlm(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + PROX_CHILDCARE + \n    PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + PROX_HAWKER_MARKET + \n    PROX_KINDERGARTEN + PROX_MRT + PROX_PARK + PROX_PRIMARY_SCH + \n    PROX_TOP_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_SUPERMARKET + \n    PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD + \n    LEASEHOLD_99YR, data = condo_resale_sf)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-3471036  -286903   -22426   239412 12254549 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)           543071.4   136210.9   3.987 7.03e-05 ***\nAREA_SQM               12688.7      370.1  34.283  &lt; 2e-16 ***\nAGE                   -24566.0     2766.0  -8.881  &lt; 2e-16 ***\nPROX_CBD              -78122.0     6791.4 -11.503  &lt; 2e-16 ***\nPROX_CHILDCARE       -333219.0   111020.3  -3.001 0.002734 ** \nPROX_ELDERLYCARE      170950.0    42110.8   4.060 5.19e-05 ***\nPROX_URA_GROWTH_AREA   38507.6    12523.7   3.075 0.002147 ** \nPROX_HAWKER_MARKET     23801.2    29299.9   0.812 0.416739    \nPROX_KINDERGARTEN     144098.0    82738.7   1.742 0.081795 .  \nPROX_MRT             -322775.9    58528.1  -5.515 4.14e-08 ***\nPROX_PARK             564487.9    66563.0   8.481  &lt; 2e-16 ***\nPROX_PRIMARY_SCH      186170.5    65515.2   2.842 0.004553 ** \nPROX_TOP_PRIMARY_SCH    -477.1    20598.0  -0.023 0.981525    \nPROX_SHOPPING_MALL   -207721.5    42855.5  -4.847 1.39e-06 ***\nPROX_SUPERMARKET      -48074.7    77145.3  -0.623 0.533273    \nPROX_BUS_STOP         675755.0   138552.0   4.877 1.20e-06 ***\nNO_Of_UNITS             -216.2       90.3  -2.394 0.016797 *  \nFAMILY_FRIENDLY       142128.3    47055.1   3.020 0.002569 ** \nFREEHOLD              300646.5    77296.5   3.890 0.000105 ***\nLEASEHOLD_99YR        -77137.4    77570.9  -0.994 0.320192    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 755800 on 1416 degrees of freedom\nMultiple R-squared:  0.652, Adjusted R-squared:  0.6474 \nF-statistic: 139.6 on 19 and 1416 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex10/In-class_Ex10.html#model-assessment-olsrr-method",
    "href": "In-class_Ex/In-class_Ex10/In-class_Ex10.html#model-assessment-olsrr-method",
    "title": "In-class Exercise 10: Calibrating Hedonic Pricing Model for Private Highrise Property with MLR methods",
    "section": "Model Assessment: olsrr method",
    "text": "Model Assessment: olsrr method\nIn this section, we would like to introduce you a fantastic R package specially programmed for performing OLS regression. It is called olsrr. It provides a collection of very useful methods for building better multiple linear regression models:\n\ncomprehensive regression output\nresidual diagnostics\nmeasures of influence\nheteroskedasticity tests\nmodel fit assessment\nvariable contribution assessment\nvariable selection procedures"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex10/In-class_Ex10.html#testing-for-spatial-autocorrelation",
    "href": "In-class_Ex/In-class_Ex10/In-class_Ex10.html#testing-for-spatial-autocorrelation",
    "title": "In-class Exercise 10: Calibrating Hedonic Pricing Model for Private Highrise Property with MLR methods",
    "section": "Testing for Spatial Autocorrelation",
    "text": "Testing for Spatial Autocorrelation\nThe hedonic model we try to build are using geographically referenced attributes, hence it is also important for us to visual the residual of the hedonic pricing model.\nFirst, we will export the residual of the hedonic pricing model and save it as a data frame.\n\nmlr_output &lt;- as.data.frame(condo_fw_mlr$model$residuals) %&gt;%\n  rename(`FW_MLR_RES` = `condo_fw_mlr$model$residuals`)\n\nNext, we will join the newly created data frame with condo_resale_sf object.\n\ncondo_resale_sf &lt;- cbind(condo_resale_sf, \n                        mlr_output$FW_MLR_RES) %&gt;%\n  rename(`MLR_RES` = `mlr_output.FW_MLR_RES`)\n\nNext, we will use tmap package to display the distribution of the residuals on an interactive map.\nThe code churn below will turn on the interactive mode of tmap.\n\ntmap_mode(\"view\")\ntm_shape(mpsz)+\n  tmap_options(check.and.fix = TRUE) +\n  tm_polygons(alpha = 0.4) +\ntm_shape(condo_resale_sf) +  \n  tm_dots(col = \"MLR_RES\",\n          alpha = 0.6,\n          style=\"quantile\")\n\n\n\n\ntmap_mode(\"plot\")\n\nThe figure above reveal that there is sign of spatial autocorrelation.\nSpatial stationary test\nTo proof that our observation is indeed true, the Moran’s I test will be performed\nHo: The residuals are randomly distributed (also known as spatial stationary) H1: The residuals are spatially non-stationary\nFirst, we will compute the distance-based weight matrix by using dnearneigh() function of spdep.\n\ncondo_resale_sf &lt;- condo_resale_sf %&gt;%\n  mutate(nb = st_knn(geometry, k=6,\n                     longlat = FALSE),\n         wt = st_weights(nb,\n                         style = \"W\"),\n         .before = 1)\n\nNext, global_moran_perm() of sfdep is used to perform global Moran permutation test.\n\nglobal_moran_perm(condo_resale_sf$MLR_RES, \n                  condo_resale_sf$nb, \n                  condo_resale_sf$wt, \n                  alternative = \"two.sided\", \n                  nsim = 99)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.32254, observed rank = 100, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n\n\nThe Global Moran’s I test for residual spatial autocorrelation shows that it’s p-value is less than 0.00000000000000022 which is less than the alpha value of 0.05. Hence, we will reject the null hypothesis that the residuals are randomly distributed.\nSince the Observed Global Moran I = 0.25586 which is greater than 0, we can infer than the residuals resemble cluster distribution."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex10/In-class_Ex10.html#introducing-olsrr-package",
    "href": "In-class_Ex/In-class_Ex10/In-class_Ex10.html#introducing-olsrr-package",
    "title": "In-class Exercise 10: Calibrating Hedonic Pricing Model for Private Highrise Property with MLR methods",
    "section": "Introducing olsrr package",
    "text": "Introducing olsrr package\nolsrr provides a collection of very useful methods for building better multiple linear regression models:\n\ncomprehensive regression output\nresidual diagnostics\nmeasures of influence\nheteroskedasticity tests\nmodel fit assessment\nvariable contribution assessment\nvariable selection procedures"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex10/In-class_Ex10.html#spatial-non-stationary-assumption",
    "href": "In-class_Ex/In-class_Ex10/In-class_Ex10.html#spatial-non-stationary-assumption",
    "title": "In-class Exercise 10: Calibrating Hedonic Pricing Model for Private Highrise Property with MLR methods",
    "section": "Spatial Non-stationary Assumption",
    "text": "Spatial Non-stationary Assumption\nThe hedonic model we try to build are using geographically referenced attributes, hence it is also important for us to visual the residual of the hedonic pricing model.\n\nHo: The residuals are randomly distributed (also known as spatial stationary)\nH1: The residuals are spatially non-stationary"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex11/In-class_Ex11.html#learning-outcome",
    "href": "In-class_Ex/In-class_Ex11/In-class_Ex11.html#learning-outcome",
    "title": "In-class Exercise 11: Calibrating Hedonic Pricing Model for Private Highrise Property: gwr methods",
    "section": "Learning Outcome",
    "text": "Learning Outcome\nBy the end of this hands-on exercise, you will be able to:\n\nPreparing data downloaded from REALIS portal for geocoding,\nGeocoding by using SLA OneMap API,\nConverting the geocoded transaction data into sf point feature data.frame, and\nWrangling the sf point features to avoid overlapping point features."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex11/In-class_Ex11.html#getting-started",
    "href": "In-class_Ex/In-class_Ex11/In-class_Ex11.html#getting-started",
    "title": "In-class Exercise 11: Calibrating Hedonic Pricing Model for Private Highrise Property: gwr methods",
    "section": "Getting Started",
    "text": "Getting Started\nLoading the R package\n\npacman::p_load(sf, GWmodel, olsrr, performance, ggstatsplot, tidyverse, tmap, sfdep)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex11/In-class_Ex11.html#the-data",
    "href": "In-class_Ex/In-class_Ex11/In-class_Ex11.html#the-data",
    "title": "In-class Exercise 11: Calibrating Hedonic Pricing Model for Private Highrise Property: gwr methods",
    "section": "The data",
    "text": "The data\n\nhunan &lt;- st_read(\n  dsn = \"data/geospatial\",\n  layer = \"Hunan\")\n\nReading layer `Hunan' from data source \n  `C:\\tskam\\IS415_AY2024-25T1\\In-class_Ex\\In-class_Ex11\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\n\ndata &lt;- read_csv(\"data/aspatial/Hunan_2012.csv\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex11/In-class_Ex11.html#eda-non-spatial-model",
    "href": "In-class_Ex/In-class_Ex11/In-class_Ex11.html#eda-non-spatial-model",
    "title": "In-class Exercise 11: Calibrating Hedonic Pricing Model for Private Highrise Property: gwr methods",
    "section": "EDA & Non-spatial Model",
    "text": "EDA & Non-spatial Model"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex11/In-class_Ex11.html#non-spatial-model",
    "href": "In-class_Ex/In-class_Ex11/In-class_Ex11.html#non-spatial-model",
    "title": "In-class Exercise 11: Calibrating Hedonic Pricing Model for Private Highrise Property: gwr methods",
    "section": "Non-spatial Model",
    "text": "Non-spatial Model\n\ndev_mlr &lt;- lm(formula = GDPPC ~ GIO + NOIP +\n                Agri + Service + ROREmp + EmpR, \n                data=data)\nsummary(dev_mlr)\n\n\nCall:\nlm(formula = GDPPC ~ GIO + NOIP + Agri + Service + ROREmp + EmpR, \n    data = data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-15961  -4270   -708   2145  41910 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  4.554e+04  9.608e+03   4.740 9.02e-06 ***\nGIO          3.459e-01  7.220e-02   4.791 7.38e-06 ***\nNOIP         3.786e+01  1.699e+01   2.228 0.028637 *  \nAgri         8.903e-01  4.939e-01   1.803 0.075149 .  \nService      5.963e-02  1.328e-01   0.449 0.654553    \nROREmp      -3.127e+04  1.269e+04  -2.464 0.015858 *  \nEmpR        -3.611e+01  9.503e+00  -3.800 0.000279 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8396 on 81 degrees of freedom\nMultiple R-squared:  0.7081,    Adjusted R-squared:  0.6865 \nF-statistic: 32.75 on 6 and 81 DF,  p-value: &lt; 2.2e-16\n\n\n\nplot(check_collinearity(dev_mlr)) +\n  theme(axis.text.x = element_text(\n    angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\nout &lt;- plot(check_model(dev_mlr, \n                        panel = FALSE))\nout[[2]]\n\n\n\n\n\n\n\n\n\nplot(check_normality(dev_mlr))\n\n\n\n\n\n\n\n\n\noutliers &lt;- check_outliers(dev_mlr,\n                           method = \"cook\")\noutliers\n\n1 outlier detected: case 66.\n- Based on the following method and threshold: cook (0.914).\n- For variable: (Whole model).\n\n\n\nplot(check_outliers(dev_mlr,\n                           method = \"cook\"))\n\n\n\n\n\n\n\n\n\nmlr_output &lt;- as.data.frame(\n  dev_mlr$residuals) %&gt;%\n  rename(`MLR_RES` = `dev_mlr$residuals`)\n\n\nhunan &lt;- cbind(\n  hunan, mlr_output)\n\n\ntm_shape(hunan) +\n  tm_fill(\"MLR_RES\",\n          n = 5,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Residauls of MLR\") \n\n\n\n\n\n\n\n\n\nwm_q &lt;- hunan %&gt;%\n  mutate(nb = st_contiguity(geometry),\n         wt = st_weights(nb,\n                         style = \"W\"),\n         .before = 1)\n\n\nglobal_moran_perm(wm_q$MLR_RES, \n                  wm_q$nb, \n                  wm_q$wt, \n                  alternative = \"two.sided\", \n                  nsim = 99)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.0029103, observed rank = 61, p-value = 0.78\nalternative hypothesis: two.sided"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex11/In-class_Ex11.html#loading-the-r-package",
    "href": "In-class_Ex/In-class_Ex11/In-class_Ex11.html#loading-the-r-package",
    "title": "In-class Exercise 11: Calibrating Hedonic Pricing Model for Private Highrise Property: gwr methods",
    "section": "Loading the R package",
    "text": "Loading the R package\n\n\npacman::p_load(tidyverse, sf, tmap, httr, performance)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex11/In-class_Ex11.html#working-with-realis-data",
    "href": "In-class_Ex/In-class_Ex11/In-class_Ex11.html#working-with-realis-data",
    "title": "In-class Exercise 11: Calibrating Hedonic Pricing Model for Private Highrise Property: gwr methods",
    "section": "Working with REALIS data",
    "text": "Working with REALIS data\n\nOverviewImporting dataWrangling dataPreparing dataGeocoding\n\n\nBy the end of this section, you will be able to\n\nimport multiple csv files in R environment and at the same time combine them into a single tibble data.frame\n\n\n\n\n\nfolder_path &lt;- \"data/aspatial\"\nfile_list &lt;- list.files(path = folder_path, \n                        pattern = \"^realis.*\\\\.csv$\", \n                        full.names = TRUE)\n\nrealis_data &lt;- file_list %&gt;%\n  map_dfr(read_csv)\n\n\n\n\ncondo_resale &lt;- realis_data %&gt;%\n  mutate(`Sale Date` = dmy(`Sale Date`)) %&gt;%\n  filter(`Type of Sale` == \"Resale\" &\n           `Property Type` == \"Condominium\")\n\n\n\n\npostcode &lt;- unique(condo_resale$`Postal Code`)\n\n\n\n\nurl &lt;- \"https://onemap.gov.sg/api/common/elastic/search\"\n\nfound &lt;- data.frame()\nnot_found &lt;- data.frame()\n\nfor (postcode in postcode){\n  query &lt;- list('searchVal'=postcode, \n                'returnGeom'='Y', \n                'getAddrDetails'='Y', \n                'pageNum'='1')\n  res &lt;- GET(url, \n             query=query)\n  if ((content(res)$found)!=0){\n    found &lt;- rbind(found, \n                   data.frame(content(res))[4:13])\n  } else {\n    not_found = data.frame(postcode)\n  }\n}\n\n\nwrite_rds(found, \"data/rds/found.rds\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex11/In-class_Ex11.html#converting-to-point-feature-data-frame",
    "href": "In-class_Ex/In-class_Ex11/In-class_Ex11.html#converting-to-point-feature-data-frame",
    "title": "In-class Exercise 11: Calibrating Hedonic Pricing Model for Private Highrise Property: gwr methods",
    "section": "Converting to Point Feature Data Frame",
    "text": "Converting to Point Feature Data Frame\n\nThe tasksJoining tablesConvering to sf\n\n\n\nWrite a code chunk to join condo_resale and found. Name the output condo_resale_geocoded.\nWrite a code chunk to convert condo_resale_geocoded from tibble data frame to sf point feature data frame.\n\n\n\n\n\ncondo_resale_geocoded = left_join(\n  condo_resale, found, \n  by = c('Postal Code' = 'POSTAL'))\n\n\n\n\n\n\ncondo_resale_sf &lt;- st_as_sf(condo_resale_geocoded, \n                            coords = c(\"XCOORD\",\n                                       \"YCOORD\"),\n                            crs=3414)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex11/In-class_Ex11.html#cleaning-spatial-data",
    "href": "In-class_Ex/In-class_Ex11/In-class_Ex11.html#cleaning-spatial-data",
    "title": "In-class Exercise 11: Calibrating Hedonic Pricing Model for Private Highrise Property: gwr methods",
    "section": "Cleaning Spatial Data",
    "text": "Cleaning Spatial Data\n\nChecking for overlapping point featuresSpatial jittering\n\n\nThe code chunk below is used to check if there are overlapping point features.\n\n\noverlapping_points &lt;- condo_resale_sf %&gt;%\n  mutate(overlap = lengths(st_equals(., .)) &gt; 1)\n\n\n\n\nIn the code code chunk below, st_jitter() of sf package is used to move the point features by 5m to avoid overlapping point features.\n\n\ncondo_resale_sf &lt;- condo_resale_sf %&gt;%\n  st_jitter(amount = 2)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex11/In-class_Ex11.html#importing-data",
    "href": "In-class_Ex/In-class_Ex11/In-class_Ex11.html#importing-data",
    "title": "In-class Exercise 11: Calibrating Hedonic Pricing Model for Private Highrise Property: gwr methods",
    "section": "Importing data",
    "text": "Importing data\nThe code chunk below imports multiple csv files in a specified folder and append them into a single tibble data frame.\n\n\nfolder_path &lt;- \"data/aspatial\"\nfile_list &lt;- list.files(path = folder_path, \n                        pattern = \"^realis.*\\\\.csv$\", \n                        full.names = TRUE)\n\nrealis_data &lt;- file_list %&gt;%\n  map_dfr(read_csv)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex11/In-class_Ex11.html#wrangling-data",
    "href": "In-class_Ex/In-class_Ex11/In-class_Ex11.html#wrangling-data",
    "title": "In-class Exercise 11: Calibrating Hedonic Pricing Model for Private Highrise Property: gwr methods",
    "section": "Wrangling data",
    "text": "Wrangling data\n\nThe taskThe code\n\n\nWrite a code chunk to perform the followings: - converting values in Sale Date field from character to numerical date format, and - extracting resale and condominium transaction records.\n\n\n\n\ncondo_resale &lt;- realis_data %&gt;%\n  mutate(`Sale Date` = dmy(`Sale Date`)) %&gt;%\n  filter(`Type of Sale` == \"Resale\" &\n           `Property Type` == \"Condominium\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex11/In-class_Ex11.html#geocoding",
    "href": "In-class_Ex/In-class_Ex11/In-class_Ex11.html#geocoding",
    "title": "In-class Exercise 11: Calibrating Hedonic Pricing Model for Private Highrise Property: gwr methods",
    "section": "Geocoding",
    "text": "Geocoding\n\nPreparing dataGeocoding\n\n\n\n\npostcode &lt;- unique(condo_resale$`Postal Code`)\n\n\n\n\n\n\nurl &lt;- \"https://onemap.gov.sg/api/common/elastic/search\"\nfound &lt;- data.frame()\nnot_found &lt;- data.frame()\n\nfor (postcode in postcode){\n  query &lt;- list('searchVal'=postcode, 'returnGeom'='Y', \n                'getAddrDetails'='Y', 'pageNum'='1')\n  res &lt;- GET(url, query=query)\n  if ((content(res)$found)!=0){\n    found &lt;- rbind(found, data.frame(content(res))[4:13])\n  } else {not_found = data.frame(postcode)\n  }\n}"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex11/In-class_Ex11.html#tidying-field-names",
    "href": "In-class_Ex/In-class_Ex11/In-class_Ex11.html#tidying-field-names",
    "title": "In-class Exercise 11: Calibrating Hedonic Pricing Model for Private Highrise Property: gwr methods",
    "section": "Tidying field names",
    "text": "Tidying field names\n\n\nfound &lt;- found %&gt;%\n  select(c(6:8)) %&gt;%\n  rename(POSTAL = `results.POSTAL`,\n         XCOORD = `results.X`,\n         YCOORD = `results.Y`)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex11/In-class_Ex11-gwr.html",
    "href": "In-class_Ex/In-class_Ex11/In-class_Ex11-gwr.html",
    "title": "Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method",
    "section": "",
    "text": "pacman::p_load(olsrr, ggstatsplot, ggpubr, \n               sf, spdep, GWmodel, tmap,\n               tidyverse, gtsummary, performance,\n               see, sfdep)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex11/In-class_Ex11-gwr.html#getting-started",
    "href": "In-class_Ex/In-class_Ex11/In-class_Ex11-gwr.html#getting-started",
    "title": "Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method",
    "section": "",
    "text": "pacman::p_load(olsrr, ggstatsplot, ggpubr, \n               sf, spdep, GWmodel, tmap,\n               tidyverse, gtsummary, performance,\n               see, sfdep)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex11/In-class_Ex11-gwr.html#importing-the-data",
    "href": "In-class_Ex/In-class_Ex11/In-class_Ex11-gwr.html#importing-the-data",
    "title": "Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method",
    "section": "Importing the data",
    "text": "Importing the data\n\nURA Master Plan 2014 planning subzone boundary\n\ncondo_resale &lt;- read_csv(\"data/aspatial/Condo_resale_2015.csv\")\n\n\nmpsz &lt;- read_rds(\"data/rds/mpsz.rds\")\n\n\ncondo_resale_sf &lt;- read_rds(\n  \"data/rds/condo_resale_sf.rds\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex11/In-class_Ex11-gwr.html#correlation-analysis---ggstatsplot-methods",
    "href": "In-class_Ex/In-class_Ex11/In-class_Ex11-gwr.html#correlation-analysis---ggstatsplot-methods",
    "title": "Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method",
    "section": "Correlation Analysis - ggstatsplot methods",
    "text": "Correlation Analysis - ggstatsplot methods\nInstead of using corrplot package, in the code chunk below, ggcorrmat() of ggstatsplot is used.\n\nggcorrmat(condo_resale[, 5:23])"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex11/In-class_Ex11-gwr.html#building-a-hedonic-pricing-model-by-using-multiple-linear-regression-method",
    "href": "In-class_Ex/In-class_Ex11/In-class_Ex11-gwr.html#building-a-hedonic-pricing-model-by-using-multiple-linear-regression-method",
    "title": "Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method",
    "section": "Building a Hedonic Pricing Model by using Multiple Linear Regression Method",
    "text": "Building a Hedonic Pricing Model by using Multiple Linear Regression Method\nThe code chunk below using lm() to calibrate the multiple linear regression model.\n\ncondo_mlr &lt;- lm(formula = SELLING_PRICE ~ AREA_SQM + \n                  AGE   + PROX_CBD + PROX_CHILDCARE + \n                  PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + \n                  PROX_HAWKER_MARKET    + PROX_KINDERGARTEN + \n                  PROX_MRT  + PROX_PARK + PROX_PRIMARY_SCH + \n                  PROX_TOP_PRIMARY_SCH + PROX_SHOPPING_MALL + \n                  PROX_SUPERMARKET + PROX_BUS_STOP + \n                  NO_Of_UNITS + FAMILY_FRIENDLY + \n                  FREEHOLD + LEASEHOLD_99YR, \n                data=condo_resale_sf)\nsummary(condo_mlr)\n\n\nCall:\nlm(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + PROX_CHILDCARE + \n    PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + PROX_HAWKER_MARKET + \n    PROX_KINDERGARTEN + PROX_MRT + PROX_PARK + PROX_PRIMARY_SCH + \n    PROX_TOP_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_SUPERMARKET + \n    PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD + \n    LEASEHOLD_99YR, data = condo_resale_sf)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-3471036  -286903   -22426   239412 12254549 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)           543071.4   136210.9   3.987 7.03e-05 ***\nAREA_SQM               12688.7      370.1  34.283  &lt; 2e-16 ***\nAGE                   -24566.0     2766.0  -8.881  &lt; 2e-16 ***\nPROX_CBD              -78122.0     6791.4 -11.503  &lt; 2e-16 ***\nPROX_CHILDCARE       -333219.0   111020.3  -3.001 0.002734 ** \nPROX_ELDERLYCARE      170950.0    42110.8   4.060 5.19e-05 ***\nPROX_URA_GROWTH_AREA   38507.6    12523.7   3.075 0.002147 ** \nPROX_HAWKER_MARKET     23801.2    29299.9   0.812 0.416739    \nPROX_KINDERGARTEN     144098.0    82738.7   1.742 0.081795 .  \nPROX_MRT             -322775.9    58528.1  -5.515 4.14e-08 ***\nPROX_PARK             564487.9    66563.0   8.481  &lt; 2e-16 ***\nPROX_PRIMARY_SCH      186170.5    65515.2   2.842 0.004553 ** \nPROX_TOP_PRIMARY_SCH    -477.1    20598.0  -0.023 0.981525    \nPROX_SHOPPING_MALL   -207721.5    42855.5  -4.847 1.39e-06 ***\nPROX_SUPERMARKET      -48074.7    77145.3  -0.623 0.533273    \nPROX_BUS_STOP         675755.0   138552.0   4.877 1.20e-06 ***\nNO_Of_UNITS             -216.2       90.3  -2.394 0.016797 *  \nFAMILY_FRIENDLY       142128.3    47055.1   3.020 0.002569 ** \nFREEHOLD              300646.5    77296.5   3.890 0.000105 ***\nLEASEHOLD_99YR        -77137.4    77570.9  -0.994 0.320192    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 755800 on 1416 degrees of freedom\nMultiple R-squared:  0.652, Adjusted R-squared:  0.6474 \nF-statistic: 139.6 on 19 and 1416 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex11/In-class_Ex11-gwr.html#model-assessment-olsrr-method",
    "href": "In-class_Ex/In-class_Ex11/In-class_Ex11-gwr.html#model-assessment-olsrr-method",
    "title": "Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method",
    "section": "Model Assessment: olsrr method",
    "text": "Model Assessment: olsrr method\nIn this section, we would like to introduce you a fantastic R package specially programmed for performing OLS regression. It is called olsrr. It provides a collection of very useful methods for building better multiple linear regression models:\n\ncomprehensive regression output\nresidual diagnostics\nmeasures of influence\nheteroskedasticity tests\nmodel fit assessment\nvariable contribution assessment\nvariable selection procedures\n\n\nGenerating tidy linear regression report\n\nols_regress(condo_mlr)\n\n                                Model Summary                                 \n-----------------------------------------------------------------------------\nR                            0.807       RMSE                     750537.537 \nR-Squared                    0.652       MSE                571262902261.223 \nAdj. R-Squared               0.647       Coef. Var                    43.160 \nPred R-Squared               0.637       AIC                       42971.173 \nMAE                     412117.987       SBC                       43081.835 \n-----------------------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n AIC: Akaike Information Criteria \n SBC: Schwarz Bayesian Criteria \n\n                                     ANOVA                                       \n--------------------------------------------------------------------------------\n                    Sum of                                                      \n                   Squares          DF         Mean Square       F         Sig. \n--------------------------------------------------------------------------------\nRegression    1.515738e+15          19        7.977571e+13    139.648    0.0000 \nResidual      8.089083e+14        1416    571262902261.223                      \nTotal         2.324647e+15        1435                                          \n--------------------------------------------------------------------------------\n\n                                               Parameter Estimates                                                \n-----------------------------------------------------------------------------------------------------------------\n               model           Beta    Std. Error    Std. Beta       t        Sig           lower          upper \n-----------------------------------------------------------------------------------------------------------------\n         (Intercept)     543071.420    136210.918                   3.987    0.000     275874.535     810268.305 \n            AREA_SQM      12688.669       370.119        0.579     34.283    0.000      11962.627      13414.710 \n                 AGE     -24566.001      2766.041       -0.166     -8.881    0.000     -29991.980     -19140.022 \n            PROX_CBD     -78121.985      6791.377       -0.267    -11.503    0.000     -91444.227     -64799.744 \n      PROX_CHILDCARE    -333219.036    111020.303       -0.087     -3.001    0.003    -551000.984    -115437.089 \n    PROX_ELDERLYCARE     170949.961     42110.748        0.083      4.060    0.000      88343.803     253556.120 \nPROX_URA_GROWTH_AREA      38507.622     12523.661        0.059      3.075    0.002      13940.700      63074.545 \n  PROX_HAWKER_MARKET      23801.197     29299.923        0.019      0.812    0.417     -33674.725      81277.120 \n   PROX_KINDERGARTEN     144097.972     82738.669        0.030      1.742    0.082     -18205.570     306401.514 \n            PROX_MRT    -322775.874     58528.079       -0.123     -5.515    0.000    -437586.937    -207964.811 \n           PROX_PARK     564487.876     66563.011        0.148      8.481    0.000     433915.162     695060.590 \n    PROX_PRIMARY_SCH     186170.524     65515.193        0.072      2.842    0.005      57653.253     314687.795 \nPROX_TOP_PRIMARY_SCH       -477.073     20597.972       -0.001     -0.023    0.982     -40882.894      39928.747 \n  PROX_SHOPPING_MALL    -207721.520     42855.500       -0.109     -4.847    0.000    -291788.613    -123654.427 \n    PROX_SUPERMARKET     -48074.679     77145.257       -0.012     -0.623    0.533    -199405.956     103256.599 \n       PROX_BUS_STOP     675755.044    138551.991        0.133      4.877    0.000     403965.817     947544.272 \n         NO_Of_UNITS       -216.180        90.302       -0.046     -2.394    0.017       -393.320        -39.040 \n     FAMILY_FRIENDLY     142128.272     47055.082        0.056      3.020    0.003      49823.107     234433.438 \n            FREEHOLD     300646.543     77296.529        0.117      3.890    0.000     149018.525     452274.561 \n      LEASEHOLD_99YR     -77137.375     77570.869       -0.030     -0.994    0.320    -229303.551      75028.801 \n-----------------------------------------------------------------------------------------------------------------\n\n\n\nMulticolinearuty\n\nols_vif_tol(condo_mlr)\n\n              Variables Tolerance      VIF\n1              AREA_SQM 0.8601326 1.162611\n2                   AGE 0.7011585 1.426211\n3              PROX_CBD 0.4575471 2.185567\n4        PROX_CHILDCARE 0.2898233 3.450378\n5      PROX_ELDERLYCARE 0.5922238 1.688551\n6  PROX_URA_GROWTH_AREA 0.6614081 1.511926\n7    PROX_HAWKER_MARKET 0.4373874 2.286303\n8     PROX_KINDERGARTEN 0.8356793 1.196631\n9              PROX_MRT 0.4949877 2.020252\n10            PROX_PARK 0.8015728 1.247547\n11     PROX_PRIMARY_SCH 0.3823248 2.615577\n12 PROX_TOP_PRIMARY_SCH 0.4878620 2.049760\n13   PROX_SHOPPING_MALL 0.4903052 2.039546\n14     PROX_SUPERMARKET 0.6142127 1.628100\n15        PROX_BUS_STOP 0.3311024 3.020213\n16          NO_Of_UNITS 0.6543336 1.528272\n17      FAMILY_FRIENDLY 0.7191719 1.390488\n18             FREEHOLD 0.2728521 3.664990\n19       LEASEHOLD_99YR 0.2645988 3.779307\n\n\n\n\nVariable selection\n\ncondo_fw_mlr &lt;- ols_step_forward_p(\n  condo_mlr,\n  p_val = 0.05,\n  details = FALSE)\n\n\nplot(condo_fw_mlr)\n\n\n\n\n\n\n\n\n\n\n\nVisualising model parameters\n\nggcoefstats(condo_mlr,\n            sort = \"ascending\")\n\n\n\n\n\n\n\n\n\n\nTest for Non-Linearity\nIn multiple linear regression, it is important for us to test the assumption that linearity and additivity of the relationship between dependent and independent variables.\nIn the code chunk below, the ols_plot_resid_fit() of olsrr package is used to perform linearity assumption test.\n\nols_plot_resid_fit(condo_fw_mlr$model)\n\n\n\n\n\n\n\n\nThe figure above reveals that most of the data poitns are scattered around the 0 line, hence we can safely conclude that the relationships between the dependent variable and independent variables are linear.\n\n\nTest for Normality Assumption\nLastly, the code chunk below uses ols_plot_resid_hist() of olsrr package to perform normality assumption test.\n\nols_plot_resid_hist(condo_fw_mlr$model)\n\n\n\n\n\n\n\n\nThe figure reveals that the residual of the multiple linear regression model (i.e. condo.mlr1) is resemble normal distribution.\nIf you prefer formal statistical test methods, the ols_test_normality() of olsrr package can be used as shown in the code chun below.\n\nols_test_normality(condo_fw_mlr$model)\n\n-----------------------------------------------\n       Test             Statistic       pvalue  \n-----------------------------------------------\nShapiro-Wilk              0.6856         0.0000 \nKolmogorov-Smirnov        0.1366         0.0000 \nCramer-von Mises         121.0768        0.0000 \nAnderson-Darling         67.9551         0.0000 \n-----------------------------------------------\n\n\nThe summary table above reveals that the p-values of the four tests are way smaller than the alpha value of 0.05. Hence we will reject the null hypothesis and infer that there is statistical evidence that the residual are not normally distributed."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex11/In-class_Ex11-gwr.html#testing-for-spatial-autocorrelation",
    "href": "In-class_Ex/In-class_Ex11/In-class_Ex11-gwr.html#testing-for-spatial-autocorrelation",
    "title": "Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method",
    "section": "Testing for Spatial Autocorrelation",
    "text": "Testing for Spatial Autocorrelation\nThe hedonic model we try to build are using geographically referenced attributes, hence it is also important for us to visual the residual of the hedonic pricing model.\nFirst, we will export the residual of the hedonic pricing model and save it as a data frame.\n\nmlr_output &lt;- as.data.frame(condo_fw_mlr$model$residuals) %&gt;%\n  rename(`FW_MLR_RES` = `condo_fw_mlr$model$residuals`)\n\nNext, we will join the newly created data frame with condo_resale_sf object.\n\ncondo_resale_sf &lt;- cbind(condo_resale_sf, \n                        mlr_output$FW_MLR_RES) %&gt;%\n  rename(`MLR_RES` = `mlr_output.FW_MLR_RES`)\n\nNext, we will use tmap package to display the distribution of the residuals on an interactive map.\nThe code churn below will turn on the interactive mode of tmap.\n\ntmap_mode(\"view\")\ntm_shape(mpsz)+\n  tmap_options(check.and.fix = TRUE) +\n  tm_polygons(alpha = 0.4) +\ntm_shape(condo_resale_sf) +  \n  tm_dots(col = \"MLR_RES\",\n          alpha = 0.6,\n          style=\"quantile\")\n\n\n\n\ntmap_mode(\"plot\")\n\nThe figure above reveal that there is sign of spatial autocorrelation.\n\nSpatial stationary test\nTo proof that our observation is indeed true, the Moran’s I test will be performed\nHo: The residuals are randomly distributed (also known as spatial stationary) H1: The residuals are spatially non-stationary\nFirst, we will compute the distance-based weight matrix by using dnearneigh() function of spdep.\n\ncondo_resale_sf &lt;- condo_resale_sf %&gt;%\n  mutate(nb = st_knn(geometry, k=6,\n                     longlat = FALSE),\n         wt = st_weights(nb,\n                         style = \"W\"),\n         .before = 1)\n\nNext, global_moran_perm() of sfdep is used to perform global Moran permutation test.\n\nglobal_moran_perm(condo_resale_sf$MLR_RES, \n                  condo_resale_sf$nb, \n                  condo_resale_sf$wt, \n                  alternative = \"two.sided\", \n                  nsim = 99)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.32254, observed rank = 100, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n\n\nThe Global Moran’s I test for residual spatial autocorrelation shows that it’s p-value is less than 0.00000000000000022 which is less than the alpha value of 0.05. Hence, we will reject the null hypothesis that the residuals are randomly distributed.\nSince the Observed Global Moran I = 0.25586 which is greater than 0, we can infer than the residuals resemble cluster distribution."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex11/In-class_Ex11-gwr.html#building-hedonic-pricing-models-using-gwmodel",
    "href": "In-class_Ex/In-class_Ex11/In-class_Ex11-gwr.html#building-hedonic-pricing-models-using-gwmodel",
    "title": "Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method",
    "section": "Building Hedonic Pricing Models using GWmodel",
    "text": "Building Hedonic Pricing Models using GWmodel\nIn this section, you are going to learn how to modelling hedonic pricing by using geographically weighted regression model. Two spatial weights will be used, they are: fixed and adaptive bandwidth schemes.\n\nBuilding Fixed Bandwidth GWR Model\n\nComputing fixed bandwith\nIn the code chunk below bw.gwr() of GWModel package is used to determine the optimal fixed bandwidth to use in the model. Notice that the argument adaptive is set to FALSE indicates that we are interested to compute the fixed bandwidth.\nThere are two possible approaches can be uused to determine the stopping rule, they are: CV cross-validation approach and AIC corrected (AICc) approach. We define the stopping rule using approach agreement.\n\nbw_fixed &lt;- bw.gwr(formula = SELLING_PRICE ~ AREA_SQM + AGE + \n                     PROX_CBD + PROX_CHILDCARE + \n                     PROX_ELDERLYCARE   + PROX_URA_GROWTH_AREA + \n                     PROX_MRT   + PROX_PARK + PROX_PRIMARY_SCH + \n                     PROX_SHOPPING_MALL + PROX_BUS_STOP + \n                     NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n                   data=condo_resale_sf, \n                   approach=\"CV\", \n                   kernel=\"gaussian\", \n                   adaptive=FALSE, \n                   longlat=FALSE)\n\nFixed bandwidth: 17660.96 CV score: 8.259118e+14 \nFixed bandwidth: 10917.26 CV score: 7.970454e+14 \nFixed bandwidth: 6749.419 CV score: 7.273273e+14 \nFixed bandwidth: 4173.553 CV score: 6.300006e+14 \nFixed bandwidth: 2581.58 CV score: 5.404958e+14 \nFixed bandwidth: 1597.687 CV score: 4.857515e+14 \nFixed bandwidth: 989.6077 CV score: 4.722431e+14 \nFixed bandwidth: 613.7939 CV score: 1.378294e+16 \nFixed bandwidth: 1221.873 CV score: 4.778717e+14 \nFixed bandwidth: 846.0596 CV score: 4.791629e+14 \nFixed bandwidth: 1078.325 CV score: 4.751406e+14 \nFixed bandwidth: 934.7772 CV score: 4.72518e+14 \nFixed bandwidth: 1023.495 CV score: 4.730305e+14 \nFixed bandwidth: 968.6643 CV score: 4.721317e+14 \nFixed bandwidth: 955.7206 CV score: 4.722072e+14 \nFixed bandwidth: 976.6639 CV score: 4.721387e+14 \nFixed bandwidth: 963.7202 CV score: 4.721484e+14 \nFixed bandwidth: 971.7199 CV score: 4.721293e+14 \nFixed bandwidth: 973.6083 CV score: 4.721309e+14 \nFixed bandwidth: 970.5527 CV score: 4.721295e+14 \nFixed bandwidth: 972.4412 CV score: 4.721296e+14 \nFixed bandwidth: 971.2741 CV score: 4.721292e+14 \nFixed bandwidth: 970.9985 CV score: 4.721293e+14 \nFixed bandwidth: 971.4443 CV score: 4.721292e+14 \nFixed bandwidth: 971.5496 CV score: 4.721293e+14 \nFixed bandwidth: 971.3793 CV score: 4.721292e+14 \nFixed bandwidth: 971.3391 CV score: 4.721292e+14 \nFixed bandwidth: 971.3143 CV score: 4.721292e+14 \nFixed bandwidth: 971.3545 CV score: 4.721292e+14 \nFixed bandwidth: 971.3296 CV score: 4.721292e+14 \nFixed bandwidth: 971.345 CV score: 4.721292e+14 \nFixed bandwidth: 971.3355 CV score: 4.721292e+14 \nFixed bandwidth: 971.3413 CV score: 4.721292e+14 \nFixed bandwidth: 971.3377 CV score: 4.721292e+14 \nFixed bandwidth: 971.34 CV score: 4.721292e+14 \nFixed bandwidth: 971.3405 CV score: 4.721292e+14 \nFixed bandwidth: 971.3408 CV score: 4.721292e+14 \nFixed bandwidth: 971.3403 CV score: 4.721292e+14 \nFixed bandwidth: 971.3406 CV score: 4.721292e+14 \nFixed bandwidth: 971.3404 CV score: 4.721292e+14 \nFixed bandwidth: 971.3405 CV score: 4.721292e+14 \nFixed bandwidth: 971.3405 CV score: 4.721292e+14 \n\n\nThe result shows that the recommended bandwidth is 971.3405 metres. (Quiz: Do you know why it is in metre?)\n\n\nGWModel method - fixed bandwith\nNow we can use the code chunk below to calibrate the gwr model using fixed bandwidth and gaussian kernel.\n\ngwr_fixed &lt;- gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + \n                         AGE    + PROX_CBD + PROX_CHILDCARE + \n                         PROX_ELDERLYCARE   +PROX_URA_GROWTH_AREA + \n                         PROX_MRT   + PROX_PARK + PROX_PRIMARY_SCH +\n                         PROX_SHOPPING_MALL + PROX_BUS_STOP + \n                         NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n                       data=condo_resale_sf, \n                       bw=bw_fixed, \n                       kernel = 'gaussian', \n                       longlat = FALSE)\n\nThe output is saved in a list of class “gwrm”. The code below can be used to display the model output.\n\ngwr_fixed\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n   Program starts at: 2024-11-04 07:48:46.228495 \n   Call:\n   gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + \n    PROX_CHILDCARE + PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + \n    PROX_MRT + PROX_PARK + PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + \n    PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n    data = condo_resale_sf, bw = bw_fixed, kernel = \"gaussian\", \n    longlat = FALSE)\n\n   Dependent (y) variable:  SELLING_PRICE\n   Independent variables:  AREA_SQM AGE PROX_CBD PROX_CHILDCARE PROX_ELDERLYCARE PROX_URA_GROWTH_AREA PROX_MRT PROX_PARK PROX_PRIMARY_SCH PROX_SHOPPING_MALL PROX_BUS_STOP NO_Of_UNITS FAMILY_FRIENDLY FREEHOLD\n   Number of data points: 1436\n   ***********************************************************************\n   *                    Results of Global Regression                     *\n   ***********************************************************************\n\n   Call:\n    lm(formula = formula, data = data)\n\n   Residuals:\n     Min       1Q   Median       3Q      Max \n-3470778  -298119   -23481   248917 12234210 \n\n   Coefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n   (Intercept)           527633.22  108183.22   4.877 1.20e-06 ***\n   AREA_SQM               12777.52     367.48  34.771  &lt; 2e-16 ***\n   AGE                   -24687.74    2754.84  -8.962  &lt; 2e-16 ***\n   PROX_CBD              -77131.32    5763.12 -13.384  &lt; 2e-16 ***\n   PROX_CHILDCARE       -318472.75  107959.51  -2.950 0.003231 ** \n   PROX_ELDERLYCARE      185575.62   39901.86   4.651 3.61e-06 ***\n   PROX_URA_GROWTH_AREA   39163.25   11754.83   3.332 0.000885 ***\n   PROX_MRT             -294745.11   56916.37  -5.179 2.56e-07 ***\n   PROX_PARK             570504.81   65507.03   8.709  &lt; 2e-16 ***\n   PROX_PRIMARY_SCH      159856.14   60234.60   2.654 0.008046 ** \n   PROX_SHOPPING_MALL   -220947.25   36561.83  -6.043 1.93e-09 ***\n   PROX_BUS_STOP         682482.22  134513.24   5.074 4.42e-07 ***\n   NO_Of_UNITS             -245.48      87.95  -2.791 0.005321 ** \n   FAMILY_FRIENDLY       146307.58   46893.02   3.120 0.001845 ** \n   FREEHOLD              350599.81   48506.48   7.228 7.98e-13 ***\n\n   ---Significance stars\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n   Residual standard error: 756000 on 1421 degrees of freedom\n   Multiple R-squared: 0.6507\n   Adjusted R-squared: 0.6472 \n   F-statistic: 189.1 on 14 and 1421 DF,  p-value: &lt; 2.2e-16 \n   ***Extra Diagnostic information\n   Residual sum of squares: 8.120609e+14\n   Sigma(hat): 752522.9\n   AIC:  42966.76\n   AICc:  42967.14\n   BIC:  41731.39\n   ***********************************************************************\n   *          Results of Geographically Weighted Regression              *\n   ***********************************************************************\n\n   *********************Model calibration information*********************\n   Kernel function: gaussian \n   Fixed bandwidth: 971.3405 \n   Regression points: the same locations as observations are used.\n   Distance metric: Euclidean distance metric is used.\n\n   ****************Summary of GWR coefficient estimates:******************\n                               Min.     1st Qu.      Median     3rd Qu.\n   Intercept            -3.5988e+07 -5.1998e+05  7.6780e+05  1.7412e+06\n   AREA_SQM              1.0003e+03  5.2758e+03  7.4740e+03  1.2301e+04\n   AGE                  -1.3475e+05 -2.0813e+04 -8.6260e+03 -3.7784e+03\n   PROX_CBD             -7.7047e+07 -2.3608e+05 -8.3600e+04  3.4646e+04\n   PROX_CHILDCARE       -6.0097e+06 -3.3667e+05 -9.7425e+04  2.9007e+05\n   PROX_ELDERLYCARE     -3.5000e+06 -1.5970e+05  3.1971e+04  1.9577e+05\n   PROX_URA_GROWTH_AREA -3.0170e+06 -8.2013e+04  7.0749e+04  2.2612e+05\n   PROX_MRT             -3.5282e+06 -6.5836e+05 -1.8833e+05  3.6922e+04\n   PROX_PARK            -1.2062e+06 -2.1732e+05  3.5383e+04  4.1335e+05\n   PROX_PRIMARY_SCH     -2.2695e+07 -1.7066e+05  4.8472e+04  5.1555e+05\n   PROX_SHOPPING_MALL   -7.2585e+06 -1.6684e+05 -1.0517e+04  1.5923e+05\n   PROX_BUS_STOP        -1.4676e+06 -4.5207e+04  3.7601e+05  1.1664e+06\n   NO_Of_UNITS          -1.3170e+03 -2.4822e+02 -3.0846e+01  2.5496e+02\n   FAMILY_FRIENDLY      -2.2749e+06 -1.1140e+05  7.6214e+03  1.6107e+05\n   FREEHOLD             -9.2067e+06  3.8073e+04  1.5169e+05  3.7528e+05\n                             Max.\n   Intercept            112793548\n   AREA_SQM                 21575\n   AGE                     434201\n   PROX_CBD               2704596\n   PROX_CHILDCARE         1654087\n   PROX_ELDERLYCARE      38867814\n   PROX_URA_GROWTH_AREA  78515730\n   PROX_MRT               3124316\n   PROX_PARK             18122425\n   PROX_PRIMARY_SCH       4637503\n   PROX_SHOPPING_MALL     1529952\n   PROX_BUS_STOP         11342182\n   NO_Of_UNITS              12907\n   FAMILY_FRIENDLY        1720744\n   FREEHOLD               6073636\n   ************************Diagnostic information*************************\n   Number of data points: 1436 \n   Effective number of parameters (2trace(S) - trace(S'S)): 438.3804 \n   Effective degrees of freedom (n-2trace(S) + trace(S'S)): 997.6196 \n   AICc (GWR book, Fotheringham, et al. 2002, p. 61, eq 2.33): 42263.61 \n   AIC (GWR book, Fotheringham, et al. 2002,GWR p. 96, eq. 4.22): 41632.36 \n   BIC (GWR book, Fotheringham, et al. 2002,GWR p. 61, eq. 2.34): 42515.71 \n   Residual sum of squares: 2.53407e+14 \n   R-square value:  0.8909912 \n   Adjusted R-square value:  0.8430417 \n\n   ***********************************************************************\n   Program stops at: 2024-11-04 07:48:46.816364 \n\n\nThe report shows that the AICc of the gwr is 42263.61 which is significantly smaller than the globel multiple linear regression model of 42967.1.\n\n\n\nBuilding Adaptive Bandwidth GWR Model\nIn this section, we will calibrate the gwr-based hedonic pricing model by using adaptive bandwidth approach.\n\nComputing the adaptive bandwidth\nSimilar to the earlier section, we will first use bw.gwr() to determine the recommended data point to use.\nThe code chunk used look very similar to the one used to compute the fixed bandwidth except the adaptive argument has changed to TRUE.\n\nbw_adaptive &lt;- bw.gwr(formula = SELLING_PRICE ~ AREA_SQM + AGE  + \n                        PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE    + \n                        PROX_URA_GROWTH_AREA + PROX_MRT + PROX_PARK + \n                        PROX_PRIMARY_SCH + PROX_SHOPPING_MALL   + PROX_BUS_STOP + \n                        NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n                      data=condo_resale_sf, \n                      approach=\"CV\", \n                      kernel=\"gaussian\", \n                      adaptive=TRUE, \n                      longlat=FALSE)\n\nAdaptive bandwidth: 895 CV score: 7.952401e+14 \nAdaptive bandwidth: 561 CV score: 7.667364e+14 \nAdaptive bandwidth: 354 CV score: 6.953454e+14 \nAdaptive bandwidth: 226 CV score: 6.15223e+14 \nAdaptive bandwidth: 147 CV score: 5.674373e+14 \nAdaptive bandwidth: 98 CV score: 5.426745e+14 \nAdaptive bandwidth: 68 CV score: 5.168117e+14 \nAdaptive bandwidth: 49 CV score: 4.859631e+14 \nAdaptive bandwidth: 37 CV score: 4.646518e+14 \nAdaptive bandwidth: 30 CV score: 4.422088e+14 \nAdaptive bandwidth: 25 CV score: 4.430816e+14 \nAdaptive bandwidth: 32 CV score: 4.505602e+14 \nAdaptive bandwidth: 27 CV score: 4.462172e+14 \nAdaptive bandwidth: 30 CV score: 4.422088e+14 \n\n\nThe result shows that the 30 is the recommended data points to be used.\n\n\nConstructing the adaptive bandwidth gwr model\nNow, we can go ahead to calibrate the gwr-based hedonic pricing model by using adaptive bandwidth and gaussian kernel as shown in the code chunk below.\n\ngwr_adaptive &lt;- gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + \n                            PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE + \n                            PROX_URA_GROWTH_AREA + PROX_MRT + PROX_PARK + \n                            PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_BUS_STOP + \n                            NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n                          data=condo_resale_sf, \n                          bw=bw_adaptive, \n                          kernel = 'gaussian', \n                          adaptive=TRUE, \n                          longlat = FALSE)\n\nThe code below can be used to display the model output.\n\ngwr_adaptive\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n   Program starts at: 2024-11-04 07:48:51.507854 \n   Call:\n   gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + \n    PROX_CHILDCARE + PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + \n    PROX_MRT + PROX_PARK + PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + \n    PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n    data = condo_resale_sf, bw = bw_adaptive, kernel = \"gaussian\", \n    adaptive = TRUE, longlat = FALSE)\n\n   Dependent (y) variable:  SELLING_PRICE\n   Independent variables:  AREA_SQM AGE PROX_CBD PROX_CHILDCARE PROX_ELDERLYCARE PROX_URA_GROWTH_AREA PROX_MRT PROX_PARK PROX_PRIMARY_SCH PROX_SHOPPING_MALL PROX_BUS_STOP NO_Of_UNITS FAMILY_FRIENDLY FREEHOLD\n   Number of data points: 1436\n   ***********************************************************************\n   *                    Results of Global Regression                     *\n   ***********************************************************************\n\n   Call:\n    lm(formula = formula, data = data)\n\n   Residuals:\n     Min       1Q   Median       3Q      Max \n-3470778  -298119   -23481   248917 12234210 \n\n   Coefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n   (Intercept)           527633.22  108183.22   4.877 1.20e-06 ***\n   AREA_SQM               12777.52     367.48  34.771  &lt; 2e-16 ***\n   AGE                   -24687.74    2754.84  -8.962  &lt; 2e-16 ***\n   PROX_CBD              -77131.32    5763.12 -13.384  &lt; 2e-16 ***\n   PROX_CHILDCARE       -318472.75  107959.51  -2.950 0.003231 ** \n   PROX_ELDERLYCARE      185575.62   39901.86   4.651 3.61e-06 ***\n   PROX_URA_GROWTH_AREA   39163.25   11754.83   3.332 0.000885 ***\n   PROX_MRT             -294745.11   56916.37  -5.179 2.56e-07 ***\n   PROX_PARK             570504.81   65507.03   8.709  &lt; 2e-16 ***\n   PROX_PRIMARY_SCH      159856.14   60234.60   2.654 0.008046 ** \n   PROX_SHOPPING_MALL   -220947.25   36561.83  -6.043 1.93e-09 ***\n   PROX_BUS_STOP         682482.22  134513.24   5.074 4.42e-07 ***\n   NO_Of_UNITS             -245.48      87.95  -2.791 0.005321 ** \n   FAMILY_FRIENDLY       146307.58   46893.02   3.120 0.001845 ** \n   FREEHOLD              350599.81   48506.48   7.228 7.98e-13 ***\n\n   ---Significance stars\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n   Residual standard error: 756000 on 1421 degrees of freedom\n   Multiple R-squared: 0.6507\n   Adjusted R-squared: 0.6472 \n   F-statistic: 189.1 on 14 and 1421 DF,  p-value: &lt; 2.2e-16 \n   ***Extra Diagnostic information\n   Residual sum of squares: 8.120609e+14\n   Sigma(hat): 752522.9\n   AIC:  42966.76\n   AICc:  42967.14\n   BIC:  41731.39\n   ***********************************************************************\n   *          Results of Geographically Weighted Regression              *\n   ***********************************************************************\n\n   *********************Model calibration information*********************\n   Kernel function: gaussian \n   Adaptive bandwidth: 30 (number of nearest neighbours)\n   Regression points: the same locations as observations are used.\n   Distance metric: Euclidean distance metric is used.\n\n   ****************Summary of GWR coefficient estimates:******************\n                               Min.     1st Qu.      Median     3rd Qu.\n   Intercept            -1.3487e+08 -2.4669e+05  7.7928e+05  1.6194e+06\n   AREA_SQM              3.3188e+03  5.6285e+03  7.7825e+03  1.2738e+04\n   AGE                  -9.6746e+04 -2.9288e+04 -1.4043e+04 -5.6119e+03\n   PROX_CBD             -2.5330e+06 -1.6256e+05 -7.7242e+04  2.6624e+03\n   PROX_CHILDCARE       -1.2790e+06 -2.0175e+05  8.7158e+03  3.7778e+05\n   PROX_ELDERLYCARE     -1.6212e+06 -9.2050e+04  6.1029e+04  2.8184e+05\n   PROX_URA_GROWTH_AREA -7.2686e+06 -3.0350e+04  4.5869e+04  2.4613e+05\n   PROX_MRT             -4.3781e+07 -6.7282e+05 -2.2115e+05 -7.4593e+04\n   PROX_PARK            -2.9020e+06 -1.6782e+05  1.1601e+05  4.6572e+05\n   PROX_PRIMARY_SCH     -8.6418e+05 -1.6627e+05 -7.7853e+03  4.3222e+05\n   PROX_SHOPPING_MALL   -1.8272e+06 -1.3175e+05 -1.4049e+04  1.3799e+05\n   PROX_BUS_STOP        -2.0579e+06 -7.1461e+04  4.1104e+05  1.2071e+06\n   NO_Of_UNITS          -2.1993e+03 -2.3685e+02 -3.4699e+01  1.1657e+02\n   FAMILY_FRIENDLY      -5.9879e+05 -5.0927e+04  2.6173e+04  2.2481e+05\n   FREEHOLD             -1.6340e+05  4.0765e+04  1.9023e+05  3.7960e+05\n                            Max.\n   Intercept            18758355\n   AREA_SQM                23064\n   AGE                     13303\n   PROX_CBD             11346650\n   PROX_CHILDCARE        2892127\n   PROX_ELDERLYCARE      2465671\n   PROX_URA_GROWTH_AREA  7384059\n   PROX_MRT              1186242\n   PROX_PARK             2588497\n   PROX_PRIMARY_SCH      3381462\n   PROX_SHOPPING_MALL   38038564\n   PROX_BUS_STOP        12081592\n   NO_Of_UNITS              1010\n   FAMILY_FRIENDLY       2072414\n   FREEHOLD              1813995\n   ************************Diagnostic information*************************\n   Number of data points: 1436 \n   Effective number of parameters (2trace(S) - trace(S'S)): 350.3088 \n   Effective degrees of freedom (n-2trace(S) + trace(S'S)): 1085.691 \n   AICc (GWR book, Fotheringham, et al. 2002, p. 61, eq 2.33): 41982.22 \n   AIC (GWR book, Fotheringham, et al. 2002,GWR p. 96, eq. 4.22): 41546.74 \n   BIC (GWR book, Fotheringham, et al. 2002,GWR p. 61, eq. 2.34): 41914.08 \n   Residual sum of squares: 2.528227e+14 \n   R-square value:  0.8912425 \n   Adjusted R-square value:  0.8561185 \n\n   ***********************************************************************\n   Program stops at: 2024-11-04 07:48:52.214415 \n\n\nThe report shows that the AICc the adaptive distance gwr is 41982.22 which is even smaller than the AICc of the fixed distance gwr of 42263.61.\n\n\n\nVisualising GWR Output\nIn addition to regression residuals, the output feature class table includes fields for observed and predicted y values, condition number (cond), Local R2, residuals, and explanatory variable coefficients and standard errors:\n\nCondition Number: this diagnostic evaluates local collinearity. In the presence of strong local collinearity, results become unstable. Results associated with condition numbers larger than 30, may be unreliable.\nLocal R2: these values range between 0.0 and 1.0 and indicate how well the local regression model fits observed y values. Very low values indicate the local model is performing poorly. Mapping the Local R2 values to see where GWR predicts well and where it predicts poorly may provide clues about important variables that may be missing from the regression model.\nPredicted: these are the estimated (or fitted) y values 3. computed by GWR.\nResiduals: to obtain the residual values, the fitted y values are subtracted from the observed y values. Standardized residuals have a mean of zero and a standard deviation of 1. A cold-to-hot rendered map of standardized residuals can be produce by using these values.\nCoefficient Standard Error: these values measure the reliability of each coefficient estimate. Confidence in those estimates are higher when standard errors are small in relation to the actual coefficient values. Large standard errors may indicate problems with local collinearity.\n\nThey are all stored in a SpatialPointsDataFrame or SpatialPolygonsDataFrame object integrated with fit.points, GWR coefficient estimates, y value, predicted values, coefficient standard errors and t-values in its “data” slot in an object called SDF of the output list.\n\n\nConverting SDF into sf data.frame\nTo visualise the fields in SDF, we need to first covert it into sf data.frame by using the code chunk below.\n\ngwr_adaptive_output &lt;- as.data.frame(\n  gwr_adaptive$SDF) %&gt;%\n  select(-c(2:15))\n\n\ngwr_sf_adaptive &lt;- cbind(condo_resale_sf,\n                         gwr_adaptive_output)\n\nNext, glimpse() is used to display the content of condo_resale_sf.adaptive sf data frame.\n\nglimpse(gwr_sf_adaptive)\n\nRows: 1,436\nColumns: 63\n$ nb                      &lt;nb&gt; &lt;66, 77, 123, 238, 239, 343&gt;, &lt;21, 162, 163, 19…\n$ wt                      &lt;list&gt; &lt;0.1666667, 0.1666667, 0.1666667, 0.1666667, …\n$ POSTCODE                &lt;dbl&gt; 118635, 288420, 267833, 258380, 467169, 466472…\n$ SELLING_PRICE           &lt;dbl&gt; 3000000, 3880000, 3325000, 4250000, 1400000, 1…\n$ AREA_SQM                &lt;dbl&gt; 309, 290, 248, 127, 145, 139, 218, 141, 165, 1…\n$ AGE                     &lt;dbl&gt; 30, 32, 33, 7, 28, 22, 24, 24, 27, 31, 17, 22,…\n$ PROX_CBD                &lt;dbl&gt; 7.941259, 6.609797, 6.898000, 4.038861, 11.783…\n$ PROX_CHILDCARE          &lt;dbl&gt; 0.16597932, 0.28027246, 0.42922669, 0.39473543…\n$ PROX_ELDERLYCARE        &lt;dbl&gt; 2.5198118, 1.9333338, 0.5021395, 1.9910316, 1.…\n$ PROX_URA_GROWTH_AREA    &lt;dbl&gt; 6.618741, 7.505109, 6.463887, 4.906512, 6.4106…\n$ PROX_HAWKER_MARKET      &lt;dbl&gt; 1.76542207, 0.54507614, 0.37789301, 1.68259969…\n$ PROX_KINDERGARTEN       &lt;dbl&gt; 0.05835552, 0.61592412, 0.14120309, 0.38200076…\n$ PROX_MRT                &lt;dbl&gt; 0.5607188, 0.6584461, 0.3053433, 0.6910183, 0.…\n$ PROX_PARK               &lt;dbl&gt; 1.1710446, 0.1992269, 0.2779886, 0.9832843, 0.…\n$ PROX_PRIMARY_SCH        &lt;dbl&gt; 1.6340256, 0.9747834, 1.4715016, 1.4546324, 0.…\n$ PROX_TOP_PRIMARY_SCH    &lt;dbl&gt; 3.3273195, 0.9747834, 1.4715016, 2.3006394, 0.…\n$ PROX_SHOPPING_MALL      &lt;dbl&gt; 2.2102717, 2.9374279, 1.2256850, 0.3525671, 1.…\n$ PROX_SUPERMARKET        &lt;dbl&gt; 0.9103958, 0.5900617, 0.4135583, 0.4162219, 0.…\n$ PROX_BUS_STOP           &lt;dbl&gt; 0.10336166, 0.28673408, 0.28504777, 0.29872340…\n$ NO_Of_UNITS             &lt;dbl&gt; 18, 20, 27, 30, 30, 31, 32, 32, 32, 32, 34, 34…\n$ FAMILY_FRIENDLY         &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0…\n$ FREEHOLD                &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1…\n$ LEASEHOLD_99YR          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ MLR_RES                 &lt;dbl&gt; -1489099.55, 415494.57, 194129.69, 1088992.71,…\n$ Intercept               &lt;dbl&gt; 2050011.67, 1633128.24, 3433608.17, 234358.91,…\n$ y                       &lt;dbl&gt; 3000000, 3880000, 3325000, 4250000, 1400000, 1…\n$ yhat                    &lt;dbl&gt; 2886531.8, 3466801.5, 3616527.2, 5435481.6, 13…\n$ residual                &lt;dbl&gt; 113468.16, 413198.52, -291527.20, -1185481.63,…\n$ CV_Score                &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Stud_residual           &lt;dbl&gt; 0.38207013, 1.01433140, -0.83780678, -2.846146…\n$ Intercept_SE            &lt;dbl&gt; 516105.5, 488083.5, 963711.4, 444185.5, 211962…\n$ AREA_SQM_SE             &lt;dbl&gt; 823.2860, 825.2380, 988.2240, 617.4007, 1376.2…\n$ AGE_SE                  &lt;dbl&gt; 5889.782, 6226.916, 6510.236, 6010.511, 8180.3…\n$ PROX_CBD_SE             &lt;dbl&gt; 37411.22, 23615.06, 56103.77, 469337.41, 41064…\n$ PROX_CHILDCARE_SE       &lt;dbl&gt; 319111.1, 299705.3, 349128.5, 304965.2, 698720…\n$ PROX_ELDERLYCARE_SE     &lt;dbl&gt; 120633.34, 84546.69, 129687.07, 127150.69, 327…\n$ PROX_URA_GROWTH_AREA_SE &lt;dbl&gt; 56207.39, 76956.50, 95774.60, 470762.12, 47433…\n$ PROX_MRT_SE             &lt;dbl&gt; 185181.3, 281133.9, 275483.7, 279877.1, 363830…\n$ PROX_PARK_SE            &lt;dbl&gt; 205499.6, 229358.7, 314124.3, 227249.4, 364580…\n$ PROX_PRIMARY_SCH_SE     &lt;dbl&gt; 152400.7, 165150.7, 196662.6, 240878.9, 249087…\n$ PROX_SHOPPING_MALL_SE   &lt;dbl&gt; 109268.8, 98906.8, 119913.3, 177104.1, 301032.…\n$ PROX_BUS_STOP_SE        &lt;dbl&gt; 600668.6, 410222.1, 464156.7, 562810.8, 740922…\n$ NO_Of_UNITS_SE          &lt;dbl&gt; 218.1258, 208.9410, 210.9828, 361.7767, 299.50…\n$ FAMILY_FRIENDLY_SE      &lt;dbl&gt; 131474.73, 114989.07, 146607.22, 108726.62, 16…\n$ FREEHOLD_SE             &lt;dbl&gt; 115954.0, 130110.0, 141031.5, 138239.1, 210641…\n$ Intercept_TV            &lt;dbl&gt; 3.9720784, 3.3460017, 3.5629010, 0.5276150, 1.…\n$ AREA_SQM_TV             &lt;dbl&gt; 11.614302, 20.087361, 13.247868, 33.577223, 4.…\n$ AGE_TV                  &lt;dbl&gt; -1.6154474, -9.3441881, -4.1023685, -15.524301…\n$ PROX_CBD_TV             &lt;dbl&gt; -3.22582173, -6.32792021, -4.62353528, 5.17080…\n$ PROX_CHILDCARE_TV       &lt;dbl&gt; 1.000488185, 1.471786337, -0.344047555, 1.5766…\n$ PROX_ELDERLYCARE_TV     &lt;dbl&gt; -3.26126929, 3.84626245, 4.13191383, 2.4756745…\n$ PROX_URA_GROWTH_AREA_TV &lt;dbl&gt; -2.846248368, -1.848971738, -2.648105057, -5.6…\n$ PROX_MRT_TV             &lt;dbl&gt; -1.61864578, -8.92998600, -3.40075727, -7.2870…\n$ PROX_PARK_TV            &lt;dbl&gt; -0.83749312, 2.28192684, 0.66565951, -3.340617…\n$ PROX_PRIMARY_SCH_TV     &lt;dbl&gt; 1.59230221, 6.70194543, 2.90580089, 12.9836104…\n$ PROX_SHOPPING_MALL_TV   &lt;dbl&gt; 2.753588422, -0.886626400, -1.056869486, -0.16…\n$ PROX_BUS_STOP_TV        &lt;dbl&gt; 2.0154464, 4.4941192, 3.0419145, 12.8383775, 0…\n$ NO_Of_UNITS_TV          &lt;dbl&gt; 0.480589953, -1.380026395, -0.045279967, -0.44…\n$ FAMILY_FRIENDLY_TV      &lt;dbl&gt; -0.06902748, 2.69655779, 0.04058290, 14.312764…\n$ FREEHOLD_TV             &lt;dbl&gt; 2.6213469, 3.0452799, 1.1970499, 8.7711485, 1.…\n$ Local_R2                &lt;dbl&gt; 0.8846744, 0.8899773, 0.8947007, 0.9073605, 0.…\n$ geometry                &lt;POINT [m]&gt; POINT (22085.12 29951.54), POINT (25656.…\n$ geometry.1              &lt;POINT [m]&gt; POINT (22085.12 29951.54), POINT (25656.…\n\n\n\nsummary(gwr_adaptive$SDF$yhat)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n  171347  1102001  1385528  1751842  1982307 13887901 \n\n\n\n\nVisualising local R2\nThe code chunks below is used to create an interactive point symbol map.\n\ntmap_mode(\"view\")\ntmap_options(check.and.fix = TRUE)\ntm_shape(mpsz)+\n  tm_polygons(alpha = 0.1) +\ntm_shape(gwr_sf_adaptive) +  \n  tm_dots(col = \"Local_R2\",\n          border.col = \"gray60\",\n          border.lwd = 1) +\n  tm_view(set.zoom.limits = c(11,14))\n\n\n\n\ntmap_mode(\"plot\")\n\n\n\nVisualising coefficient estimates\nThe code chunks below is used to create an interactive point symbol map.\n\ntmap_options(check.and.fix = TRUE)\ntmap_mode(\"view\")\nAREA_SQM_SE &lt;- tm_shape(mpsz)+\n  tm_polygons(alpha = 0.1) +\ntm_shape(gwr_sf_adaptive) +  \n  tm_dots(col = \"AREA_SQM_SE\",\n          border.col = \"gray60\",\n          border.lwd = 1) +\n  tm_view(set.zoom.limits = c(11,14))\n\nAREA_SQM_TV &lt;- tm_shape(mpsz)+\n  tm_polygons(alpha = 0.1) +\ntm_shape(gwr_sf_adaptive) +  \n  tm_dots(col = \"AREA_SQM_TV\",\n          border.col = \"gray60\",\n          border.lwd = 1) +\n  tm_view(set.zoom.limits = c(11,14))\n\ntmap_arrange(AREA_SQM_SE, AREA_SQM_TV, \n             asp=1, ncol=2,\n             sync = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\ntmap_mode(\"plot\")\n\n\nBy URA Plannign Region\n\ntm_shape(mpsz[mpsz$REGION_N==\"CENTRAL REGION\", ])+\n  tm_polygons()+\ntm_shape(gwr_sf_adaptive) + \n  tm_bubbles(col = \"Local_R2\",\n           size = 0.15,\n           border.col = \"gray60\",\n           border.lwd = 1)"
  }
]